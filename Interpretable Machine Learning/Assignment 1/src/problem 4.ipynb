{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "training_set = \"../data/adult_train.csv\"\n",
    "test_set = \"../data/adult_test.csv\"\n",
    "\n",
    "training_data = pd.read_csv(training_set)\n",
    "test_data = pd.read_csv(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the dataset\n",
    "- check training and test labels\n",
    "- check training and test features\n",
    "    - check dtypes, and seperate numerical and object features\n",
    "    - for \"object\" features, check if there are such labels in test set, but not in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels:\n",
      " <=50K    24720\n",
      " >50K      7841\n",
      "Name: label, dtype: int64\n",
      "--------------------------------------------------\n",
      "test labels:\n",
      " <=50K.    12435\n",
      " >50K.      3846\n",
      "Name: label, dtype: int64\n",
      "(32561, 14)\n",
      "(16281, 14)\n"
     ]
    }
   ],
   "source": [
    "train_y = training_data[\"label\"]\n",
    "train_x = training_data.drop(\"label\", axis = 1)\n",
    "print(\"training labels:\")\n",
    "print(train_y.value_counts())\n",
    "print(\"-\"*50)\n",
    "\n",
    "test_y = test_data[\"label\"]\n",
    "test_x = test_data.drop(\"label\", axis = 1)\n",
    "print(\"test labels:\")\n",
    "print(test_y.value_counts())\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>160187</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Jamaica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>209642</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>45781</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>159449</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "5   37            Private  284582     Masters             14   \n",
       "6   49            Private  160187         9th              5   \n",
       "7   52   Self-emp-not-inc  209642     HS-grad              9   \n",
       "8   31            Private   45781     Masters             14   \n",
       "9   42            Private  159449   Bachelors             13   \n",
       "\n",
       "           marital-status          occupation    relationship    race  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black   \n",
       "5      Married-civ-spouse     Exec-managerial            Wife   White   \n",
       "6   Married-spouse-absent       Other-service   Not-in-family   Black   \n",
       "7      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "8           Never-married      Prof-specialty   Not-in-family   White   \n",
       "9      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "\n",
       "       sex  capital-gain  capital-loss  hours-per-week  native-country  \n",
       "0     Male          2174             0              40   United-States  \n",
       "1     Male             0             0              13   United-States  \n",
       "2     Male             0             0              40   United-States  \n",
       "3     Male             0             0              40   United-States  \n",
       "4   Female             0             0              40            Cuba  \n",
       "5   Female             0             0              40   United-States  \n",
       "6   Female             0             0              16         Jamaica  \n",
       "7     Male             0             0              45   United-States  \n",
       "8   Female         14084             0              50   United-States  \n",
       "9     Male          5178             0              40   United-States  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>198693</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>?</td>\n",
       "      <td>227026</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>104626</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>3103</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>Private</td>\n",
       "      <td>369667</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>104996</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt      education  education-num  \\\n",
       "0   25            Private  226802           11th              7   \n",
       "1   38            Private   89814        HS-grad              9   \n",
       "2   28          Local-gov  336951     Assoc-acdm             12   \n",
       "3   44            Private  160323   Some-college             10   \n",
       "4   18                  ?  103497   Some-college             10   \n",
       "5   34            Private  198693           10th              6   \n",
       "6   29                  ?  227026        HS-grad              9   \n",
       "7   63   Self-emp-not-inc  104626    Prof-school             15   \n",
       "8   24            Private  369667   Some-college             10   \n",
       "9   55            Private  104996        7th-8th              4   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married   Machine-op-inspct       Own-child   Black     Male   \n",
       "1   Married-civ-spouse     Farming-fishing         Husband   White     Male   \n",
       "2   Married-civ-spouse     Protective-serv         Husband   White     Male   \n",
       "3   Married-civ-spouse   Machine-op-inspct         Husband   Black     Male   \n",
       "4        Never-married                   ?       Own-child   White   Female   \n",
       "5        Never-married       Other-service   Not-in-family   White     Male   \n",
       "6        Never-married                   ?       Unmarried   Black     Male   \n",
       "7   Married-civ-spouse      Prof-specialty         Husband   White     Male   \n",
       "8        Never-married       Other-service       Unmarried   White   Female   \n",
       "9   Married-civ-spouse        Craft-repair         Husband   White     Male   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  \n",
       "0             0             0              40   United-States  \n",
       "1             0             0              50   United-States  \n",
       "2             0             0              40   United-States  \n",
       "3          7688             0              40   United-States  \n",
       "4             0             0              30   United-States  \n",
       "5             0             0              30   United-States  \n",
       "6             0             0              40   United-States  \n",
       "7          3103             0              32   United-States  \n",
       "8             0             0              40   United-States  \n",
       "9             0             0              10   United-States  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training types:\n",
      " age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "test types:\n",
      " age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "difference: set()\n"
     ]
    }
   ],
   "source": [
    "print(\"training types:\\n\", train_x.dtypes)\n",
    "print(\"-\"*50)\n",
    "print(\"test types:\\n\", test_x.dtypes)\n",
    "print(\"-\"*50)\n",
    "print(\"difference:\", set(train_x.dtypes.index) - set(test_x.dtypes.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding features and labels\n",
    "\n",
    "- Cannot find an elegant approach (i.e., in no more than 2 lines) to use `sklearn.preprocessing.OneHotEncoder`, so I just use `pd.get_dummies` instead\n",
    "- Scale all features to [0, 1] (in a featurewise approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=32561, step=1)\n",
      "RangeIndex(start=32560, stop=48841, step=1)\n",
      "(32561, 108)\n",
      "(16281, 108)\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: label, dtype: int64\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    1\n",
      "8    0\n",
      "9    0\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_ ?</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_ Portugal</th>\n",
       "      <th>native-country_ Puerto-Rico</th>\n",
       "      <th>native-country_ Scotland</th>\n",
       "      <th>native-country_ South</th>\n",
       "      <th>native-country_ Taiwan</th>\n",
       "      <th>native-country_ Thailand</th>\n",
       "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_ United-States</th>\n",
       "      <th>native-country_ Vietnam</th>\n",
       "      <th>native-country_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.044131</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.048052</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.137581</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.150486</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.220635</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.184219</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.438356</td>\n",
       "      <td>0.100061</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.133519</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.191781</td>\n",
       "      <td>0.022661</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.140841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.342466</td>\n",
       "      <td>0.099562</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.051781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0  0.301370  0.044131       0.800000      0.021740           0.0   \n",
       "1  0.452055  0.048052       0.800000      0.000000           0.0   \n",
       "2  0.287671  0.137581       0.533333      0.000000           0.0   \n",
       "3  0.493151  0.150486       0.400000      0.000000           0.0   \n",
       "4  0.150685  0.220635       0.800000      0.000000           0.0   \n",
       "5  0.273973  0.184219       0.866667      0.000000           0.0   \n",
       "6  0.438356  0.100061       0.266667      0.000000           0.0   \n",
       "7  0.479452  0.133519       0.533333      0.000000           0.0   \n",
       "8  0.191781  0.022661       0.866667      0.140841           0.0   \n",
       "9  0.342466  0.099562       0.800000      0.051781           0.0   \n",
       "\n",
       "   hours-per-week  workclass_ ?  workclass_ Federal-gov  workclass_ Local-gov  \\\n",
       "0        0.397959           0.0                     0.0                   0.0   \n",
       "1        0.122449           0.0                     0.0                   0.0   \n",
       "2        0.397959           0.0                     0.0                   0.0   \n",
       "3        0.397959           0.0                     0.0                   0.0   \n",
       "4        0.397959           0.0                     0.0                   0.0   \n",
       "5        0.397959           0.0                     0.0                   0.0   \n",
       "6        0.153061           0.0                     0.0                   0.0   \n",
       "7        0.448980           0.0                     0.0                   0.0   \n",
       "8        0.500000           0.0                     0.0                   0.0   \n",
       "9        0.397959           0.0                     0.0                   0.0   \n",
       "\n",
       "   workclass_ Never-worked  ...  native-country_ Portugal  \\\n",
       "0                      0.0  ...                       0.0   \n",
       "1                      0.0  ...                       0.0   \n",
       "2                      0.0  ...                       0.0   \n",
       "3                      0.0  ...                       0.0   \n",
       "4                      0.0  ...                       0.0   \n",
       "5                      0.0  ...                       0.0   \n",
       "6                      0.0  ...                       0.0   \n",
       "7                      0.0  ...                       0.0   \n",
       "8                      0.0  ...                       0.0   \n",
       "9                      0.0  ...                       0.0   \n",
       "\n",
       "   native-country_ Puerto-Rico  native-country_ Scotland  \\\n",
       "0                          0.0                       0.0   \n",
       "1                          0.0                       0.0   \n",
       "2                          0.0                       0.0   \n",
       "3                          0.0                       0.0   \n",
       "4                          0.0                       0.0   \n",
       "5                          0.0                       0.0   \n",
       "6                          0.0                       0.0   \n",
       "7                          0.0                       0.0   \n",
       "8                          0.0                       0.0   \n",
       "9                          0.0                       0.0   \n",
       "\n",
       "   native-country_ South  native-country_ Taiwan  native-country_ Thailand  \\\n",
       "0                    0.0                     0.0                       0.0   \n",
       "1                    0.0                     0.0                       0.0   \n",
       "2                    0.0                     0.0                       0.0   \n",
       "3                    0.0                     0.0                       0.0   \n",
       "4                    0.0                     0.0                       0.0   \n",
       "5                    0.0                     0.0                       0.0   \n",
       "6                    0.0                     0.0                       0.0   \n",
       "7                    0.0                     0.0                       0.0   \n",
       "8                    0.0                     0.0                       0.0   \n",
       "9                    0.0                     0.0                       0.0   \n",
       "\n",
       "   native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
       "0                              0.0                            1.0   \n",
       "1                              0.0                            1.0   \n",
       "2                              0.0                            1.0   \n",
       "3                              0.0                            1.0   \n",
       "4                              0.0                            0.0   \n",
       "5                              0.0                            1.0   \n",
       "6                              0.0                            0.0   \n",
       "7                              0.0                            1.0   \n",
       "8                              0.0                            1.0   \n",
       "9                              0.0                            1.0   \n",
       "\n",
       "   native-country_ Vietnam  native-country_ Yugoslavia  \n",
       "0                      0.0                         0.0  \n",
       "1                      0.0                         0.0  \n",
       "2                      0.0                         0.0  \n",
       "3                      0.0                         0.0  \n",
       "4                      0.0                         0.0  \n",
       "5                      0.0                         0.0  \n",
       "6                      0.0                         0.0  \n",
       "7                      0.0                         0.0  \n",
       "8                      0.0                         0.0  \n",
       "9                      0.0                         0.0  \n",
       "\n",
       "[10 rows x 108 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index = train_x.index\n",
    "test_index = test_x.index + train_x.index[-1]\n",
    "print(train_index)\n",
    "print(test_index)\n",
    "\n",
    "x = pd.concat([train_x, test_x], axis = 0)\n",
    "x = pd.get_dummies(x)\n",
    "\n",
    "# scale to [0, 1]\n",
    "x -= x.min()\n",
    "x /= x.max()\n",
    "\n",
    "train_X = x.iloc[train_index]\n",
    "test_X = x.iloc[test_index]\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "\n",
    "train_Y = train_y.replace({\" <=50K\": 0, \" >50K\": 1})\n",
    "test_Y = test_y.replace({\" <=50K.\": 0, \" >50K.\": 1})\n",
    "print(train_Y.head(10))\n",
    "print(test_Y.head(10))\n",
    "train_X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# global seed\n",
    "np.random.seed(1)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(train_X.to_numpy(), train_Y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = rfc.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis = 0)\n",
    "indices = np.argsort(importances)[::-1][:20]\n",
    "\n",
    "names = train_X.columns[indices]\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "f, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "ax.bar(np.arange(len(indices)), importances[indices], color = 'r', yerr = std[indices], align = \"center\")\n",
    "plt.sca(ax)\n",
    "plt.xticks(np.arange(len(indices)), names, rotation=-80, fontsize = 10)\n",
    "ax.set_xlim([-1, len(indices)])\n",
    "ax.set_title(\"Importance of the {0} most important features\".format(len(indices)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the output of RFC by logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, training loss: 0.6931471805599453\n",
      "Iteration: 1, training loss: 0.6897877565248203\n",
      "Iteration: 2, training loss: 0.6865070405650309\n",
      "Iteration: 3, training loss: 0.6833030129578818\n",
      "Iteration: 4, training loss: 0.6801737007043541\n",
      "Iteration: 5, training loss: 0.6771171769753086\n",
      "Iteration: 6, training loss: 0.6741315605211428\n",
      "Iteration: 7, training loss: 0.671215015048889\n",
      "Iteration: 8, training loss: 0.6683657485705019\n",
      "Iteration: 9, training loss: 0.6655820127258746\n",
      "Iteration: 10, training loss: 0.6628621020839012\n",
      "Iteration: 11, training loss: 0.6602043534246935\n",
      "Iteration: 12, training loss: 0.6576071450058688\n",
      "Iteration: 13, training loss: 0.655068895815613\n",
      "Iteration: 14, training loss: 0.6525880648150573\n",
      "Iteration: 15, training loss: 0.6501631501723099\n",
      "Iteration: 16, training loss: 0.6477926884903229\n",
      "Iteration: 17, training loss: 0.6454752540306067\n",
      "Iteration: 18, training loss: 0.6432094579346492\n",
      "Iteration: 19, training loss: 0.6409939474447496\n",
      "Iteration: 20, training loss: 0.6388274051258321\n",
      "Iteration: 21, training loss: 0.63670854808968\n",
      "Iteration: 22, training loss: 0.6346361272228916\n",
      "Iteration: 23, training loss: 0.6326089264197569\n",
      "Iteration: 24, training loss: 0.6306257618211273\n",
      "Iteration: 25, training loss: 0.6286854810602542\n",
      "Iteration: 26, training loss: 0.6267869625164744\n",
      "Iteration: 27, training loss: 0.6249291145775183\n",
      "Iteration: 28, training loss: 0.6231108749111457\n",
      "Iteration: 29, training loss: 0.6213312097467182\n",
      "Iteration: 30, training loss: 0.6195891131672524\n",
      "Iteration: 31, training loss: 0.6178836064124256\n",
      "Iteration: 32, training loss: 0.6162137371929368\n",
      "Iteration: 33, training loss: 0.6145785790165724\n",
      "Iteration: 34, training loss: 0.6129772305262642\n",
      "Iteration: 35, training loss: 0.6114088148503807\n",
      "Iteration: 36, training loss: 0.6098724789654417\n",
      "Iteration: 37, training loss: 0.6083673930714053\n",
      "Iteration: 38, training loss: 0.6068927499796332\n",
      "Iteration: 39, training loss: 0.6054477645136073\n",
      "Iteration: 40, training loss: 0.6040316729224351\n",
      "Iteration: 41, training loss: 0.6026437323071492\n",
      "Iteration: 42, training loss: 0.6012832200597813\n",
      "Iteration: 43, training loss: 0.5999494333151651\n",
      "Iteration: 44, training loss: 0.5986416884153972\n",
      "Iteration: 45, training loss: 0.597359320386869\n",
      "Iteration: 46, training loss: 0.5961016824297591\n",
      "Iteration: 47, training loss: 0.5948681454198643\n",
      "Iteration: 48, training loss: 0.5936580974226272\n",
      "Iteration: 49, training loss: 0.5924709432192102\n",
      "Iteration: 50, training loss: 0.5913061038444523\n",
      "Iteration: 51, training loss: 0.5901630161365318\n",
      "Iteration: 52, training loss: 0.5890411322981585\n",
      "Iteration: 53, training loss: 0.5879399194690981\n",
      "Iteration: 54, training loss: 0.586858859309839\n",
      "Iteration: 55, training loss: 0.5857974475961956\n",
      "Iteration: 56, training loss: 0.5847551938246449\n",
      "Iteration: 57, training loss: 0.583731620828186\n",
      "Iteration: 58, training loss: 0.5827262644025085\n",
      "Iteration: 59, training loss: 0.5817386729422623\n",
      "Iteration: 60, training loss: 0.5807684070872042\n",
      "Iteration: 61, training loss: 0.5798150393780132\n",
      "Iteration: 62, training loss: 0.5788781539215557\n",
      "Iteration: 63, training loss: 0.577957346065384\n",
      "Iteration: 64, training loss: 0.577052222081255\n",
      "Iteration: 65, training loss: 0.5761623988574548\n",
      "Iteration: 66, training loss: 0.5752875035997173\n",
      "Iteration: 67, training loss: 0.5744271735405269\n",
      "Iteration: 68, training loss: 0.5735810556565972\n",
      "Iteration: 69, training loss: 0.5727488063943225\n",
      "Iteration: 70, training loss: 0.5719300914029934\n",
      "Iteration: 71, training loss: 0.5711245852755865\n",
      "Iteration: 72, training loss: 0.5703319712969207\n",
      "Iteration: 73, training loss: 0.5695519411989948\n",
      "Iteration: 74, training loss: 0.5687841949233124\n",
      "Iteration: 75, training loss: 0.5680284403900087\n",
      "Iteration: 76, training loss: 0.5672843932735925\n",
      "Iteration: 77, training loss: 0.5665517767851307\n",
      "Iteration: 78, training loss: 0.5658303214606918\n",
      "Iteration: 79, training loss: 0.5651197649558808\n",
      "Iteration: 80, training loss: 0.564419851846294\n",
      "Iteration: 81, training loss: 0.5637303334337315\n",
      "Iteration: 82, training loss: 0.5630509675580038\n",
      "Iteration: 83, training loss: 0.5623815184141754\n",
      "Iteration: 84, training loss: 0.5617217563750931\n",
      "Iteration: 85, training loss: 0.5610714578190467\n",
      "Iteration: 86, training loss: 0.5604304049624176\n",
      "Iteration: 87, training loss: 0.559798385697171\n",
      "Iteration: 88, training loss: 0.5591751934330529\n",
      "Iteration: 89, training loss: 0.5585606269443573\n",
      "Iteration: 90, training loss: 0.5579544902211293\n",
      "Iteration: 91, training loss: 0.5573565923246778\n",
      "Iteration: 92, training loss: 0.5567667472472696\n",
      "Iteration: 93, training loss: 0.5561847737758862\n",
      "Iteration: 94, training loss: 0.5556104953599222\n",
      "Iteration: 95, training loss: 0.5550437399827111\n",
      "Iteration: 96, training loss: 0.5544843400367643\n",
      "Iteration: 97, training loss: 0.5539321322026173\n",
      "Iteration: 98, training loss: 0.5533869573311748\n",
      "Iteration: 99, training loss: 0.5528486603294512\n",
      "Iteration: 100, training loss: 0.5523170900496078\n",
      "Iteration: 101, training loss: 0.5517920991811881\n",
      "Iteration: 102, training loss: 0.5512735441464569\n",
      "Iteration: 103, training loss: 0.5507612849987519\n",
      "Iteration: 104, training loss: 0.5502551853237575\n",
      "Iteration: 105, training loss: 0.5497551121436152\n",
      "Iteration: 106, training loss: 0.549260935823785\n",
      "Iteration: 107, training loss: 0.5487725299825787\n",
      "Iteration: 108, training loss: 0.5482897714032822\n",
      "Iteration: 109, training loss: 0.547812539948793\n",
      "Iteration: 110, training loss: 0.547340718478697\n",
      "Iteration: 111, training loss: 0.5468741927687126\n",
      "Iteration: 112, training loss: 0.5464128514324292\n",
      "Iteration: 113, training loss: 0.5459565858452774\n",
      "Iteration: 114, training loss: 0.545505290070658\n",
      "Iteration: 115, training loss: 0.545058860788172\n",
      "Iteration: 116, training loss: 0.544617197223884\n",
      "Iteration: 117, training loss: 0.5441802010825626\n",
      "Iteration: 118, training loss: 0.543747776481835\n",
      "Iteration: 119, training loss: 0.543319829888206\n",
      "Iteration: 120, training loss: 0.5428962700548758\n",
      "Iteration: 121, training loss: 0.5424770079613155\n",
      "Iteration: 122, training loss: 0.5420619567545357\n",
      "Iteration: 123, training loss: 0.5416510316920111\n",
      "Iteration: 124, training loss: 0.5412441500861991\n",
      "Iteration: 125, training loss: 0.5408412312506173\n",
      "Iteration: 126, training loss: 0.5404421964474245\n",
      "Iteration: 127, training loss: 0.5400469688364663\n",
      "Iteration: 128, training loss: 0.5396554734257416\n",
      "Iteration: 129, training loss: 0.5392676370232456\n",
      "Iteration: 130, training loss: 0.5388833881901515\n",
      "Iteration: 131, training loss: 0.5385026571952902\n",
      "Iteration: 132, training loss: 0.5381253759708913\n",
      "Iteration: 133, training loss: 0.537751478069546\n",
      "Iteration: 134, training loss: 0.5373808986223599\n",
      "Iteration: 135, training loss: 0.5370135742982567\n",
      "Iteration: 136, training loss: 0.5366494432644017\n",
      "Iteration: 137, training loss: 0.5362884451477132\n",
      "Iteration: 138, training loss: 0.535930520997429\n",
      "Iteration: 139, training loss: 0.5355756132486946\n",
      "Iteration: 140, training loss: 0.5352236656871499\n",
      "Iteration: 141, training loss: 0.534874623414479\n",
      "Iteration: 142, training loss: 0.5345284328148997\n",
      "Iteration: 143, training loss: 0.5341850415225624\n",
      "Iteration: 144, training loss: 0.5338443983898343\n",
      "Iteration: 145, training loss: 0.533506453456443\n",
      "Iteration: 146, training loss: 0.5331711579194534\n",
      "Iteration: 147, training loss: 0.5328384641040553\n",
      "Iteration: 148, training loss: 0.5325083254351389\n",
      "Iteration: 149, training loss: 0.5321806964096336\n",
      "Iteration: 150, training loss: 0.5318555325695902\n",
      "Iteration: 151, training loss: 0.5315327904759845\n",
      "Iteration: 152, training loss: 0.5312124276832213\n",
      "Iteration: 153, training loss: 0.5308944027143208\n",
      "Iteration: 154, training loss: 0.530578675036764\n",
      "Iteration: 155, training loss: 0.5302652050389834\n",
      "Iteration: 156, training loss: 0.5299539540074772\n",
      "Iteration: 157, training loss: 0.5296448841045307\n",
      "Iteration: 158, training loss: 0.5293379583465266\n",
      "Iteration: 159, training loss: 0.5290331405828302\n",
      "Iteration: 160, training loss: 0.5287303954752303\n",
      "Iteration: 161, training loss: 0.5284296884779219\n",
      "Iteration: 162, training loss: 0.5281309858180148\n",
      "Iteration: 163, training loss: 0.5278342544765544\n",
      "Iteration: 164, training loss: 0.5275394621700393\n",
      "Iteration: 165, training loss: 0.5272465773324222\n",
      "Iteration: 166, training loss: 0.5269555690975798\n",
      "Iteration: 167, training loss: 0.5266664072822413\n",
      "Iteration: 168, training loss: 0.52637906236936\n",
      "Iteration: 169, training loss: 0.5260935054919161\n",
      "Iteration: 170, training loss: 0.5258097084171405\n",
      "Iteration: 171, training loss: 0.5255276435311459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 172, training loss: 0.525247283823956\n",
      "Iteration: 173, training loss: 0.5249686028749194\n",
      "Iteration: 174, training loss: 0.5246915748385005\n",
      "Iteration: 175, training loss: 0.5244161744304344\n",
      "Iteration: 176, training loss: 0.5241423769142373\n",
      "Iteration: 177, training loss: 0.523870158088063\n",
      "Iteration: 178, training loss: 0.5235994942718946\n",
      "Iteration: 179, training loss: 0.5233303622950636\n",
      "Iteration: 180, training loss: 0.5230627394840871\n",
      "Iteration: 181, training loss: 0.5227966036508139\n",
      "Iteration: 182, training loss: 0.522531933080873\n",
      "Iteration: 183, training loss: 0.5222687065224121\n",
      "Iteration: 184, training loss: 0.5220069031751248\n",
      "Iteration: 185, training loss: 0.5217465026795528\n",
      "Iteration: 186, training loss: 0.5214874851066577\n",
      "Iteration: 187, training loss: 0.521229830947657\n",
      "Iteration: 188, training loss: 0.5209735211041148\n",
      "Iteration: 189, training loss: 0.5207185368782808\n",
      "Iteration: 190, training loss: 0.5204648599636724\n",
      "Iteration: 191, training loss: 0.5202124724358919\n",
      "Iteration: 192, training loss: 0.5199613567436726\n",
      "Iteration: 193, training loss: 0.5197114957001495\n",
      "Iteration: 194, training loss: 0.5194628724743465\n",
      "Iteration: 195, training loss: 0.5192154705828749\n",
      "Iteration: 196, training loss: 0.5189692738818388\n",
      "Iteration: 197, training loss: 0.5187242665589402\n",
      "Iteration: 198, training loss: 0.518480433125781\n",
      "Iteration: 199, training loss: 0.5182377584103525\n",
      "Iteration: 200, training loss: 0.5179962275497131\n",
      "Iteration: 201, training loss: 0.5177558259828442\n",
      "Iteration: 202, training loss: 0.5175165394436831\n",
      "Iteration: 203, training loss: 0.5172783539543255\n",
      "Iteration: 204, training loss: 0.5170412558183962\n",
      "Iteration: 205, training loss: 0.516805231614581\n",
      "Iteration: 206, training loss: 0.516570268190317\n",
      "Iteration: 207, training loss: 0.5163363526556366\n",
      "Iteration: 208, training loss: 0.5161034723771629\n",
      "Iteration: 209, training loss: 0.5158716149722478\n",
      "Iteration: 210, training loss: 0.515640768303257\n",
      "Iteration: 211, training loss: 0.5154109204719899\n",
      "Iteration: 212, training loss: 0.5151820598142366\n",
      "Iteration: 213, training loss: 0.5149541748944654\n",
      "Iteration: 214, training loss: 0.5147272545006403\n",
      "Iteration: 215, training loss: 0.514501287639161\n",
      "Iteration: 216, training loss: 0.5142762635299274\n",
      "Iteration: 217, training loss: 0.5140521716015203\n",
      "Iteration: 218, training loss: 0.5138290014864996\n",
      "Iteration: 219, training loss: 0.5136067430168146\n",
      "Iteration: 220, training loss: 0.5133853862193241\n",
      "Iteration: 221, training loss: 0.5131649213114233\n",
      "Iteration: 222, training loss: 0.5129453386967764\n",
      "Iteration: 223, training loss: 0.5127266289611488\n",
      "Iteration: 224, training loss: 0.5125087828683412\n",
      "Iteration: 225, training loss: 0.5122917913562177\n",
      "Iteration: 226, training loss: 0.5120756455328301\n",
      "Iteration: 227, training loss: 0.5118603366726333\n",
      "Iteration: 228, training loss: 0.5116458562127908\n",
      "Iteration: 229, training loss: 0.5114321957495663\n",
      "Iteration: 230, training loss: 0.5112193470348025\n",
      "Iteration: 231, training loss: 0.5110073019724819\n",
      "Iteration: 232, training loss: 0.5107960526153671\n",
      "Iteration: 233, training loss: 0.5105855911617236\n",
      "Iteration: 234, training loss: 0.5103759099521166\n",
      "Iteration: 235, training loss: 0.5101670014662839\n",
      "Iteration: 236, training loss: 0.5099588583200828\n",
      "Iteration: 237, training loss: 0.509751473262507\n",
      "Iteration: 238, training loss: 0.5095448391727748\n",
      "Iteration: 239, training loss: 0.5093389490574839\n",
      "Iteration: 240, training loss: 0.5091337960478334\n",
      "Iteration: 241, training loss: 0.5089293733969101\n",
      "Iteration: 242, training loss: 0.5087256744770384\n",
      "Iteration: 243, training loss: 0.5085226927771908\n",
      "Iteration: 244, training loss: 0.5083204219004591\n",
      "Iteration: 245, training loss: 0.5081188555615848\n",
      "Iteration: 246, training loss: 0.507917987584545\n",
      "Iteration: 247, training loss: 0.5077178119001959\n",
      "Iteration: 248, training loss: 0.5075183225439696\n",
      "Iteration: 249, training loss: 0.5073195136536237\n",
      "Iteration: 250, training loss: 0.5071213794670446\n",
      "Iteration: 251, training loss: 0.5069239143200986\n",
      "Iteration: 252, training loss: 0.5067271126445351\n",
      "Iteration: 253, training loss: 0.5065309689659354\n",
      "Iteration: 254, training loss: 0.5063354779017107\n",
      "Iteration: 255, training loss: 0.506140634159145\n",
      "Iteration: 256, training loss: 0.505946432533482\n",
      "Iteration: 257, training loss: 0.5057528679060569\n",
      "Iteration: 258, training loss: 0.5055599352424707\n",
      "Iteration: 259, training loss: 0.5053676295908052\n",
      "Iteration: 260, training loss: 0.5051759460798797\n",
      "Iteration: 261, training loss: 0.5049848799175466\n",
      "Iteration: 262, training loss: 0.5047944263890263\n",
      "Iteration: 263, training loss: 0.5046045808552792\n",
      "Iteration: 264, training loss: 0.5044153387514149\n",
      "Iteration: 265, training loss: 0.5042266955851386\n",
      "Iteration: 266, training loss: 0.5040386469352297\n",
      "Iteration: 267, training loss: 0.5038511884500586\n",
      "Iteration: 268, training loss: 0.5036643158461334\n",
      "Iteration: 269, training loss: 0.5034780249066817\n",
      "Iteration: 270, training loss: 0.5032923114802635\n",
      "Iteration: 271, training loss: 0.5031071714794153\n",
      "Iteration: 272, training loss: 0.5029226008793249\n",
      "Iteration: 273, training loss: 0.5027385957165356\n",
      "Iteration: 274, training loss: 0.5025551520876796\n",
      "Iteration: 275, training loss: 0.5023722661482405\n",
      "Iteration: 276, training loss: 0.502189934111342\n",
      "Iteration: 277, training loss: 0.5020081522465648\n",
      "Iteration: 278, training loss: 0.5018269168787899\n",
      "Iteration: 279, training loss: 0.5016462243870673\n",
      "Iteration: 280, training loss: 0.5014660712035095\n",
      "Iteration: 281, training loss: 0.5012864538122115\n",
      "Iteration: 282, training loss: 0.5011073687481918\n",
      "Iteration: 283, training loss: 0.5009288125963604\n",
      "Iteration: 284, training loss: 0.5007507819905063\n",
      "Iteration: 285, training loss: 0.5005732736123105\n",
      "Iteration: 286, training loss: 0.5003962841903785\n",
      "Iteration: 287, training loss: 0.5002198104992956\n",
      "Iteration: 288, training loss: 0.5000438493587027\n",
      "Iteration: 289, training loss: 0.4998683976323921\n",
      "Iteration: 290, training loss: 0.49969345222742434\n",
      "Iteration: 291, training loss: 0.4995190100932629\n",
      "Iteration: 292, training loss: 0.4993450682209295\n",
      "Iteration: 293, training loss: 0.49917162364217726\n",
      "Iteration: 294, training loss: 0.49899867342868176\n",
      "Iteration: 295, training loss: 0.49882621469125055\n",
      "Iteration: 296, training loss: 0.4986542445790494\n",
      "Iteration: 297, training loss: 0.49848276027884625\n",
      "Iteration: 298, training loss: 0.49831175901427055\n",
      "Iteration: 299, training loss: 0.4981412380450902\n",
      "Iteration: 300, training loss: 0.49797119466650275\n",
      "Iteration: 301, training loss: 0.49780162620844376\n",
      "Iteration: 302, training loss: 0.4976325300349087\n",
      "Iteration: 303, training loss: 0.4974639035432908\n",
      "Iteration: 304, training loss: 0.4972957441637325\n",
      "Iteration: 305, training loss: 0.4971280493584914\n",
      "Iteration: 306, training loss: 0.49696081662131997\n",
      "Iteration: 307, training loss: 0.49679404347685885\n",
      "Iteration: 308, training loss: 0.4966277274800426\n",
      "Iteration: 309, training loss: 0.49646186621551913\n",
      "Iteration: 310, training loss: 0.49629645729708183\n",
      "Iteration: 311, training loss: 0.4961314983671129\n",
      "Iteration: 312, training loss: 0.4959669870960396\n",
      "Iteration: 313, training loss: 0.49580292118180236\n",
      "Iteration: 314, training loss: 0.49563929834933323\n",
      "Iteration: 315, training loss: 0.4954761163500472\n",
      "Iteration: 316, training loss: 0.49531337296134287\n",
      "Iteration: 317, training loss: 0.49515106598611525\n",
      "Iteration: 318, training loss: 0.4949891932522777\n",
      "Iteration: 319, training loss: 0.49482775261229545\n",
      "Iteration: 320, training loss: 0.49466674194272775\n",
      "Iteration: 321, training loss: 0.4945061591437813\n",
      "Iteration: 322, training loss: 0.49434600213887175\n",
      "Iteration: 323, training loss: 0.494186268874196\n",
      "Iteration: 324, training loss: 0.49402695731831203\n",
      "Iteration: 325, training loss: 0.4938680654617297\n",
      "Iteration: 326, training loss: 0.4937095913165084\n",
      "Iteration: 327, training loss: 0.49355153291586373\n",
      "Iteration: 328, training loss: 0.49339388831378406\n",
      "Iteration: 329, training loss: 0.493236655584653\n",
      "Iteration: 330, training loss: 0.49307983282288087\n",
      "Iteration: 331, training loss: 0.4929234181425449\n",
      "Iteration: 332, training loss: 0.49276740967703525\n",
      "Iteration: 333, training loss: 0.49261180557871\n",
      "Iteration: 334, training loss: 0.49245660401855657\n",
      "Iteration: 335, training loss: 0.49230180318586086\n",
      "Iteration: 336, training loss: 0.4921474012878829\n",
      "Iteration: 337, training loss: 0.4919933965495398\n",
      "Iteration: 338, training loss: 0.49183978721309485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 339, training loss: 0.49168657153785356\n",
      "Iteration: 340, training loss: 0.49153374779986597\n",
      "Iteration: 341, training loss: 0.4913813142916353\n",
      "Iteration: 342, training loss: 0.4912292693218324\n",
      "Iteration: 343, training loss: 0.4910776112150165\n",
      "Iteration: 344, training loss: 0.49092633831136173\n",
      "Iteration: 345, training loss: 0.4907754489663894\n",
      "Iteration: 346, training loss: 0.49062494155070546\n",
      "Iteration: 347, training loss: 0.4904748144497443\n",
      "Iteration: 348, training loss: 0.4903250660635167\n",
      "Iteration: 349, training loss: 0.49017569480636425\n",
      "Iteration: 350, training loss: 0.49002669910671826\n",
      "Iteration: 351, training loss: 0.489878077406863\n",
      "Iteration: 352, training loss: 0.4897298281627055\n",
      "Iteration: 353, training loss: 0.48958194984354897\n",
      "Iteration: 354, training loss: 0.48943444093187055\n",
      "Iteration: 355, training loss: 0.4892872999231048\n",
      "Iteration: 356, training loss: 0.4891405253254311\n",
      "Iteration: 357, training loss: 0.488994115659565\n",
      "Iteration: 358, training loss: 0.48884806945855475\n",
      "Iteration: 359, training loss: 0.48870238526758114\n",
      "Iteration: 360, training loss: 0.4885570616437623\n",
      "Iteration: 361, training loss: 0.48841209715596173\n",
      "Iteration: 362, training loss: 0.4882674903846007\n",
      "Iteration: 363, training loss: 0.48812323992147444\n",
      "Iteration: 364, training loss: 0.48797934436957224\n",
      "Iteration: 365, training loss: 0.4878358023429004\n",
      "Iteration: 366, training loss: 0.4876926124663102\n",
      "Iteration: 367, training loss: 0.48754977337532807\n",
      "Iteration: 368, training loss: 0.48740728371598974\n",
      "Iteration: 369, training loss: 0.48726514214467814\n",
      "Iteration: 370, training loss: 0.48712334732796375\n",
      "Iteration: 371, training loss: 0.486981897942449\n",
      "Iteration: 372, training loss: 0.48684079267461516\n",
      "Iteration: 373, training loss: 0.4867000302206726\n",
      "Iteration: 374, training loss: 0.48655960928641445\n",
      "Iteration: 375, training loss: 0.4864195285870724\n",
      "Iteration: 376, training loss: 0.4862797868471761\n",
      "Iteration: 377, training loss: 0.486140382800415\n",
      "Iteration: 378, training loss: 0.4860013151895031\n",
      "Iteration: 379, training loss: 0.4858625827660465\n",
      "Iteration: 380, training loss: 0.4857241842904132\n",
      "Iteration: 381, training loss: 0.48558611853160616\n",
      "Iteration: 382, training loss: 0.4854483842671382\n",
      "Iteration: 383, training loss: 0.48531098028290987\n",
      "Iteration: 384, training loss: 0.4851739053730895\n",
      "Iteration: 385, training loss: 0.4850371583399957\n",
      "Iteration: 386, training loss: 0.4849007379939823\n",
      "Iteration: 387, training loss: 0.48476464315332535\n",
      "Iteration: 388, training loss: 0.48462887264411236\n",
      "Iteration: 389, training loss: 0.4844934253001341\n",
      "Iteration: 390, training loss: 0.48435829996277807\n",
      "Iteration: 391, training loss: 0.48422349548092414\n",
      "Iteration: 392, training loss: 0.48408901071084265\n",
      "Iteration: 393, training loss: 0.4839548445160938\n",
      "Iteration: 394, training loss: 0.4838209957674295\n",
      "Iteration: 395, training loss: 0.48368746334269724\n",
      "Iteration: 396, training loss: 0.4835542461267456\n",
      "Iteration: 397, training loss: 0.48342134301133116\n",
      "Iteration: 398, training loss: 0.4832887528950282\n",
      "Iteration: 399, training loss: 0.4831564746831392\n",
      "Iteration: 400, training loss: 0.48302450728760793\n",
      "Iteration: 401, training loss: 0.48289284962693324\n",
      "Iteration: 402, training loss: 0.48276150062608536\n",
      "Iteration: 403, training loss: 0.48263045921642334\n",
      "Iteration: 404, training loss: 0.4824997243356145\n",
      "Iteration: 405, training loss: 0.4823692949275545\n",
      "Iteration: 406, training loss: 0.4822391699422903\n",
      "Iteration: 407, training loss: 0.48210934833594316\n",
      "Iteration: 408, training loss: 0.48197982907063436\n",
      "Iteration: 409, training loss: 0.4818506111144116\n",
      "Iteration: 410, training loss: 0.4817216934411763\n",
      "Iteration: 411, training loss: 0.4815930750306141\n",
      "Iteration: 412, training loss: 0.4814647548681244\n",
      "Iteration: 413, training loss: 0.4813367319447531\n",
      "Iteration: 414, training loss: 0.4812090052571251\n",
      "Iteration: 415, training loss: 0.48108157380737937\n",
      "Iteration: 416, training loss: 0.48095443660310416\n",
      "Iteration: 417, training loss: 0.4808275926572744\n",
      "Iteration: 418, training loss: 0.4807010409881893\n",
      "Iteration: 419, training loss: 0.4805747806194114\n",
      "Iteration: 420, training loss: 0.4804488105797075\n",
      "Iteration: 421, training loss: 0.48032312990298964\n",
      "Iteration: 422, training loss: 0.48019773762825774\n",
      "Iteration: 423, training loss: 0.4800726327995429\n",
      "Iteration: 424, training loss: 0.4799478144658522\n",
      "Iteration: 425, training loss: 0.4798232816811145\n",
      "Iteration: 426, training loss: 0.4796990335041263\n",
      "Iteration: 427, training loss: 0.47957506899849994\n",
      "Iteration: 428, training loss: 0.479451387232612\n",
      "Iteration: 429, training loss: 0.4793279872795521\n",
      "Iteration: 430, training loss: 0.4792048682170742\n",
      "Iteration: 431, training loss: 0.47908202912754694\n",
      "Iteration: 432, training loss: 0.47895946909790593\n",
      "Iteration: 433, training loss: 0.4788371872196072\n",
      "Iteration: 434, training loss: 0.47871518258858037\n",
      "Iteration: 435, training loss: 0.47859345430518324\n",
      "Iteration: 436, training loss: 0.47847200147415747\n",
      "Iteration: 437, training loss: 0.4783508232045847\n",
      "Iteration: 438, training loss: 0.4782299186098432\n",
      "Iteration: 439, training loss: 0.478109286807566\n",
      "Iteration: 440, training loss: 0.4779889269195987\n",
      "Iteration: 441, training loss: 0.4778688380719591\n",
      "Iteration: 442, training loss: 0.4777490193947969\n",
      "Iteration: 443, training loss: 0.47762947002235445\n",
      "Iteration: 444, training loss: 0.47751018909292714\n",
      "Iteration: 445, training loss: 0.4773911757488264\n",
      "Iteration: 446, training loss: 0.47727242913634144\n",
      "Iteration: 447, training loss: 0.4771539484057026\n",
      "Iteration: 448, training loss: 0.47703573271104505\n",
      "Iteration: 449, training loss: 0.4769177812103738\n",
      "Iteration: 450, training loss: 0.47680009306552773\n",
      "Iteration: 451, training loss: 0.4766826674421456\n",
      "Iteration: 452, training loss: 0.4765655035096323\n",
      "Iteration: 453, training loss: 0.4764486004411254\n",
      "Iteration: 454, training loss: 0.4763319574134626\n",
      "Iteration: 455, training loss: 0.47621557360714917\n",
      "Iteration: 456, training loss: 0.47609944820632655\n",
      "Iteration: 457, training loss: 0.47598358039874106\n",
      "Iteration: 458, training loss: 0.47586796937571324\n",
      "Iteration: 459, training loss: 0.47575261433210786\n",
      "Iteration: 460, training loss: 0.4756375144663039\n",
      "Iteration: 461, training loss: 0.4755226689801657\n",
      "Iteration: 462, training loss: 0.47540807707901406\n",
      "Iteration: 463, training loss: 0.4752937379715978\n",
      "Iteration: 464, training loss: 0.47517965087006625\n",
      "Iteration: 465, training loss: 0.4750658149899417\n",
      "Iteration: 466, training loss: 0.4749522295500924\n",
      "Iteration: 467, training loss: 0.47483889377270583\n",
      "Iteration: 468, training loss: 0.47472580688326305\n",
      "Iteration: 469, training loss: 0.4746129681105127\n",
      "Iteration: 470, training loss: 0.4745003766864453\n",
      "Iteration: 471, training loss: 0.47438803184626904\n",
      "Iteration: 472, training loss: 0.4742759328283847\n",
      "Iteration: 473, training loss: 0.47416407887436157\n",
      "Iteration: 474, training loss: 0.47405246922891353\n",
      "Iteration: 475, training loss: 0.47394110313987586\n",
      "Iteration: 476, training loss: 0.47382997985818187\n",
      "Iteration: 477, training loss: 0.4737190986378401\n",
      "Iteration: 478, training loss: 0.4736084587359121\n",
      "Iteration: 479, training loss: 0.47349805941248996\n",
      "Iteration: 480, training loss: 0.4733878999306749\n",
      "Iteration: 481, training loss: 0.47327797955655565\n",
      "Iteration: 482, training loss: 0.47316829755918705\n",
      "Iteration: 483, training loss: 0.47305885321056956\n",
      "Iteration: 484, training loss: 0.4729496457856286\n",
      "Iteration: 485, training loss: 0.472840674562194\n",
      "Iteration: 486, training loss: 0.47273193882098047\n",
      "Iteration: 487, training loss: 0.4726234378455673\n",
      "Iteration: 488, training loss: 0.4725151709223797\n",
      "Iteration: 489, training loss: 0.47240713734066897\n",
      "Iteration: 490, training loss: 0.4722993363924941\n",
      "Iteration: 491, training loss: 0.4721917673727025\n",
      "Iteration: 492, training loss: 0.47208442957891233\n",
      "Iteration: 493, training loss: 0.471977322311494\n",
      "Iteration: 494, training loss: 0.47187044487355234\n",
      "Iteration: 495, training loss: 0.4717637965709089\n",
      "Iteration: 496, training loss: 0.47165737671208463\n",
      "Iteration: 497, training loss: 0.47155118460828266\n",
      "Iteration: 498, training loss: 0.47144521957337127\n",
      "Iteration: 499, training loss: 0.47133948092386746\n",
      "Iteration: 500, training loss: 0.4712339679789199\n",
      "Iteration: 501, training loss: 0.4711286800602931\n",
      "Iteration: 502, training loss: 0.4710236164923513\n",
      "Iteration: 503, training loss: 0.470918776602042\n",
      "Iteration: 504, training loss: 0.470814159718881\n",
      "Iteration: 505, training loss: 0.47070976517493673\n",
      "Iteration: 506, training loss: 0.47060559230481447\n",
      "Iteration: 507, training loss: 0.4705016404456417\n",
      "Iteration: 508, training loss: 0.47039790893705336\n",
      "Iteration: 509, training loss: 0.4702943971211765\n",
      "Iteration: 510, training loss: 0.47019110434261613\n",
      "Iteration: 511, training loss: 0.47008802994844096\n",
      "Iteration: 512, training loss: 0.46998517328816863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 513, training loss: 0.4698825337137523\n",
      "Iteration: 514, training loss: 0.4697801105795663\n",
      "Iteration: 515, training loss: 0.4696779032423929\n",
      "Iteration: 516, training loss: 0.469575911061408\n",
      "Iteration: 517, training loss: 0.46947413339816857\n",
      "Iteration: 518, training loss: 0.46937256961659896\n",
      "Iteration: 519, training loss: 0.4692712190829778\n",
      "Iteration: 520, training loss: 0.4691700811659251\n",
      "Iteration: 521, training loss: 0.46906915523638976\n",
      "Iteration: 522, training loss: 0.46896844066763615\n",
      "Iteration: 523, training loss: 0.4688679368352327\n",
      "Iteration: 524, training loss: 0.4687676431170384\n",
      "Iteration: 525, training loss: 0.4686675588931913\n",
      "Iteration: 526, training loss: 0.468567683546096\n",
      "Iteration: 527, training loss: 0.4684680164604123\n",
      "Iteration: 528, training loss: 0.46836855702304225\n",
      "Iteration: 529, training loss: 0.4682693046231196\n",
      "Iteration: 530, training loss: 0.4681702586519974\n",
      "Iteration: 531, training loss: 0.46807141850323664\n",
      "Iteration: 532, training loss: 0.4679727835725954\n",
      "Iteration: 533, training loss: 0.46787435325801685\n",
      "Iteration: 534, training loss: 0.4677761269596189\n",
      "Iteration: 535, training loss: 0.46767810407968247\n",
      "Iteration: 536, training loss: 0.46758028402264107\n",
      "Iteration: 537, training loss: 0.46748266619506956\n",
      "Iteration: 538, training loss: 0.4673852500056742\n",
      "Iteration: 539, training loss: 0.46728803486528125\n",
      "Iteration: 540, training loss: 0.46719102018682684\n",
      "Iteration: 541, training loss: 0.46709420538534674\n",
      "Iteration: 542, training loss: 0.4669975898779658\n",
      "Iteration: 543, training loss: 0.46690117308388784\n",
      "Iteration: 544, training loss: 0.4668049544243858\n",
      "Iteration: 545, training loss: 0.4667089333227914\n",
      "Iteration: 546, training loss: 0.46661310920448534\n",
      "Iteration: 547, training loss: 0.46651748149688776\n",
      "Iteration: 548, training loss: 0.46642204962944817\n",
      "Iteration: 549, training loss: 0.4663268130336358\n",
      "Iteration: 550, training loss: 0.4662317711429306\n",
      "Iteration: 551, training loss: 0.4661369233928129\n",
      "Iteration: 552, training loss: 0.46604226922075503\n",
      "Iteration: 553, training loss: 0.4659478080662113\n",
      "Iteration: 554, training loss: 0.4658535393706088\n",
      "Iteration: 555, training loss: 0.46575946257733875\n",
      "Iteration: 556, training loss: 0.4656655771317473\n",
      "Iteration: 557, training loss: 0.46557188248112585\n",
      "Iteration: 558, training loss: 0.46547837807470366\n",
      "Iteration: 559, training loss: 0.4653850633636372\n",
      "Iteration: 560, training loss: 0.465291937801003\n",
      "Iteration: 561, training loss: 0.46519900084178767\n",
      "Iteration: 562, training loss: 0.46510625194288036\n",
      "Iteration: 563, training loss: 0.46501369056306363\n",
      "Iteration: 564, training loss: 0.4649213161630049\n",
      "Iteration: 565, training loss: 0.4648291282052486\n",
      "Iteration: 566, training loss: 0.4647371261542073\n",
      "Iteration: 567, training loss: 0.46464530947615373\n",
      "Iteration: 568, training loss: 0.4645536776392123\n",
      "Iteration: 569, training loss: 0.4644622301133513\n",
      "Iteration: 570, training loss: 0.4643709663703748\n",
      "Iteration: 571, training loss: 0.46427988588391417\n",
      "Iteration: 572, training loss: 0.4641889881294209\n",
      "Iteration: 573, training loss: 0.4640982725841577\n",
      "Iteration: 574, training loss: 0.46400773872719187\n",
      "Iteration: 575, training loss: 0.4639173860393863\n",
      "Iteration: 576, training loss: 0.46382721400339294\n",
      "Iteration: 577, training loss: 0.4637372221036439\n",
      "Iteration: 578, training loss: 0.4636474098263449\n",
      "Iteration: 579, training loss: 0.46355777665946724\n",
      "Iteration: 580, training loss: 0.4634683220927402\n",
      "Iteration: 581, training loss: 0.4633790456176439\n",
      "Iteration: 582, training loss: 0.46328994672740176\n",
      "Iteration: 583, training loss: 0.463201024916973\n",
      "Iteration: 584, training loss: 0.46311227968304575\n",
      "Iteration: 585, training loss: 0.46302371052402924\n",
      "Iteration: 586, training loss: 0.4629353169400473\n",
      "Iteration: 587, training loss: 0.4628470984329306\n",
      "Iteration: 588, training loss: 0.4627590545062098\n",
      "Iteration: 589, training loss: 0.46267118466510876\n",
      "Iteration: 590, training loss: 0.46258348841653707\n",
      "Iteration: 591, training loss: 0.46249596526908326\n",
      "Iteration: 592, training loss: 0.4624086147330083\n",
      "Iteration: 593, training loss: 0.462321436320238\n",
      "Iteration: 594, training loss: 0.46223442954435684\n",
      "Iteration: 595, training loss: 0.46214759392060073\n",
      "Iteration: 596, training loss: 0.46206092896585066\n",
      "Iteration: 597, training loss: 0.4619744341986258\n",
      "Iteration: 598, training loss: 0.4618881091390769\n",
      "Iteration: 599, training loss: 0.46180195330897955\n",
      "Iteration: 600, training loss: 0.4617159662317277\n",
      "Iteration: 601, training loss: 0.46163014743232766\n",
      "Iteration: 602, training loss: 0.4615444964373905\n",
      "Iteration: 603, training loss: 0.46145901277512646\n",
      "Iteration: 604, training loss: 0.46137369597533845\n",
      "Iteration: 605, training loss: 0.46128854556941545\n",
      "Iteration: 606, training loss: 0.46120356109032634\n",
      "Iteration: 607, training loss: 0.4611187420726135\n",
      "Iteration: 608, training loss: 0.46103408805238644\n",
      "Iteration: 609, training loss: 0.4609495985673161\n",
      "Iteration: 610, training loss: 0.46086527315662806\n",
      "Iteration: 611, training loss: 0.46078111136109673\n",
      "Iteration: 612, training loss: 0.46069711272303904\n",
      "Iteration: 613, training loss: 0.46061327678630865\n",
      "Iteration: 614, training loss: 0.46052960309628965\n",
      "Iteration: 615, training loss: 0.4604460911998906\n",
      "Iteration: 616, training loss: 0.46036274064553867\n",
      "Iteration: 617, training loss: 0.4602795509831737\n",
      "Iteration: 618, training loss: 0.460196521764242\n",
      "Iteration: 619, training loss: 0.4601136525416907\n",
      "Iteration: 620, training loss: 0.4600309428699621\n",
      "Iteration: 621, training loss: 0.4599483923049875\n",
      "Iteration: 622, training loss: 0.45986600040418163\n",
      "Iteration: 623, training loss: 0.4597837667264369\n",
      "Iteration: 624, training loss: 0.45970169083211737\n",
      "Iteration: 625, training loss: 0.4596197722830537\n",
      "Iteration: 626, training loss: 0.4595380106425367\n",
      "Iteration: 627, training loss: 0.4594564054753124\n",
      "Iteration: 628, training loss: 0.4593749563475762\n",
      "Iteration: 629, training loss: 0.4592936628269669\n",
      "Iteration: 630, training loss: 0.4592125244825619\n",
      "Iteration: 631, training loss: 0.4591315408848712\n",
      "Iteration: 632, training loss: 0.45905071160583183\n",
      "Iteration: 633, training loss: 0.45897003621880306\n",
      "Iteration: 634, training loss: 0.45888951429855984\n",
      "Iteration: 635, training loss: 0.45880914542128864\n",
      "Iteration: 636, training loss: 0.45872892916458125\n",
      "Iteration: 637, training loss: 0.45864886510742964\n",
      "Iteration: 638, training loss: 0.45856895283022076\n",
      "Iteration: 639, training loss: 0.45848919191473125\n",
      "Iteration: 640, training loss: 0.45840958194412196\n",
      "Iteration: 641, training loss: 0.45833012250293287\n",
      "Iteration: 642, training loss: 0.4582508131770779\n",
      "Iteration: 643, training loss: 0.4581716535538395\n",
      "Iteration: 644, training loss: 0.45809264322186394\n",
      "Iteration: 645, training loss: 0.45801378177115554\n",
      "Iteration: 646, training loss: 0.4579350687930722\n",
      "Iteration: 647, training loss: 0.4578565038803198\n",
      "Iteration: 648, training loss: 0.45777808662694747\n",
      "Iteration: 649, training loss: 0.4576998166283423\n",
      "Iteration: 650, training loss: 0.45762169348122456\n",
      "Iteration: 651, training loss: 0.4575437167836424\n",
      "Iteration: 652, training loss: 0.4574658861349673\n",
      "Iteration: 653, training loss: 0.4573882011358885\n",
      "Iteration: 654, training loss: 0.45731066138840903\n",
      "Iteration: 655, training loss: 0.4572332664958397\n",
      "Iteration: 656, training loss: 0.45715601606279493\n",
      "Iteration: 657, training loss: 0.45707890969518783\n",
      "Iteration: 658, training loss: 0.45700194700022523\n",
      "Iteration: 659, training loss: 0.45692512758640275\n",
      "Iteration: 660, training loss: 0.45684845106350014\n",
      "Iteration: 661, training loss: 0.45677191704257664\n",
      "Iteration: 662, training loss: 0.4566955251359662\n",
      "Iteration: 663, training loss: 0.45661927495727234\n",
      "Iteration: 664, training loss: 0.4565431661213641\n",
      "Iteration: 665, training loss: 0.4564671982443709\n",
      "Iteration: 666, training loss: 0.45639137094367793\n",
      "Iteration: 667, training loss: 0.45631568383792154\n",
      "Iteration: 668, training loss: 0.4562401365469849\n",
      "Iteration: 669, training loss: 0.45616472869199304\n",
      "Iteration: 670, training loss: 0.4560894598953083\n",
      "Iteration: 671, training loss: 0.45601432978052586\n",
      "Iteration: 672, training loss: 0.4559393379724694\n",
      "Iteration: 673, training loss: 0.4558644840971861\n",
      "Iteration: 674, training loss: 0.45578976778194247\n",
      "Iteration: 675, training loss: 0.45571518865522026\n",
      "Iteration: 676, training loss: 0.45564074634671076\n",
      "Iteration: 677, training loss: 0.4555664404873117\n",
      "Iteration: 678, training loss: 0.4554922707091222\n",
      "Iteration: 679, training loss: 0.4554182366454382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 680, training loss: 0.45534433793074863\n",
      "Iteration: 681, training loss: 0.4552705742007306\n",
      "Iteration: 682, training loss: 0.455196945092245\n",
      "Iteration: 683, training loss: 0.45512345024333273\n",
      "Iteration: 684, training loss: 0.4550500892932095\n",
      "Iteration: 685, training loss: 0.4549768618822625\n",
      "Iteration: 686, training loss: 0.4549037676520455\n",
      "Iteration: 687, training loss: 0.4548308062452746\n",
      "Iteration: 688, training loss: 0.45475797730582446\n",
      "Iteration: 689, training loss: 0.45468528047872336\n",
      "Iteration: 690, training loss: 0.45461271541014986\n",
      "Iteration: 691, training loss: 0.45454028174742794\n",
      "Iteration: 692, training loss: 0.45446797913902304\n",
      "Iteration: 693, training loss: 0.45439580723453776\n",
      "Iteration: 694, training loss: 0.45432376568470834\n",
      "Iteration: 695, training loss: 0.4542518541413997\n",
      "Iteration: 696, training loss: 0.45418007225760176\n",
      "Iteration: 697, training loss: 0.4541084196874254\n",
      "Iteration: 698, training loss: 0.45403689608609843\n",
      "Iteration: 699, training loss: 0.4539655011099613\n",
      "Iteration: 700, training loss: 0.45389423441646315\n",
      "Iteration: 701, training loss: 0.4538230956641579\n",
      "Iteration: 702, training loss: 0.45375208451270027\n",
      "Iteration: 703, training loss: 0.4536812006228417\n",
      "Iteration: 704, training loss: 0.4536104436564261\n",
      "Iteration: 705, training loss: 0.4535398132763867\n",
      "Iteration: 706, training loss: 0.4534693091467413\n",
      "Iteration: 707, training loss: 0.45339893093258843\n",
      "Iteration: 708, training loss: 0.4533286783001044\n",
      "Iteration: 709, training loss: 0.453258550916538\n",
      "Iteration: 710, training loss: 0.45318854845020756\n",
      "Iteration: 711, training loss: 0.4531186705704971\n",
      "Iteration: 712, training loss: 0.4530489169478517\n",
      "Iteration: 713, training loss: 0.4529792872537748\n",
      "Iteration: 714, training loss: 0.45290978116082337\n",
      "Iteration: 715, training loss: 0.45284039834260487\n",
      "Iteration: 716, training loss: 0.452771138473773\n",
      "Iteration: 717, training loss: 0.4527020012300239\n",
      "Iteration: 718, training loss: 0.45263298628809295\n",
      "Iteration: 719, training loss: 0.4525640933257505\n",
      "Iteration: 720, training loss: 0.4524953220217984\n",
      "Iteration: 721, training loss: 0.45242667205606596\n",
      "Iteration: 722, training loss: 0.4523581431094069\n",
      "Iteration: 723, training loss: 0.4522897348636951\n",
      "Iteration: 724, training loss: 0.4522214470018212\n",
      "Iteration: 725, training loss: 0.45215327920768866\n",
      "Iteration: 726, training loss: 0.45208523116621074\n",
      "Iteration: 727, training loss: 0.45201730256330624\n",
      "Iteration: 728, training loss: 0.451949493085896\n",
      "Iteration: 729, training loss: 0.45188180242189996\n",
      "Iteration: 730, training loss: 0.45181423026023276\n",
      "Iteration: 731, training loss: 0.45174677629080034\n",
      "Iteration: 732, training loss: 0.45167944020449713\n",
      "Iteration: 733, training loss: 0.45161222169320125\n",
      "Iteration: 734, training loss: 0.45154512044977224\n",
      "Iteration: 735, training loss: 0.4514781361680468\n",
      "Iteration: 736, training loss: 0.45141126854283536\n",
      "Iteration: 737, training loss: 0.451344517269919\n",
      "Iteration: 738, training loss: 0.4512778820460457\n",
      "Iteration: 739, training loss: 0.4512113625689267\n",
      "Iteration: 740, training loss: 0.45114495853723346\n",
      "Iteration: 741, training loss: 0.451078669650594\n",
      "Iteration: 742, training loss: 0.45101249560958956\n",
      "Iteration: 743, training loss: 0.4509464361157512\n",
      "Iteration: 744, training loss: 0.4508804908715564\n",
      "Iteration: 745, training loss: 0.4508146595804257\n",
      "Iteration: 746, training loss: 0.4507489419467192\n",
      "Iteration: 747, training loss: 0.4506833376757336\n",
      "Iteration: 748, training loss: 0.45061784647369846\n",
      "Iteration: 749, training loss: 0.450552468047773\n",
      "Iteration: 750, training loss: 0.4504872021060429\n",
      "Iteration: 751, training loss: 0.4504220483575169\n",
      "Iteration: 752, training loss: 0.4503570065121236\n",
      "Iteration: 753, training loss: 0.4502920762807083\n",
      "Iteration: 754, training loss: 0.45022725737502906\n",
      "Iteration: 755, training loss: 0.4501625495077546\n",
      "Iteration: 756, training loss: 0.4500979523924601\n",
      "Iteration: 757, training loss: 0.45003346574362446\n",
      "Iteration: 758, training loss: 0.4499690892766269\n",
      "Iteration: 759, training loss: 0.4499048227077439\n",
      "Iteration: 760, training loss: 0.449840665754146\n",
      "Iteration: 761, training loss: 0.44977661813389447\n",
      "Iteration: 762, training loss: 0.44971267956593836\n",
      "Iteration: 763, training loss: 0.4496488497701112\n",
      "Iteration: 764, training loss: 0.44958512846712806\n",
      "Iteration: 765, training loss: 0.44952151537858215\n",
      "Iteration: 766, training loss: 0.4494580102269418\n",
      "Iteration: 767, training loss: 0.44939461273554776\n",
      "Iteration: 768, training loss: 0.4493313226286094\n",
      "Iteration: 769, training loss: 0.44926813963120227\n",
      "Iteration: 770, training loss: 0.4492050634692646\n",
      "Iteration: 771, training loss: 0.44914209386959475\n",
      "Iteration: 772, training loss: 0.44907923055984744\n",
      "Iteration: 773, training loss: 0.44901647326853134\n",
      "Iteration: 774, training loss: 0.448953821725006\n",
      "Iteration: 775, training loss: 0.4488912756594785\n",
      "Iteration: 776, training loss: 0.4488288348030006\n",
      "Iteration: 777, training loss: 0.448766498887466\n",
      "Iteration: 778, training loss: 0.44870426764560734\n",
      "Iteration: 779, training loss: 0.4486421408109926\n",
      "Iteration: 780, training loss: 0.44858011811802295\n",
      "Iteration: 781, training loss: 0.4485181993019296\n",
      "Iteration: 782, training loss: 0.44845638409877064\n",
      "Iteration: 783, training loss: 0.4483946722454282\n",
      "Iteration: 784, training loss: 0.44833306347960583\n",
      "Iteration: 785, training loss: 0.44827155753982517\n",
      "Iteration: 786, training loss: 0.4482101541654234\n",
      "Iteration: 787, training loss: 0.44814885309655034\n",
      "Iteration: 788, training loss: 0.4480876540741654\n",
      "Iteration: 789, training loss: 0.44802655684003484\n",
      "Iteration: 790, training loss: 0.4479655611367292\n",
      "Iteration: 791, training loss: 0.4479046667076197\n",
      "Iteration: 792, training loss: 0.4478438732968765\n",
      "Iteration: 793, training loss: 0.4477831806494651\n",
      "Iteration: 794, training loss: 0.44772258851114355\n",
      "Iteration: 795, training loss: 0.44766209662846035\n",
      "Iteration: 796, training loss: 0.4476017047487509\n",
      "Iteration: 797, training loss: 0.4475414126201352\n",
      "Iteration: 798, training loss: 0.4474812199915148\n",
      "Iteration: 799, training loss: 0.44742112661257055\n",
      "Iteration: 800, training loss: 0.44736113223375923\n",
      "Iteration: 801, training loss: 0.44730123660631116\n",
      "Iteration: 802, training loss: 0.4472414394822275\n",
      "Iteration: 803, training loss: 0.44718174061427785\n",
      "Iteration: 804, training loss: 0.44712213975599663\n",
      "Iteration: 805, training loss: 0.44706263666168145\n",
      "Iteration: 806, training loss: 0.44700323108638973\n",
      "Iteration: 807, training loss: 0.4469439227859365\n",
      "Iteration: 808, training loss: 0.4468847115168913\n",
      "Iteration: 809, training loss: 0.4468255970365761\n",
      "Iteration: 810, training loss: 0.44676657910306206\n",
      "Iteration: 811, training loss: 0.44670765747516733\n",
      "Iteration: 812, training loss: 0.4466488319124544\n",
      "Iteration: 813, training loss: 0.44659010217522715\n",
      "Iteration: 814, training loss: 0.446531468024529\n",
      "Iteration: 815, training loss: 0.44647292922213916\n",
      "Iteration: 816, training loss: 0.4464144855305713\n",
      "Iteration: 817, training loss: 0.44635613671307034\n",
      "Iteration: 818, training loss: 0.4462978825336098\n",
      "Iteration: 819, training loss: 0.4462397227568898\n",
      "Iteration: 820, training loss: 0.44618165714833374\n",
      "Iteration: 821, training loss: 0.44612368547408665\n",
      "Iteration: 822, training loss: 0.4460658075010119\n",
      "Iteration: 823, training loss: 0.4460080229966892\n",
      "Iteration: 824, training loss: 0.445950331729412\n",
      "Iteration: 825, training loss: 0.44589273346818487\n",
      "Iteration: 826, training loss: 0.44583522798272124\n",
      "Iteration: 827, training loss: 0.44577781504344044\n",
      "Iteration: 828, training loss: 0.445720494421466\n",
      "Iteration: 829, training loss: 0.4456632658886226\n",
      "Iteration: 830, training loss: 0.44560612921743387\n",
      "Iteration: 831, training loss: 0.44554908418111994\n",
      "Iteration: 832, training loss: 0.44549213055359504\n",
      "Iteration: 833, training loss: 0.44543526810946504\n",
      "Iteration: 834, training loss: 0.44537849662402484\n",
      "Iteration: 835, training loss: 0.4453218158732566\n",
      "Iteration: 836, training loss: 0.44526522563382664\n",
      "Iteration: 837, training loss: 0.44520872568308334\n",
      "Iteration: 838, training loss: 0.4451523157990552\n",
      "Iteration: 839, training loss: 0.44509599576044756\n",
      "Iteration: 840, training loss: 0.44503976534664114\n",
      "Iteration: 841, training loss: 0.4449836243376894\n",
      "Iteration: 842, training loss: 0.44492757251431575\n",
      "Iteration: 843, training loss: 0.44487160965791195\n",
      "Iteration: 844, training loss: 0.44481573555053544\n",
      "Iteration: 845, training loss: 0.44475994997490703\n",
      "Iteration: 846, training loss: 0.44470425271440844\n",
      "Iteration: 847, training loss: 0.4446486435530807\n",
      "Iteration: 848, training loss: 0.44459312227562076\n",
      "Iteration: 849, training loss: 0.4445376886673803\n",
      "Iteration: 850, training loss: 0.4444823425143629\n",
      "Iteration: 851, training loss: 0.44442708360322186\n",
      "Iteration: 852, training loss: 0.4443719117212579\n",
      "Iteration: 853, training loss: 0.4443168266564172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 854, training loss: 0.44426182819728877\n",
      "Iteration: 855, training loss: 0.4442069161331025\n",
      "Iteration: 856, training loss: 0.44415209025372704\n",
      "Iteration: 857, training loss: 0.4440973503496674\n",
      "Iteration: 858, training loss: 0.4440426962120626\n",
      "Iteration: 859, training loss: 0.44398812763268386\n",
      "Iteration: 860, training loss: 0.44393364440393224\n",
      "Iteration: 861, training loss: 0.44387924631883646\n",
      "Iteration: 862, training loss: 0.44382493317105054\n",
      "Iteration: 863, training loss: 0.4437707047548522\n",
      "Iteration: 864, training loss: 0.4437165608651401\n",
      "Iteration: 865, training loss: 0.4436625012974321\n",
      "Iteration: 866, training loss: 0.4436085258478629\n",
      "Iteration: 867, training loss: 0.4435546343131821\n",
      "Iteration: 868, training loss: 0.44350082649075195\n",
      "Iteration: 869, training loss: 0.44344710217854527\n",
      "Iteration: 870, training loss: 0.4433934611751434\n",
      "Iteration: 871, training loss: 0.44333990327973416\n",
      "Iteration: 872, training loss: 0.44328642829210946\n",
      "Iteration: 873, training loss: 0.4432330360126637\n",
      "Iteration: 874, training loss: 0.44317972624239127\n",
      "Iteration: 875, training loss: 0.4431264987828846\n",
      "Iteration: 876, training loss: 0.44307335343633264\n",
      "Iteration: 877, training loss: 0.4430202900055177\n",
      "Iteration: 878, training loss: 0.44296730829381453\n",
      "Iteration: 879, training loss: 0.44291440810518773\n",
      "Iteration: 880, training loss: 0.4428615892441895\n",
      "Iteration: 881, training loss: 0.44280885151595856\n",
      "Iteration: 882, training loss: 0.44275619472621686\n",
      "Iteration: 883, training loss: 0.44270361868126884\n",
      "Iteration: 884, training loss: 0.4426511231879984\n",
      "Iteration: 885, training loss: 0.4425987080538675\n",
      "Iteration: 886, training loss: 0.4425463730869141\n",
      "Iteration: 887, training loss: 0.4424941180957502\n",
      "Iteration: 888, training loss: 0.4424419428895596\n",
      "Iteration: 889, training loss: 0.4423898472780963\n",
      "Iteration: 890, training loss: 0.4423378310716825\n",
      "Iteration: 891, training loss: 0.44228589408120633\n",
      "Iteration: 892, training loss: 0.44223403611812007\n",
      "Iteration: 893, training loss: 0.4421822569944388\n",
      "Iteration: 894, training loss: 0.4421305565227375\n",
      "Iteration: 895, training loss: 0.4420789345161498\n",
      "Iteration: 896, training loss: 0.4420273907883659\n",
      "Iteration: 897, training loss: 0.44197592515363066\n",
      "Iteration: 898, training loss: 0.44192453742674165\n",
      "Iteration: 899, training loss: 0.4418732274230476\n",
      "Iteration: 900, training loss: 0.4418219949584457\n",
      "Iteration: 901, training loss: 0.44177083984938087\n",
      "Iteration: 902, training loss: 0.44171976191284307\n",
      "Iteration: 903, training loss: 0.44166876096636565\n",
      "Iteration: 904, training loss: 0.44161783682802347\n",
      "Iteration: 905, training loss: 0.4415669893164313\n",
      "Iteration: 906, training loss: 0.4415162182507417\n",
      "Iteration: 907, training loss: 0.4414655234506433\n",
      "Iteration: 908, training loss: 0.4414149047363591\n",
      "Iteration: 909, training loss: 0.4413643619286443\n",
      "Iteration: 910, training loss: 0.4413138948487849\n",
      "Iteration: 911, training loss: 0.4412635033185957\n",
      "Iteration: 912, training loss: 0.44121318716041835\n",
      "Iteration: 913, training loss: 0.44116294619712004\n",
      "Iteration: 914, training loss: 0.4411127802520913\n",
      "Iteration: 915, training loss: 0.4410626891492439\n",
      "Iteration: 916, training loss: 0.4410126727130102\n",
      "Iteration: 917, training loss: 0.44096273076834047\n",
      "Iteration: 918, training loss: 0.4409128631407011\n",
      "Iteration: 919, training loss: 0.4408630696560734\n",
      "Iteration: 920, training loss: 0.4408133501409516\n",
      "Iteration: 921, training loss: 0.44076370442234086\n",
      "Iteration: 922, training loss: 0.4407141323277559\n",
      "Iteration: 923, training loss: 0.44066463368521924\n",
      "Iteration: 924, training loss: 0.4406152083232593\n",
      "Iteration: 925, training loss: 0.44056585607090865\n",
      "Iteration: 926, training loss: 0.44051657675770267\n",
      "Iteration: 927, training loss: 0.4404673702136776\n",
      "Iteration: 928, training loss: 0.44041823626936866\n",
      "Iteration: 929, training loss: 0.44036917475580867\n",
      "Iteration: 930, training loss: 0.44032018550452634\n",
      "Iteration: 931, training loss: 0.4402712683475445\n",
      "Iteration: 932, training loss: 0.44022242311737836\n",
      "Iteration: 933, training loss: 0.4401736496470341\n",
      "Iteration: 934, training loss: 0.44012494777000694\n",
      "Iteration: 935, training loss: 0.44007631732027963\n",
      "Iteration: 936, training loss: 0.4400277581323209\n",
      "Iteration: 937, training loss: 0.4399792700410836\n",
      "Iteration: 938, training loss: 0.4399308528820033\n",
      "Iteration: 939, training loss: 0.43988250649099636\n",
      "Iteration: 940, training loss: 0.4398342307044587\n",
      "Iteration: 941, training loss: 0.4397860253592638\n",
      "Iteration: 942, training loss: 0.4397378902927616\n",
      "Iteration: 943, training loss: 0.4396898253427762\n",
      "Iteration: 944, training loss: 0.4396418303476049\n",
      "Iteration: 945, training loss: 0.4395939051460163\n",
      "Iteration: 946, training loss: 0.43954604957724874\n",
      "Iteration: 947, training loss: 0.4394982634810088\n",
      "Iteration: 948, training loss: 0.4394505466974696\n",
      "Iteration: 949, training loss: 0.4394028990672695\n",
      "Iteration: 950, training loss: 0.43935532043151027\n",
      "Iteration: 951, training loss: 0.43930781063175545\n",
      "Iteration: 952, training loss: 0.4392603695100292\n",
      "Iteration: 953, training loss: 0.4392129969088146\n",
      "Iteration: 954, training loss: 0.43916569267105193\n",
      "Iteration: 955, training loss: 0.4391184566401371\n",
      "Iteration: 956, training loss: 0.4390712886599206\n",
      "Iteration: 957, training loss: 0.4390241885747055\n",
      "Iteration: 958, training loss: 0.438977156229246\n",
      "Iteration: 959, training loss: 0.43893019146874623\n",
      "Iteration: 960, training loss: 0.43888329413885846\n",
      "Iteration: 961, training loss: 0.4388364640856816\n",
      "Iteration: 962, training loss: 0.4387897011557598\n",
      "Iteration: 963, training loss: 0.4387430051960811\n",
      "Iteration: 964, training loss: 0.4386963760540757\n",
      "Iteration: 965, training loss: 0.43864981357761446\n",
      "Iteration: 966, training loss: 0.4386033176150078\n",
      "Iteration: 967, training loss: 0.43855688801500375\n",
      "Iteration: 968, training loss: 0.4385105246267869\n",
      "Iteration: 969, training loss: 0.43846422729997675\n",
      "Iteration: 970, training loss: 0.4384179958846261\n",
      "Iteration: 971, training loss: 0.43837183023121995\n",
      "Iteration: 972, training loss: 0.43832573019067406\n",
      "Iteration: 973, training loss: 0.43827969561433305\n",
      "Iteration: 974, training loss: 0.43823372635396945\n",
      "Iteration: 975, training loss: 0.438187822261782\n",
      "Iteration: 976, training loss: 0.4381419831903945\n",
      "Iteration: 977, training loss: 0.4380962089928542\n",
      "Iteration: 978, training loss: 0.43805049952263014\n",
      "Iteration: 979, training loss: 0.43800485463361255\n",
      "Iteration: 980, training loss: 0.43795927418011055\n",
      "Iteration: 981, training loss: 0.43791375801685123\n",
      "Iteration: 982, training loss: 0.43786830599897836\n",
      "Iteration: 983, training loss: 0.4378229179820507\n",
      "Iteration: 984, training loss: 0.43777759382204046\n",
      "Iteration: 985, training loss: 0.4377323333753329\n",
      "Iteration: 986, training loss: 0.43768713649872376\n",
      "Iteration: 987, training loss: 0.4376420030494185\n",
      "Iteration: 988, training loss: 0.437596932885031\n",
      "Iteration: 989, training loss: 0.43755192586358216\n",
      "Iteration: 990, training loss: 0.4375069818434981\n",
      "Iteration: 991, training loss: 0.4374621006836096\n",
      "Iteration: 992, training loss: 0.43741728224315013\n",
      "Iteration: 993, training loss: 0.43737252638175467\n",
      "Iteration: 994, training loss: 0.4373278329594588\n",
      "Iteration: 995, training loss: 0.4372832018366967\n",
      "Iteration: 996, training loss: 0.4372386328743002\n",
      "Iteration: 997, training loss: 0.43719412593349766\n",
      "Iteration: 998, training loss: 0.4371496808759121\n",
      "Iteration: 999, training loss: 0.43710529756356054\n",
      "Iteration: 1000, training loss: 0.43706097585885223\n",
      "Iteration: 1001, training loss: 0.4370167156245876\n",
      "Iteration: 1002, training loss: 0.4369725167239568\n",
      "Iteration: 1003, training loss: 0.43692837902053855\n",
      "Iteration: 1004, training loss: 0.43688430237829884\n",
      "Iteration: 1005, training loss: 0.4368402866615895\n",
      "Iteration: 1006, training loss: 0.43679633173514737\n",
      "Iteration: 1007, training loss: 0.4367524374640924\n",
      "Iteration: 1008, training loss: 0.436708603713927\n",
      "Iteration: 1009, training loss: 0.4366648303505341\n",
      "Iteration: 1010, training loss: 0.4366211172401768\n",
      "Iteration: 1011, training loss: 0.4365774642494962\n",
      "Iteration: 1012, training loss: 0.43653387124551074\n",
      "Iteration: 1013, training loss: 0.43649033809561494\n",
      "Iteration: 1014, training loss: 0.4364468646675777\n",
      "Iteration: 1015, training loss: 0.4364034508295418\n",
      "Iteration: 1016, training loss: 0.43636009645002194\n",
      "Iteration: 1017, training loss: 0.436316801397904\n",
      "Iteration: 1018, training loss: 0.4362735655424436\n",
      "Iteration: 1019, training loss: 0.43623038875326503\n",
      "Iteration: 1020, training loss: 0.43618727090036\n",
      "Iteration: 1021, training loss: 0.4361442118540864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1022, training loss: 0.43610121148516706\n",
      "Iteration: 1023, training loss: 0.43605826966468864\n",
      "Iteration: 1024, training loss: 0.4360153862641003\n",
      "Iteration: 1025, training loss: 0.43597256115521293\n",
      "Iteration: 1026, training loss: 0.43592979421019734\n",
      "Iteration: 1027, training loss: 0.43588708530158365\n",
      "Iteration: 1028, training loss: 0.4358444343022597\n",
      "Iteration: 1029, training loss: 0.4358018410854702\n",
      "Iteration: 1030, training loss: 0.4357593055248153\n",
      "Iteration: 1031, training loss: 0.43571682749424967\n",
      "Iteration: 1032, training loss: 0.4356744068680812\n",
      "Iteration: 1033, training loss: 0.43563204352096985\n",
      "Iteration: 1034, training loss: 0.43558973732792644\n",
      "Iteration: 1035, training loss: 0.43554748816431177\n",
      "Iteration: 1036, training loss: 0.4355052959058352\n",
      "Iteration: 1037, training loss: 0.4354631604285536\n",
      "Iteration: 1038, training loss: 0.4354210816088704\n",
      "Iteration: 1039, training loss: 0.43537905932353416\n",
      "Iteration: 1040, training loss: 0.43533709344963767\n",
      "Iteration: 1041, training loss: 0.4352951838646167\n",
      "Iteration: 1042, training loss: 0.4352533304462489\n",
      "Iteration: 1043, training loss: 0.4352115330726529\n",
      "Iteration: 1044, training loss: 0.43516979162228686\n",
      "Iteration: 1045, training loss: 0.43512810597394763\n",
      "Iteration: 1046, training loss: 0.43508647600676964\n",
      "Iteration: 1047, training loss: 0.4350449016002234\n",
      "Iteration: 1048, training loss: 0.4350033826341151\n",
      "Iteration: 1049, training loss: 0.43496191898858505\n",
      "Iteration: 1050, training loss: 0.4349205105441066\n",
      "Iteration: 1051, training loss: 0.43487915718148523\n",
      "Iteration: 1052, training loss: 0.43483785878185754\n",
      "Iteration: 1053, training loss: 0.43479661522668994\n",
      "Iteration: 1054, training loss: 0.43475542639777764\n",
      "Iteration: 1055, training loss: 0.43471429217724367\n",
      "Iteration: 1056, training loss: 0.43467321244753787\n",
      "Iteration: 1057, training loss: 0.43463218709143575\n",
      "Iteration: 1058, training loss: 0.4345912159920374\n",
      "Iteration: 1059, training loss: 0.4345502990327665\n",
      "Iteration: 1060, training loss: 0.43450943609736903\n",
      "Iteration: 1061, training loss: 0.43446862706991296\n",
      "Iteration: 1062, training loss: 0.4344278718347862\n",
      "Iteration: 1063, training loss: 0.4343871702766966\n",
      "Iteration: 1064, training loss: 0.4343465222806698\n",
      "Iteration: 1065, training loss: 0.43430592773204935\n",
      "Iteration: 1066, training loss: 0.43426538651649477\n",
      "Iteration: 1067, training loss: 0.43422489851998114\n",
      "Iteration: 1068, training loss: 0.4341844636287977\n",
      "Iteration: 1069, training loss: 0.434144081729547\n",
      "Iteration: 1070, training loss: 0.434103752709144\n",
      "Iteration: 1071, training loss: 0.43406347645481486\n",
      "Iteration: 1072, training loss: 0.43402325285409593\n",
      "Iteration: 1073, training loss: 0.433983081794833\n",
      "Iteration: 1074, training loss: 0.4339429631651801\n",
      "Iteration: 1075, training loss: 0.4339028968535987\n",
      "Iteration: 1076, training loss: 0.4338628827488562\n",
      "Iteration: 1077, training loss: 0.43382292074002576\n",
      "Iteration: 1078, training loss: 0.43378301071648473\n",
      "Iteration: 1079, training loss: 0.43374315256791385\n",
      "Iteration: 1080, training loss: 0.4337033461842961\n",
      "Iteration: 1081, training loss: 0.4336635914559162\n",
      "Iteration: 1082, training loss: 0.43362388827335935\n",
      "Iteration: 1083, training loss: 0.4335842365275097\n",
      "Iteration: 1084, training loss: 0.4335446361095508\n",
      "Iteration: 1085, training loss: 0.43350508691096307\n",
      "Iteration: 1086, training loss: 0.43346558882352404\n",
      "Iteration: 1087, training loss: 0.43342614173930677\n",
      "Iteration: 1088, training loss: 0.433386745550679\n",
      "Iteration: 1089, training loss: 0.43334740015030254\n",
      "Iteration: 1090, training loss: 0.4333081054311318\n",
      "Iteration: 1091, training loss: 0.4332688612864132\n",
      "Iteration: 1092, training loss: 0.4332296676096841\n",
      "Iteration: 1093, training loss: 0.43319052429477234\n",
      "Iteration: 1094, training loss: 0.43315143123579447\n",
      "Iteration: 1095, training loss: 0.43311238832715543\n",
      "Iteration: 1096, training loss: 0.43307339546354745\n",
      "Iteration: 1097, training loss: 0.4330344525399492\n",
      "Iteration: 1098, training loss: 0.4329955594516249\n",
      "Iteration: 1099, training loss: 0.4329567160941232\n",
      "Iteration: 1100, training loss: 0.4329179223632766\n",
      "Iteration: 1101, training loss: 0.43287917815520005\n",
      "Iteration: 1102, training loss: 0.43284048336629094\n",
      "Iteration: 1103, training loss: 0.4328018378932271\n",
      "Iteration: 1104, training loss: 0.4327632416329667\n",
      "Iteration: 1105, training loss: 0.4327246944827471\n",
      "Iteration: 1106, training loss: 0.4326861963400839\n",
      "Iteration: 1107, training loss: 0.4326477471027701\n",
      "Iteration: 1108, training loss: 0.43260934666887535\n",
      "Iteration: 1109, training loss: 0.43257099493674506\n",
      "Iteration: 1110, training loss: 0.4325326918049992\n",
      "Iteration: 1111, training loss: 0.4324944371725318\n",
      "Iteration: 1112, training loss: 0.4324562309385099\n",
      "Iteration: 1113, training loss: 0.4324180730023727\n",
      "Iteration: 1114, training loss: 0.43237996326383116\n",
      "Iteration: 1115, training loss: 0.43234190162286595\n",
      "Iteration: 1116, training loss: 0.4323038879797279\n",
      "Iteration: 1117, training loss: 0.4322659222349366\n",
      "Iteration: 1118, training loss: 0.4322280042892793\n",
      "Iteration: 1119, training loss: 0.43219013404381057\n",
      "Iteration: 1120, training loss: 0.432152311399851\n",
      "Iteration: 1121, training loss: 0.43211453625898677\n",
      "Iteration: 1122, training loss: 0.4320768085230685\n",
      "Iteration: 1123, training loss: 0.43203912809421036\n",
      "Iteration: 1124, training loss: 0.4320014948747899\n",
      "Iteration: 1125, training loss: 0.4319639087674463\n",
      "Iteration: 1126, training loss: 0.43192636967507986\n",
      "Iteration: 1127, training loss: 0.431888877500852\n",
      "Iteration: 1128, training loss: 0.4318514321481828\n",
      "Iteration: 1129, training loss: 0.4318140335207518\n",
      "Iteration: 1130, training loss: 0.43177668152249626\n",
      "Iteration: 1131, training loss: 0.4317393760576106\n",
      "Iteration: 1132, training loss: 0.43170211703054534\n",
      "Iteration: 1133, training loss: 0.43166490434600713\n",
      "Iteration: 1134, training loss: 0.43162773790895675\n",
      "Iteration: 1135, training loss: 0.4315906176246093\n",
      "Iteration: 1136, training loss: 0.43155354339843277\n",
      "Iteration: 1137, training loss: 0.4315165151361477\n",
      "Iteration: 1138, training loss: 0.4314795327437259\n",
      "Iteration: 1139, training loss: 0.4314425961273902\n",
      "Iteration: 1140, training loss: 0.4314057051936135\n",
      "Iteration: 1141, training loss: 0.4313688598491173\n",
      "Iteration: 1142, training loss: 0.4313320600008723\n",
      "Iteration: 1143, training loss: 0.4312953055560962\n",
      "Iteration: 1144, training loss: 0.431258596422254\n",
      "Iteration: 1145, training loss: 0.4312219325070565\n",
      "Iteration: 1146, training loss: 0.43118531371846003\n",
      "Iteration: 1147, training loss: 0.4311487399646652\n",
      "Iteration: 1148, training loss: 0.43111221115411674\n",
      "Iteration: 1149, training loss: 0.4310757271955021\n",
      "Iteration: 1150, training loss: 0.43103928799775126\n",
      "Iteration: 1151, training loss: 0.4310028934700354\n",
      "Iteration: 1152, training loss: 0.43096654352176683\n",
      "Iteration: 1153, training loss: 0.4309302380625975\n",
      "Iteration: 1154, training loss: 0.4308939770024189\n",
      "Iteration: 1155, training loss: 0.43085776025136097\n",
      "Iteration: 1156, training loss: 0.43082158771979134\n",
      "Iteration: 1157, training loss: 0.43078545931831475\n",
      "Iteration: 1158, training loss: 0.43074937495777194\n",
      "Iteration: 1159, training loss: 0.4307133345492398\n",
      "Iteration: 1160, training loss: 0.4306773380040295\n",
      "Iteration: 1161, training loss: 0.4306413852336866\n",
      "Iteration: 1162, training loss: 0.4306054761499899\n",
      "Iteration: 1163, training loss: 0.4305696106649508\n",
      "Iteration: 1164, training loss: 0.43053378869081266\n",
      "Iteration: 1165, training loss: 0.43049801014005035\n",
      "Iteration: 1166, training loss: 0.43046227492536865\n",
      "Iteration: 1167, training loss: 0.43042658295970254\n",
      "Iteration: 1168, training loss: 0.43039093415621577\n",
      "Iteration: 1169, training loss: 0.4303553284283008\n",
      "Iteration: 1170, training loss: 0.43031976568957736\n",
      "Iteration: 1171, training loss: 0.4302842458538922\n",
      "Iteration: 1172, training loss: 0.4302487688353185\n",
      "Iteration: 1173, training loss: 0.43021333454815475\n",
      "Iteration: 1174, training loss: 0.4301779429069244\n",
      "Iteration: 1175, training loss: 0.4301425938263748\n",
      "Iteration: 1176, training loss: 0.4301072872214772\n",
      "Iteration: 1177, training loss: 0.4300720230074251\n",
      "Iteration: 1178, training loss: 0.43003680109963444\n",
      "Iteration: 1179, training loss: 0.43000162141374226\n",
      "Iteration: 1180, training loss: 0.42996648386560643\n",
      "Iteration: 1181, training loss: 0.4299313883713049\n",
      "Iteration: 1182, training loss: 0.429896334847135\n",
      "Iteration: 1183, training loss: 0.42986132320961234\n",
      "Iteration: 1184, training loss: 0.42982635337547104\n",
      "Iteration: 1185, training loss: 0.429791425261662\n",
      "Iteration: 1186, training loss: 0.42975653878535325\n",
      "Iteration: 1187, training loss: 0.4297216938639285\n",
      "Iteration: 1188, training loss: 0.42968689041498687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1189, training loss: 0.42965212835634214\n",
      "Iteration: 1190, training loss: 0.429617407606022\n",
      "Iteration: 1191, training loss: 0.42958272808226766\n",
      "Iteration: 1192, training loss: 0.42954808970353275\n",
      "Iteration: 1193, training loss: 0.4295134923884832\n",
      "Iteration: 1194, training loss: 0.42947893605599596\n",
      "Iteration: 1195, training loss: 0.4294444206251591\n",
      "Iteration: 1196, training loss: 0.4294099460152706\n",
      "Iteration: 1197, training loss: 0.4293755121458377\n",
      "Iteration: 1198, training loss: 0.42934111893657667\n",
      "Iteration: 1199, training loss: 0.4293067663074118\n",
      "Iteration: 1200, training loss: 0.4292724541784751\n",
      "Iteration: 1201, training loss: 0.42923818247010515\n",
      "Iteration: 1202, training loss: 0.4292039511028471\n",
      "Iteration: 1203, training loss: 0.4291697599974513\n",
      "Iteration: 1204, training loss: 0.4291356090748736\n",
      "Iteration: 1205, training loss: 0.4291014982562737\n",
      "Iteration: 1206, training loss: 0.42906742746301535\n",
      "Iteration: 1207, training loss: 0.4290333966166655\n",
      "Iteration: 1208, training loss: 0.4289994056389932\n",
      "Iteration: 1209, training loss: 0.42896545445196976\n",
      "Iteration: 1210, training loss: 0.42893154297776753\n",
      "Iteration: 1211, training loss: 0.4288976711387596\n",
      "Iteration: 1212, training loss: 0.42886383885751905\n",
      "Iteration: 1213, training loss: 0.42883004605681835\n",
      "Iteration: 1214, training loss: 0.4287962926596289\n",
      "Iteration: 1215, training loss: 0.42876257858912015\n",
      "Iteration: 1216, training loss: 0.4287289037686593\n",
      "Iteration: 1217, training loss: 0.4286952681218105\n",
      "Iteration: 1218, training loss: 0.4286616715723343\n",
      "Iteration: 1219, training loss: 0.42862811404418705\n",
      "Iteration: 1220, training loss: 0.4285945954615203\n",
      "Iteration: 1221, training loss: 0.4285611157486802\n",
      "Iteration: 1222, training loss: 0.42852767483020715\n",
      "Iteration: 1223, training loss: 0.4284942726308346\n",
      "Iteration: 1224, training loss: 0.4284609090754892\n",
      "Iteration: 1225, training loss: 0.42842758408928977\n",
      "Iteration: 1226, training loss: 0.4283942975975465\n",
      "Iteration: 1227, training loss: 0.42836104952576126\n",
      "Iteration: 1228, training loss: 0.4283278397996261\n",
      "Iteration: 1229, training loss: 0.4282946683450228\n",
      "Iteration: 1230, training loss: 0.42826153508802306\n",
      "Iteration: 1231, training loss: 0.4282284399548869\n",
      "Iteration: 1232, training loss: 0.42819538287206294\n",
      "Iteration: 1233, training loss: 0.42816236376618705\n",
      "Iteration: 1234, training loss: 0.42812938256408256\n",
      "Iteration: 1235, training loss: 0.42809643919275914\n",
      "Iteration: 1236, training loss: 0.4280635335794125\n",
      "Iteration: 1237, training loss: 0.42803066565142356\n",
      "Iteration: 1238, training loss: 0.4279978353363583\n",
      "Iteration: 1239, training loss: 0.427965042561967\n",
      "Iteration: 1240, training loss: 0.4279322872561835\n",
      "Iteration: 1241, training loss: 0.42789956934712486\n",
      "Iteration: 1242, training loss: 0.42786688876309087\n",
      "Iteration: 1243, training loss: 0.4278342454325632\n",
      "Iteration: 1244, training loss: 0.42780163928420517\n",
      "Iteration: 1245, training loss: 0.4277690702468611\n",
      "Iteration: 1246, training loss: 0.4277365382495557\n",
      "Iteration: 1247, training loss: 0.4277040432214936\n",
      "Iteration: 1248, training loss: 0.42767158509205855\n",
      "Iteration: 1249, training loss: 0.4276391637908134\n",
      "Iteration: 1250, training loss: 0.42760677924749924\n",
      "Iteration: 1251, training loss: 0.42757443139203477\n",
      "Iteration: 1252, training loss: 0.4275421201545159\n",
      "Iteration: 1253, training loss: 0.42750984546521537\n",
      "Iteration: 1254, training loss: 0.42747760725458167\n",
      "Iteration: 1255, training loss: 0.42744540545323945\n",
      "Iteration: 1256, training loss: 0.42741323999198794\n",
      "Iteration: 1257, training loss: 0.42738111080180136\n",
      "Iteration: 1258, training loss: 0.4273490178138276\n",
      "Iteration: 1259, training loss: 0.42731696095938837\n",
      "Iteration: 1260, training loss: 0.42728494016997803\n",
      "Iteration: 1261, training loss: 0.4272529553772639\n",
      "Iteration: 1262, training loss: 0.42722100651308464\n",
      "Iteration: 1263, training loss: 0.42718909350945095\n",
      "Iteration: 1264, training loss: 0.4271572162985441\n",
      "Iteration: 1265, training loss: 0.42712537481271595\n",
      "Iteration: 1266, training loss: 0.42709356898448825\n",
      "Iteration: 1267, training loss: 0.4270617987465521\n",
      "Iteration: 1268, training loss: 0.4270300640317675\n",
      "Iteration: 1269, training loss: 0.42699836477316294\n",
      "Iteration: 1270, training loss: 0.4269667009039348\n",
      "Iteration: 1271, training loss: 0.42693507235744665\n",
      "Iteration: 1272, training loss: 0.42690347906722936\n",
      "Iteration: 1273, training loss: 0.42687192096697973\n",
      "Iteration: 1274, training loss: 0.42684039799056084\n",
      "Iteration: 1275, training loss: 0.42680891007200095\n",
      "Iteration: 1276, training loss: 0.4267774571454935\n",
      "Iteration: 1277, training loss: 0.4267460391453959\n",
      "Iteration: 1278, training loss: 0.4267146560062301\n",
      "Iteration: 1279, training loss: 0.426683307662681\n",
      "Iteration: 1280, training loss: 0.42665199404959675\n",
      "Iteration: 1281, training loss: 0.42662071510198785\n",
      "Iteration: 1282, training loss: 0.4265894707550268\n",
      "Iteration: 1283, training loss: 0.42655826094404786\n",
      "Iteration: 1284, training loss: 0.42652708560454594\n",
      "Iteration: 1285, training loss: 0.4264959446721768\n",
      "Iteration: 1286, training loss: 0.4264648380827562\n",
      "Iteration: 1287, training loss: 0.4264337657722595\n",
      "Iteration: 1288, training loss: 0.42640272767682125\n",
      "Iteration: 1289, training loss: 0.4263717237327347\n",
      "Iteration: 1290, training loss: 0.42634075387645115\n",
      "Iteration: 1291, training loss: 0.4263098180445797\n",
      "Iteration: 1292, training loss: 0.42627891617388686\n",
      "Iteration: 1293, training loss: 0.42624804820129575\n",
      "Iteration: 1294, training loss: 0.42621721406388596\n",
      "Iteration: 1295, training loss: 0.4261864136988929\n",
      "Iteration: 1296, training loss: 0.42615564704370734\n",
      "Iteration: 1297, training loss: 0.42612491403587516\n",
      "Iteration: 1298, training loss: 0.42609421461309654\n",
      "Iteration: 1299, training loss: 0.42606354871322594\n",
      "Iteration: 1300, training loss: 0.4260329162742712\n",
      "Iteration: 1301, training loss: 0.42600231723439347\n",
      "Iteration: 1302, training loss: 0.4259717515319064\n",
      "Iteration: 1303, training loss: 0.42594121910527616\n",
      "Iteration: 1304, training loss: 0.4259107198931204\n",
      "Iteration: 1305, training loss: 0.4258802538342083\n",
      "Iteration: 1306, training loss: 0.4258498208674599\n",
      "Iteration: 1307, training loss: 0.4258194209319457\n",
      "Iteration: 1308, training loss: 0.4257890539668862\n",
      "Iteration: 1309, training loss: 0.4257587199116513\n",
      "Iteration: 1310, training loss: 0.4257284187057605\n",
      "Iteration: 1311, training loss: 0.42569815028888164\n",
      "Iteration: 1312, training loss: 0.4256679146008307\n",
      "Iteration: 1313, training loss: 0.4256377115815719\n",
      "Iteration: 1314, training loss: 0.4256075411712165\n",
      "Iteration: 1315, training loss: 0.4255774033100228\n",
      "Iteration: 1316, training loss: 0.4255472979383958\n",
      "Iteration: 1317, training loss: 0.4255172249968865\n",
      "Iteration: 1318, training loss: 0.4254871844261913\n",
      "Iteration: 1319, training loss: 0.4254571761671524\n",
      "Iteration: 1320, training loss: 0.42542720016075636\n",
      "Iteration: 1321, training loss: 0.4253972563481343\n",
      "Iteration: 1322, training loss: 0.42536734467056136\n",
      "Iteration: 1323, training loss: 0.425337465069456\n",
      "Iteration: 1324, training loss: 0.42530761748638035\n",
      "Iteration: 1325, training loss: 0.42527780186303865\n",
      "Iteration: 1326, training loss: 0.4252480181412778\n",
      "Iteration: 1327, training loss: 0.4252182662630864\n",
      "Iteration: 1328, training loss: 0.42518854617059476\n",
      "Iteration: 1329, training loss: 0.425158857806074\n",
      "Iteration: 1330, training loss: 0.42512920111193603\n",
      "Iteration: 1331, training loss: 0.425099576030733\n",
      "Iteration: 1332, training loss: 0.4250699825051568\n",
      "Iteration: 1333, training loss: 0.42504042047803886\n",
      "Iteration: 1334, training loss: 0.42501088989234964\n",
      "Iteration: 1335, training loss: 0.42498139069119795\n",
      "Iteration: 1336, training loss: 0.4249519228178312\n",
      "Iteration: 1337, training loss: 0.4249224862156344\n",
      "Iteration: 1338, training loss: 0.42489308082812993\n",
      "Iteration: 1339, training loss: 0.4248637065989772\n",
      "Iteration: 1340, training loss: 0.42483436347197256\n",
      "Iteration: 1341, training loss: 0.424805051391048\n",
      "Iteration: 1342, training loss: 0.42477577030027175\n",
      "Iteration: 1343, training loss: 0.42474652014384745\n",
      "Iteration: 1344, training loss: 0.42471730086611353\n",
      "Iteration: 1345, training loss: 0.42468811241154325\n",
      "Iteration: 1346, training loss: 0.4246589547247442\n",
      "Iteration: 1347, training loss: 0.4246298277504576\n",
      "Iteration: 1348, training loss: 0.4246007314335584\n",
      "Iteration: 1349, training loss: 0.4245716657190545\n",
      "Iteration: 1350, training loss: 0.4245426305520865\n",
      "Iteration: 1351, training loss: 0.4245136258779274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1352, training loss: 0.42448465164198196\n",
      "Iteration: 1353, training loss: 0.4244557077897869\n",
      "Iteration: 1354, training loss: 0.4244267942670098\n",
      "Iteration: 1355, training loss: 0.424397911019449\n",
      "Iteration: 1356, training loss: 0.4243690579930335\n",
      "Iteration: 1357, training loss: 0.42434023513382213\n",
      "Iteration: 1358, training loss: 0.42431144238800356\n",
      "Iteration: 1359, training loss: 0.42428267970189576\n",
      "Iteration: 1360, training loss: 0.42425394702194547\n",
      "Iteration: 1361, training loss: 0.4242252442947282\n",
      "Iteration: 1362, training loss: 0.4241965714669475\n",
      "Iteration: 1363, training loss: 0.4241679284854348\n",
      "Iteration: 1364, training loss: 0.424139315297149\n",
      "Iteration: 1365, training loss: 0.4241107318491759\n",
      "Iteration: 1366, training loss: 0.4240821780887284\n",
      "Iteration: 1367, training loss: 0.42405365396314537\n",
      "Iteration: 1368, training loss: 0.42402515941989194\n",
      "Iteration: 1369, training loss: 0.4239966944065588\n",
      "Iteration: 1370, training loss: 0.4239682588708618\n",
      "Iteration: 1371, training loss: 0.4239398527606418\n",
      "Iteration: 1372, training loss: 0.4239114760238642\n",
      "Iteration: 1373, training loss: 0.4238831286086187\n",
      "Iteration: 1374, training loss: 0.42385481046311874\n",
      "Iteration: 1375, training loss: 0.42382652153570116\n",
      "Iteration: 1376, training loss: 0.4237982617748261\n",
      "Iteration: 1377, training loss: 0.42377003112907646\n",
      "Iteration: 1378, training loss: 0.42374182954715744\n",
      "Iteration: 1379, training loss: 0.42371365697789654\n",
      "Iteration: 1380, training loss: 0.42368551337024274\n",
      "Iteration: 1381, training loss: 0.4236573986732667\n",
      "Iteration: 1382, training loss: 0.42362931283615973\n",
      "Iteration: 1383, training loss: 0.4236012558082342\n",
      "Iteration: 1384, training loss: 0.4235732275389226\n",
      "Iteration: 1385, training loss: 0.42354522797777766\n",
      "Iteration: 1386, training loss: 0.42351725707447163\n",
      "Iteration: 1387, training loss: 0.42348931477879587\n",
      "Iteration: 1388, training loss: 0.4234614010406611\n",
      "Iteration: 1389, training loss: 0.4234335158100965\n",
      "Iteration: 1390, training loss: 0.4234056590372494\n",
      "Iteration: 1391, training loss: 0.4233778306723856\n",
      "Iteration: 1392, training loss: 0.423350030665888\n",
      "Iteration: 1393, training loss: 0.423322258968257\n",
      "Iteration: 1394, training loss: 0.42329451553011005\n",
      "Iteration: 1395, training loss: 0.42326680030218106\n",
      "Iteration: 1396, training loss: 0.42323911323532054\n",
      "Iteration: 1397, training loss: 0.4232114542804945\n",
      "Iteration: 1398, training loss: 0.4231838233887852\n",
      "Iteration: 1399, training loss: 0.4231562205113896\n",
      "Iteration: 1400, training loss: 0.4231286455996201\n",
      "Iteration: 1401, training loss: 0.4231010986049035\n",
      "Iteration: 1402, training loss: 0.42307357947878116\n",
      "Iteration: 1403, training loss: 0.4230460881729083\n",
      "Iteration: 1404, training loss: 0.4230186246390538\n",
      "Iteration: 1405, training loss: 0.4229911888291\n",
      "Iteration: 1406, training loss: 0.42296378069504226\n",
      "Iteration: 1407, training loss: 0.42293640018898865\n",
      "Iteration: 1408, training loss: 0.4229090472631598\n",
      "Iteration: 1409, training loss: 0.4228817218698882\n",
      "Iteration: 1410, training loss: 0.42285442396161826\n",
      "Iteration: 1411, training loss: 0.4228271534909058\n",
      "Iteration: 1412, training loss: 0.4227999104104179\n",
      "Iteration: 1413, training loss: 0.4227726946729322\n",
      "Iteration: 1414, training loss: 0.4227455062313373\n",
      "Iteration: 1415, training loss: 0.4227183450386314\n",
      "Iteration: 1416, training loss: 0.42269121104792323\n",
      "Iteration: 1417, training loss: 0.4226641042124307\n",
      "Iteration: 1418, training loss: 0.42263702448548124\n",
      "Iteration: 1419, training loss: 0.42260997182051113\n",
      "Iteration: 1420, training loss: 0.4225829461710651\n",
      "Iteration: 1421, training loss: 0.4225559474907967\n",
      "Iteration: 1422, training loss: 0.42252897573346737\n",
      "Iteration: 1423, training loss: 0.422502030852946\n",
      "Iteration: 1424, training loss: 0.4224751128032093\n",
      "Iteration: 1425, training loss: 0.4224482215383411\n",
      "Iteration: 1426, training loss: 0.42242135701253175\n",
      "Iteration: 1427, training loss: 0.4223945191800786\n",
      "Iteration: 1428, training loss: 0.4223677079953848\n",
      "Iteration: 1429, training loss: 0.4223409234129598\n",
      "Iteration: 1430, training loss: 0.4223141653874186\n",
      "Iteration: 1431, training loss: 0.42228743387348155\n",
      "Iteration: 1432, training loss: 0.42226072882597393\n",
      "Iteration: 1433, training loss: 0.42223405019982596\n",
      "Iteration: 1434, training loss: 0.42220739795007234\n",
      "Iteration: 1435, training loss: 0.42218077203185195\n",
      "Iteration: 1436, training loss: 0.4221541724004074\n",
      "Iteration: 1437, training loss: 0.422127599011085\n",
      "Iteration: 1438, training loss: 0.4221010518193344\n",
      "Iteration: 1439, training loss: 0.42207453078070845\n",
      "Iteration: 1440, training loss: 0.42204803585086226\n",
      "Iteration: 1441, training loss: 0.4220215669855538\n",
      "Iteration: 1442, training loss: 0.4219951241406432\n",
      "Iteration: 1443, training loss: 0.42196870727209224\n",
      "Iteration: 1444, training loss: 0.4219423163359645\n",
      "Iteration: 1445, training loss: 0.4219159512884246\n",
      "Iteration: 1446, training loss: 0.42188961208573844\n",
      "Iteration: 1447, training loss: 0.4218632986842727\n",
      "Iteration: 1448, training loss: 0.4218370110404944\n",
      "Iteration: 1449, training loss: 0.4218107491109707\n",
      "Iteration: 1450, training loss: 0.4217845128523688\n",
      "Iteration: 1451, training loss: 0.4217583022214555\n",
      "Iteration: 1452, training loss: 0.4217321171750969\n",
      "Iteration: 1453, training loss: 0.42170595767025826\n",
      "Iteration: 1454, training loss: 0.4216798236640036\n",
      "Iteration: 1455, training loss: 0.4216537151134955\n",
      "Iteration: 1456, training loss: 0.4216276319759946\n",
      "Iteration: 1457, training loss: 0.4216015742088599\n",
      "Iteration: 1458, training loss: 0.4215755417695478\n",
      "Iteration: 1459, training loss: 0.4215495346156124\n",
      "Iteration: 1460, training loss: 0.42152355270470454\n",
      "Iteration: 1461, training loss: 0.4214975959945726\n",
      "Iteration: 1462, training loss: 0.421471664443061\n",
      "Iteration: 1463, training loss: 0.42144575800811096\n",
      "Iteration: 1464, training loss: 0.4214198766477594\n",
      "Iteration: 1465, training loss: 0.4213940203201393\n",
      "Iteration: 1466, training loss: 0.4213681889834795\n",
      "Iteration: 1467, training loss: 0.4213423825961035\n",
      "Iteration: 1468, training loss: 0.4213166011164304\n",
      "Iteration: 1469, training loss: 0.4212908445029739\n",
      "Iteration: 1470, training loss: 0.421265112714342\n",
      "Iteration: 1471, training loss: 0.42123940570923735\n",
      "Iteration: 1472, training loss: 0.4212137234464563\n",
      "Iteration: 1473, training loss: 0.42118806588488905\n",
      "Iteration: 1474, training loss: 0.4211624329835193\n",
      "Iteration: 1475, training loss: 0.42113682470142405\n",
      "Iteration: 1476, training loss: 0.421111240997773\n",
      "Iteration: 1477, training loss: 0.4210856818318289\n",
      "Iteration: 1478, training loss: 0.42106014716294665\n",
      "Iteration: 1479, training loss: 0.4210346369505736\n",
      "Iteration: 1480, training loss: 0.42100915115424875\n",
      "Iteration: 1481, training loss: 0.42098368973360323\n",
      "Iteration: 1482, training loss: 0.42095825264835923\n",
      "Iteration: 1483, training loss: 0.42093283985833035\n",
      "Iteration: 1484, training loss: 0.42090745132342117\n",
      "Iteration: 1485, training loss: 0.42088208700362667\n",
      "Iteration: 1486, training loss: 0.42085674685903246\n",
      "Iteration: 1487, training loss: 0.42083143084981456\n",
      "Iteration: 1488, training loss: 0.42080613893623847\n",
      "Iteration: 1489, training loss: 0.4207808710786599\n",
      "Iteration: 1490, training loss: 0.4207556272375237\n",
      "Iteration: 1491, training loss: 0.42073040737336387\n",
      "Iteration: 1492, training loss: 0.42070521144680373\n",
      "Iteration: 1493, training loss: 0.420680039418555\n",
      "Iteration: 1494, training loss: 0.420654891249418\n",
      "Iteration: 1495, training loss: 0.42062976690028137\n",
      "Iteration: 1496, training loss: 0.4206046663321217\n",
      "Iteration: 1497, training loss: 0.4205795895060031\n",
      "Iteration: 1498, training loss: 0.42055453638307777\n",
      "Iteration: 1499, training loss: 0.4205295069245846\n",
      "Iteration: 1500, training loss: 0.4205045010918498\n",
      "Iteration: 1501, training loss: 0.4204795188462863\n",
      "Iteration: 1502, training loss: 0.42045456014939386\n",
      "Iteration: 1503, training loss: 0.42042962496275826\n",
      "Iteration: 1504, training loss: 0.42040471324805134\n",
      "Iteration: 1505, training loss: 0.42037982496703125\n",
      "Iteration: 1506, training loss: 0.4203549600815412\n",
      "Iteration: 1507, training loss: 0.4203301185535103\n",
      "Iteration: 1508, training loss: 0.4203053003449526\n",
      "Iteration: 1509, training loss: 0.4202805054179669\n",
      "Iteration: 1510, training loss: 0.42025573373473707\n",
      "Iteration: 1511, training loss: 0.4202309852575312\n",
      "Iteration: 1512, training loss: 0.4202062599487017\n",
      "Iteration: 1513, training loss: 0.42018155777068505\n",
      "Iteration: 1514, training loss: 0.42015687868600154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1515, training loss: 0.42013222265725486\n",
      "Iteration: 1516, training loss: 0.42010758964713213\n",
      "Iteration: 1517, training loss: 0.42008297961840363\n",
      "Iteration: 1518, training loss: 0.42005839253392235\n",
      "Iteration: 1519, training loss: 0.4200338283566243\n",
      "Iteration: 1520, training loss: 0.42000928704952745\n",
      "Iteration: 1521, training loss: 0.41998476857573247\n",
      "Iteration: 1522, training loss: 0.4199602728984215\n",
      "Iteration: 1523, training loss: 0.419935799980859\n",
      "Iteration: 1524, training loss: 0.41991134978639055\n",
      "Iteration: 1525, training loss: 0.4198869222784432\n",
      "Iteration: 1526, training loss: 0.41986251742052516\n",
      "Iteration: 1527, training loss: 0.4198381351762256\n",
      "Iteration: 1528, training loss: 0.4198137755092141\n",
      "Iteration: 1529, training loss: 0.41978943838324095\n",
      "Iteration: 1530, training loss: 0.4197651237621365\n",
      "Iteration: 1531, training loss: 0.4197408316098113\n",
      "Iteration: 1532, training loss: 0.41971656189025547\n",
      "Iteration: 1533, training loss: 0.4196923145675389\n",
      "Iteration: 1534, training loss: 0.4196680896058109\n",
      "Iteration: 1535, training loss: 0.41964388696929983\n",
      "Iteration: 1536, training loss: 0.41961970662231296\n",
      "Iteration: 1537, training loss: 0.4195955485292365\n",
      "Iteration: 1538, training loss: 0.41957141265453485\n",
      "Iteration: 1539, training loss: 0.419547298962751\n",
      "Iteration: 1540, training loss: 0.419523207418506\n",
      "Iteration: 1541, training loss: 0.41949913798649885\n",
      "Iteration: 1542, training loss: 0.41947509063150595\n",
      "Iteration: 1543, training loss: 0.4194510653183816\n",
      "Iteration: 1544, training loss: 0.41942706201205704\n",
      "Iteration: 1545, training loss: 0.4194030806775406\n",
      "Iteration: 1546, training loss: 0.4193791212799177\n",
      "Iteration: 1547, training loss: 0.41935518378435027\n",
      "Iteration: 1548, training loss: 0.4193312681560765\n",
      "Iteration: 1549, training loss: 0.41930737436041127\n",
      "Iteration: 1550, training loss: 0.419283502362745\n",
      "Iteration: 1551, training loss: 0.41925965212854427\n",
      "Iteration: 1552, training loss: 0.41923582362335116\n",
      "Iteration: 1553, training loss: 0.4192120168127832\n",
      "Iteration: 1554, training loss: 0.4191882316625332\n",
      "Iteration: 1555, training loss: 0.419164468138369\n",
      "Iteration: 1556, training loss: 0.41914072620613313\n",
      "Iteration: 1557, training loss: 0.41911700583174294\n",
      "Iteration: 1558, training loss: 0.41909330698119\n",
      "Iteration: 1559, training loss: 0.4190696296205403\n",
      "Iteration: 1560, training loss: 0.4190459737159338\n",
      "Iteration: 1561, training loss: 0.4190223392335842\n",
      "Iteration: 1562, training loss: 0.418998726139779\n",
      "Iteration: 1563, training loss: 0.4189751344008791\n",
      "Iteration: 1564, training loss: 0.41895156398331834\n",
      "Iteration: 1565, training loss: 0.41892801485360426\n",
      "Iteration: 1566, training loss: 0.41890448697831667\n",
      "Iteration: 1567, training loss: 0.4188809803241082\n",
      "Iteration: 1568, training loss: 0.41885749485770407\n",
      "Iteration: 1569, training loss: 0.41883403054590185\n",
      "Iteration: 1570, training loss: 0.4188105873555708\n",
      "Iteration: 1571, training loss: 0.4187871652536526\n",
      "Iteration: 1572, training loss: 0.4187637642071602\n",
      "Iteration: 1573, training loss: 0.41874038418317827\n",
      "Iteration: 1574, training loss: 0.4187170251488628\n",
      "Iteration: 1575, training loss: 0.41869368707144095\n",
      "Iteration: 1576, training loss: 0.41867036991821066\n",
      "Iteration: 1577, training loss: 0.4186470736565408\n",
      "Iteration: 1578, training loss: 0.4186237982538707\n",
      "Iteration: 1579, training loss: 0.41860054367771027\n",
      "Iteration: 1580, training loss: 0.41857730989563935\n",
      "Iteration: 1581, training loss: 0.4185540968753079\n",
      "Iteration: 1582, training loss: 0.4185309045844359\n",
      "Iteration: 1583, training loss: 0.4185077329908128\n",
      "Iteration: 1584, training loss: 0.41848458206229733\n",
      "Iteration: 1585, training loss: 0.4184614517668179\n",
      "Iteration: 1586, training loss: 0.4184383420723717\n",
      "Iteration: 1587, training loss: 0.418415252947025\n",
      "Iteration: 1588, training loss: 0.4183921843589127\n",
      "Iteration: 1589, training loss: 0.41836913627623834\n",
      "Iteration: 1590, training loss: 0.41834610866727373\n",
      "Iteration: 1591, training loss: 0.41832310150035906\n",
      "Iteration: 1592, training loss: 0.41830011474390233\n",
      "Iteration: 1593, training loss: 0.4182771483663794\n",
      "Iteration: 1594, training loss: 0.4182542023363339\n",
      "Iteration: 1595, training loss: 0.41823127662237686\n",
      "Iteration: 1596, training loss: 0.4182083711931866\n",
      "Iteration: 1597, training loss: 0.41818548601750855\n",
      "Iteration: 1598, training loss: 0.4181626210641551\n",
      "Iteration: 1599, training loss: 0.4181397763020055\n",
      "Iteration: 1600, training loss: 0.4181169517000055\n",
      "Iteration: 1601, training loss: 0.41809414722716726\n",
      "Iteration: 1602, training loss: 0.41807136285256924\n",
      "Iteration: 1603, training loss: 0.4180485985453559\n",
      "Iteration: 1604, training loss: 0.41802585427473776\n",
      "Iteration: 1605, training loss: 0.41800313000999084\n",
      "Iteration: 1606, training loss: 0.41798042572045696\n",
      "Iteration: 1607, training loss: 0.41795774137554303\n",
      "Iteration: 1608, training loss: 0.41793507694472143\n",
      "Iteration: 1609, training loss: 0.4179124323975295\n",
      "Iteration: 1610, training loss: 0.4178898077035695\n",
      "Iteration: 1611, training loss: 0.41786720283250817\n",
      "Iteration: 1612, training loss: 0.41784461775407705\n",
      "Iteration: 1613, training loss: 0.4178220524380721\n",
      "Iteration: 1614, training loss: 0.41779950685435296\n",
      "Iteration: 1615, training loss: 0.417776980972844\n",
      "Iteration: 1616, training loss: 0.4177544747635329\n",
      "Iteration: 1617, training loss: 0.41773198819647145\n",
      "Iteration: 1618, training loss: 0.4177095212417745\n",
      "Iteration: 1619, training loss: 0.41768707386962084\n",
      "Iteration: 1620, training loss: 0.41766464605025205\n",
      "Iteration: 1621, training loss: 0.4176422377539727\n",
      "Iteration: 1622, training loss: 0.4176198489511507\n",
      "Iteration: 1623, training loss: 0.4175974796122161\n",
      "Iteration: 1624, training loss: 0.41757512970766175\n",
      "Iteration: 1625, training loss: 0.4175527992080431\n",
      "Iteration: 1626, training loss: 0.4175304880839774\n",
      "Iteration: 1627, training loss: 0.4175081963061443\n",
      "Iteration: 1628, training loss: 0.41748592384528516\n",
      "Iteration: 1629, training loss: 0.41746367067220314\n",
      "Iteration: 1630, training loss: 0.4174414367577629\n",
      "Iteration: 1631, training loss: 0.4174192220728907\n",
      "Iteration: 1632, training loss: 0.41739702658857397\n",
      "Iteration: 1633, training loss: 0.4173748502758611\n",
      "Iteration: 1634, training loss: 0.41735269310586165\n",
      "Iteration: 1635, training loss: 0.4173305550497457\n",
      "Iteration: 1636, training loss: 0.4173084360787443\n",
      "Iteration: 1637, training loss: 0.4172863361641487\n",
      "Iteration: 1638, training loss: 0.41726425527731054\n",
      "Iteration: 1639, training loss: 0.4172421933896417\n",
      "Iteration: 1640, training loss: 0.4172201504726139\n",
      "Iteration: 1641, training loss: 0.4171981264977588\n",
      "Iteration: 1642, training loss: 0.4171761214366679\n",
      "Iteration: 1643, training loss: 0.4171541352609919\n",
      "Iteration: 1644, training loss: 0.4171321679424412\n",
      "Iteration: 1645, training loss: 0.41711021945278526\n",
      "Iteration: 1646, training loss: 0.4170882897638526\n",
      "Iteration: 1647, training loss: 0.417066378847531\n",
      "Iteration: 1648, training loss: 0.4170444866757664\n",
      "Iteration: 1649, training loss: 0.417022613220564\n",
      "Iteration: 1650, training loss: 0.4170007584539871\n",
      "Iteration: 1651, training loss: 0.4169789223481575\n",
      "Iteration: 1652, training loss: 0.416957104875255\n",
      "Iteration: 1653, training loss: 0.41693530600751755\n",
      "Iteration: 1654, training loss: 0.4169135257172409\n",
      "Iteration: 1655, training loss: 0.4168917639767786\n",
      "Iteration: 1656, training loss: 0.4168700207585417\n",
      "Iteration: 1657, training loss: 0.4168482960349989\n",
      "Iteration: 1658, training loss: 0.4168265897786759\n",
      "Iteration: 1659, training loss: 0.4168049019621557\n",
      "Iteration: 1660, training loss: 0.41678323255807803\n",
      "Iteration: 1661, training loss: 0.41676158153914\n",
      "Iteration: 1662, training loss: 0.41673994887809496\n",
      "Iteration: 1663, training loss: 0.4167183345477529\n",
      "Iteration: 1664, training loss: 0.4166967385209803\n",
      "Iteration: 1665, training loss: 0.41667516077069994\n",
      "Iteration: 1666, training loss: 0.41665360126989054\n",
      "Iteration: 1667, training loss: 0.416632059991587\n",
      "Iteration: 1668, training loss: 0.41661053690887984\n",
      "Iteration: 1669, training loss: 0.4165890319949156\n",
      "Iteration: 1670, training loss: 0.416567545222896\n",
      "Iteration: 1671, training loss: 0.41654607656607845\n",
      "Iteration: 1672, training loss: 0.4165246259977754\n",
      "Iteration: 1673, training loss: 0.41650319349135456\n",
      "Iteration: 1674, training loss: 0.41648177902023875\n",
      "Iteration: 1675, training loss: 0.41646038255790524\n",
      "Iteration: 1676, training loss: 0.4164390040778864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1677, training loss: 0.4164176435537691\n",
      "Iteration: 1678, training loss: 0.4163963009591944\n",
      "Iteration: 1679, training loss: 0.4163749762678578\n",
      "Iteration: 1680, training loss: 0.4163536694535091\n",
      "Iteration: 1681, training loss: 0.4163323804899518\n",
      "Iteration: 1682, training loss: 0.4163111093510435\n",
      "Iteration: 1683, training loss: 0.4162898560106955\n",
      "Iteration: 1684, training loss: 0.41626862044287277\n",
      "Iteration: 1685, training loss: 0.41624740262159343\n",
      "Iteration: 1686, training loss: 0.41622620252092946\n",
      "Iteration: 1687, training loss: 0.41620502011500543\n",
      "Iteration: 1688, training loss: 0.41618385537799946\n",
      "Iteration: 1689, training loss: 0.4161627082841425\n",
      "Iteration: 1690, training loss: 0.4161415788077178\n",
      "Iteration: 1691, training loss: 0.41612046692306215\n",
      "Iteration: 1692, training loss: 0.4160993726045641\n",
      "Iteration: 1693, training loss: 0.41607829582666495\n",
      "Iteration: 1694, training loss: 0.41605723656385823\n",
      "Iteration: 1695, training loss: 0.4160361947906895\n",
      "Iteration: 1696, training loss: 0.4160151704817563\n",
      "Iteration: 1697, training loss: 0.4159941636117084\n",
      "Iteration: 1698, training loss: 0.4159731741552466\n",
      "Iteration: 1699, training loss: 0.4159522020871239\n",
      "Iteration: 1700, training loss: 0.4159312473821447\n",
      "Iteration: 1701, training loss: 0.41591031001516443\n",
      "Iteration: 1702, training loss: 0.41588938996109\n",
      "Iteration: 1703, training loss: 0.41586848719487945\n",
      "Iteration: 1704, training loss: 0.41584760169154145\n",
      "Iteration: 1705, training loss: 0.41582673342613585\n",
      "Iteration: 1706, training loss: 0.41580588237377303\n",
      "Iteration: 1707, training loss: 0.41578504850961384\n",
      "Iteration: 1708, training loss: 0.4157642318088699\n",
      "Iteration: 1709, training loss: 0.41574343224680277\n",
      "Iteration: 1710, training loss: 0.4157226497987246\n",
      "Iteration: 1711, training loss: 0.41570188443999706\n",
      "Iteration: 1712, training loss: 0.41568113614603236\n",
      "Iteration: 1713, training loss: 0.41566040489229217\n",
      "Iteration: 1714, training loss: 0.41563969065428785\n",
      "Iteration: 1715, training loss: 0.4156189934075804\n",
      "Iteration: 1716, training loss: 0.41559831312778034\n",
      "Iteration: 1717, training loss: 0.41557764979054734\n",
      "Iteration: 1718, training loss: 0.4155570033715904\n",
      "Iteration: 1719, training loss: 0.4155363738466676\n",
      "Iteration: 1720, training loss: 0.4155157611915857\n",
      "Iteration: 1721, training loss: 0.41549516538220066\n",
      "Iteration: 1722, training loss: 0.4154745863944168\n",
      "Iteration: 1723, training loss: 0.4154540242041874\n",
      "Iteration: 1724, training loss: 0.4154334787875138\n",
      "Iteration: 1725, training loss: 0.4154129501204459\n",
      "Iteration: 1726, training loss: 0.41539243817908184\n",
      "Iteration: 1727, training loss: 0.41537194293956764\n",
      "Iteration: 1728, training loss: 0.41535146437809767\n",
      "Iteration: 1729, training loss: 0.41533100247091376\n",
      "Iteration: 1730, training loss: 0.4153105571943057\n",
      "Iteration: 1731, training loss: 0.41529012852461084\n",
      "Iteration: 1732, training loss: 0.41526971643821414\n",
      "Iteration: 1733, training loss: 0.41524932091154754\n",
      "Iteration: 1734, training loss: 0.4152289419210908\n",
      "Iteration: 1735, training loss: 0.4152085794433705\n",
      "Iteration: 1736, training loss: 0.4151882334549603\n",
      "Iteration: 1737, training loss: 0.4151679039324808\n",
      "Iteration: 1738, training loss: 0.4151475908525993\n",
      "Iteration: 1739, training loss: 0.41512729419203\n",
      "Iteration: 1740, training loss: 0.41510701392753335\n",
      "Iteration: 1741, training loss: 0.41508675003591655\n",
      "Iteration: 1742, training loss: 0.4150665024940328\n",
      "Iteration: 1743, training loss: 0.41504627127878196\n",
      "Iteration: 1744, training loss: 0.4150260563671095\n",
      "Iteration: 1745, training loss: 0.41500585773600723\n",
      "Iteration: 1746, training loss: 0.4149856753625127\n",
      "Iteration: 1747, training loss: 0.4149655092237093\n",
      "Iteration: 1748, training loss: 0.4149453592967259\n",
      "Iteration: 1749, training loss: 0.41492522555873695\n",
      "Iteration: 1750, training loss: 0.41490510798696256\n",
      "Iteration: 1751, training loss: 0.4148850065586679\n",
      "Iteration: 1752, training loss: 0.4148649212511633\n",
      "Iteration: 1753, training loss: 0.4148448520418044\n",
      "Iteration: 1754, training loss: 0.41482479890799157\n",
      "Iteration: 1755, training loss: 0.41480476182717013\n",
      "Iteration: 1756, training loss: 0.4147847407768304\n",
      "Iteration: 1757, training loss: 0.4147647357345068\n",
      "Iteration: 1758, training loss: 0.41474474667777883\n",
      "Iteration: 1759, training loss: 0.4147247735842701\n",
      "Iteration: 1760, training loss: 0.41470481643164864\n",
      "Iteration: 1761, training loss: 0.41468487519762653\n",
      "Iteration: 1762, training loss: 0.41466494985996016\n",
      "Iteration: 1763, training loss: 0.41464504039644984\n",
      "Iteration: 1764, training loss: 0.4146251467849396\n",
      "Iteration: 1765, training loss: 0.41460526900331746\n",
      "Iteration: 1766, training loss: 0.4145854070295149\n",
      "Iteration: 1767, training loss: 0.41456556084150714\n",
      "Iteration: 1768, training loss: 0.41454573041731274\n",
      "Iteration: 1769, training loss: 0.4145259157349936\n",
      "Iteration: 1770, training loss: 0.41450611677265475\n",
      "Iteration: 1771, training loss: 0.4144863335084447\n",
      "Iteration: 1772, training loss: 0.41446656592055453\n",
      "Iteration: 1773, training loss: 0.4144468139872185\n",
      "Iteration: 1774, training loss: 0.41442707768671366\n",
      "Iteration: 1775, training loss: 0.4144073569973598\n",
      "Iteration: 1776, training loss: 0.41438765189751914\n",
      "Iteration: 1777, training loss: 0.4143679623655965\n",
      "Iteration: 1778, training loss: 0.414348288380039\n",
      "Iteration: 1779, training loss: 0.4143286299193364\n",
      "Iteration: 1780, training loss: 0.41430898696202023\n",
      "Iteration: 1781, training loss: 0.4142893594866641\n",
      "Iteration: 1782, training loss: 0.41426974747188405\n",
      "Iteration: 1783, training loss: 0.4142501508963375\n",
      "Iteration: 1784, training loss: 0.41423056973872385\n",
      "Iteration: 1785, training loss: 0.41421100397778426\n",
      "Iteration: 1786, training loss: 0.4141914535923012\n",
      "Iteration: 1787, training loss: 0.414171918561099\n",
      "Iteration: 1788, training loss: 0.4141523988630429\n",
      "Iteration: 1789, training loss: 0.41413289447703977\n",
      "Iteration: 1790, training loss: 0.41411340538203745\n",
      "Iteration: 1791, training loss: 0.4140939315570249\n",
      "Iteration: 1792, training loss: 0.41407447298103217\n",
      "Iteration: 1793, training loss: 0.41405502963312985\n",
      "Iteration: 1794, training loss: 0.41403560149242963\n",
      "Iteration: 1795, training loss: 0.41401618853808375\n",
      "Iteration: 1796, training loss: 0.41399679074928497\n",
      "Iteration: 1797, training loss: 0.41397740810526656\n",
      "Iteration: 1798, training loss: 0.4139580405853022\n",
      "Iteration: 1799, training loss: 0.41393868816870577\n",
      "Iteration: 1800, training loss: 0.41391935083483133\n",
      "Iteration: 1801, training loss: 0.4139000285630731\n",
      "Iteration: 1802, training loss: 0.41388072133286524\n",
      "Iteration: 1803, training loss: 0.4138614291236817\n",
      "Iteration: 1804, training loss: 0.4138421519150363\n",
      "Iteration: 1805, training loss: 0.41382288968648256\n",
      "Iteration: 1806, training loss: 0.4138036424176138\n",
      "Iteration: 1807, training loss: 0.41378441008806227\n",
      "Iteration: 1808, training loss: 0.41376519267750017\n",
      "Iteration: 1809, training loss: 0.41374599016563884\n",
      "Iteration: 1810, training loss: 0.4137268025322289\n",
      "Iteration: 1811, training loss: 0.4137076297570596\n",
      "Iteration: 1812, training loss: 0.41368847181995994\n",
      "Iteration: 1813, training loss: 0.4136693287007975\n",
      "Iteration: 1814, training loss: 0.4136502003794786\n",
      "Iteration: 1815, training loss: 0.4136310868359485\n",
      "Iteration: 1816, training loss: 0.41361198805019106\n",
      "Iteration: 1817, training loss: 0.4135929040022285\n",
      "Iteration: 1818, training loss: 0.41357383467212167\n",
      "Iteration: 1819, training loss: 0.41355478003997004\n",
      "Iteration: 1820, training loss: 0.4135357400859109\n",
      "Iteration: 1821, training loss: 0.4135167147901199\n",
      "Iteration: 1822, training loss: 0.41349770413281084\n",
      "Iteration: 1823, training loss: 0.4134787080942355\n",
      "Iteration: 1824, training loss: 0.41345972665468356\n",
      "Iteration: 1825, training loss: 0.4134407597944825\n",
      "Iteration: 1826, training loss: 0.4134218074939976\n",
      "Iteration: 1827, training loss: 0.41340286973363155\n",
      "Iteration: 1828, training loss: 0.4133839464938249\n",
      "Iteration: 1829, training loss: 0.4133650377550555\n",
      "Iteration: 1830, training loss: 0.4133461434978386\n",
      "Iteration: 1831, training loss: 0.41332726370272654\n",
      "Iteration: 1832, training loss: 0.4133083983503093\n",
      "Iteration: 1833, training loss: 0.41328954742121343\n",
      "Iteration: 1834, training loss: 0.41327071089610284\n",
      "Iteration: 1835, training loss: 0.4132518887556782\n",
      "Iteration: 1836, training loss: 0.41323308098067707\n",
      "Iteration: 1837, training loss: 0.41321428755187384\n",
      "Iteration: 1838, training loss: 0.4131955084500793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1839, training loss: 0.41317674365614115\n",
      "Iteration: 1840, training loss: 0.4131579931509433\n",
      "Iteration: 1841, training loss: 0.4131392569154062\n",
      "Iteration: 1842, training loss: 0.41312053493048645\n",
      "Iteration: 1843, training loss: 0.4131018271771771\n",
      "Iteration: 1844, training loss: 0.4130831336365072\n",
      "Iteration: 1845, training loss: 0.4130644542895418\n",
      "Iteration: 1846, training loss: 0.413045789117382\n",
      "Iteration: 1847, training loss: 0.41302713810116487\n",
      "Iteration: 1848, training loss: 0.413008501222063\n",
      "Iteration: 1849, training loss: 0.4129898784612848\n",
      "Iteration: 1850, training loss: 0.4129712698000744\n",
      "Iteration: 1851, training loss: 0.41295267521971146\n",
      "Iteration: 1852, training loss: 0.412934094701511\n",
      "Iteration: 1853, training loss: 0.4129155282268235\n",
      "Iteration: 1854, training loss: 0.41289697577703455\n",
      "Iteration: 1855, training loss: 0.4128784373335651\n",
      "Iteration: 1856, training loss: 0.4128599128778712\n",
      "Iteration: 1857, training loss: 0.41284140239144385\n",
      "Iteration: 1858, training loss: 0.41282290585580905\n",
      "Iteration: 1859, training loss: 0.4128044232525277\n",
      "Iteration: 1860, training loss: 0.41278595456319545\n",
      "Iteration: 1861, training loss: 0.41276749976944277\n",
      "Iteration: 1862, training loss: 0.4127490588529344\n",
      "Iteration: 1863, training loss: 0.41273063179536995\n",
      "Iteration: 1864, training loss: 0.4127122185784834\n",
      "Iteration: 1865, training loss: 0.412693819184043\n",
      "Iteration: 1866, training loss: 0.41267543359385156\n",
      "Iteration: 1867, training loss: 0.4126570617897457\n",
      "Iteration: 1868, training loss: 0.41263870375359657\n",
      "Iteration: 1869, training loss: 0.4126203594673091\n",
      "Iteration: 1870, training loss: 0.41260202891282216\n",
      "Iteration: 1871, training loss: 0.4125837120721088\n",
      "Iteration: 1872, training loss: 0.41256540892717564\n",
      "Iteration: 1873, training loss: 0.4125471194600631\n",
      "Iteration: 1874, training loss: 0.41252884365284526\n",
      "Iteration: 1875, training loss: 0.4125105814876297\n",
      "Iteration: 1876, training loss: 0.41249233294655757\n",
      "Iteration: 1877, training loss: 0.41247409801180335\n",
      "Iteration: 1878, training loss: 0.412455876665575\n",
      "Iteration: 1879, training loss: 0.4124376688901135\n",
      "Iteration: 1880, training loss: 0.41241947466769335\n",
      "Iteration: 1881, training loss: 0.41240129398062175\n",
      "Iteration: 1882, training loss: 0.41238312681123923\n",
      "Iteration: 1883, training loss: 0.4123649731419191\n",
      "Iteration: 1884, training loss: 0.41234683295506763\n",
      "Iteration: 1885, training loss: 0.4123287062331236\n",
      "Iteration: 1886, training loss: 0.41231059295855915\n",
      "Iteration: 1887, training loss: 0.41229249311387844\n",
      "Iteration: 1888, training loss: 0.4122744066816182\n",
      "Iteration: 1889, training loss: 0.4122563336443481\n",
      "Iteration: 1890, training loss: 0.4122382739846697\n",
      "Iteration: 1891, training loss: 0.41222022768521743\n",
      "Iteration: 1892, training loss: 0.41220219472865743\n",
      "Iteration: 1893, training loss: 0.41218417509768834\n",
      "Iteration: 1894, training loss: 0.4121661687750408\n",
      "Iteration: 1895, training loss: 0.41214817574347756\n",
      "Iteration: 1896, training loss: 0.41213019598579326\n",
      "Iteration: 1897, training loss: 0.4121122294848144\n",
      "Iteration: 1898, training loss: 0.4120942762233993\n",
      "Iteration: 1899, training loss: 0.41207633618443806\n",
      "Iteration: 1900, training loss: 0.41205840935085225\n",
      "Iteration: 1901, training loss: 0.41204049570559537\n",
      "Iteration: 1902, training loss: 0.4120225952316521\n",
      "Iteration: 1903, training loss: 0.41200470791203875\n",
      "Iteration: 1904, training loss: 0.41198683372980277\n",
      "Iteration: 1905, training loss: 0.41196897266802324\n",
      "Iteration: 1906, training loss: 0.4119511247098102\n",
      "Iteration: 1907, training loss: 0.4119332898383048\n",
      "Iteration: 1908, training loss: 0.4119154680366794\n",
      "Iteration: 1909, training loss: 0.4118976592881374\n",
      "Iteration: 1910, training loss: 0.4118798635759129\n",
      "Iteration: 1911, training loss: 0.4118620808832711\n",
      "Iteration: 1912, training loss: 0.4118443111935077\n",
      "Iteration: 1913, training loss: 0.4118265544899494\n",
      "Iteration: 1914, training loss: 0.41180881075595355\n",
      "Iteration: 1915, training loss: 0.4117910799749077\n",
      "Iteration: 1916, training loss: 0.4117733621302303\n",
      "Iteration: 1917, training loss: 0.4117556572053699\n",
      "Iteration: 1918, training loss: 0.4117379651838057\n",
      "Iteration: 1919, training loss: 0.4117202860490469\n",
      "Iteration: 1920, training loss: 0.41170261978463324\n",
      "Iteration: 1921, training loss: 0.4116849663741345\n",
      "Iteration: 1922, training loss: 0.4116673258011501\n",
      "Iteration: 1923, training loss: 0.4116496980493102\n",
      "Iteration: 1924, training loss: 0.4116320831022743\n",
      "Iteration: 1925, training loss: 0.41161448094373204\n",
      "Iteration: 1926, training loss: 0.41159689155740287\n",
      "Iteration: 1927, training loss: 0.4115793149270358\n",
      "Iteration: 1928, training loss: 0.41156175103640974\n",
      "Iteration: 1929, training loss: 0.41154419986933305\n",
      "Iteration: 1930, training loss: 0.41152666140964345\n",
      "Iteration: 1931, training loss: 0.41150913564120845\n",
      "Iteration: 1932, training loss: 0.4114916225479248\n",
      "Iteration: 1933, training loss: 0.4114741221137185\n",
      "Iteration: 1934, training loss: 0.4114566343225448\n",
      "Iteration: 1935, training loss: 0.4114391591583882\n",
      "Iteration: 1936, training loss: 0.4114216966052625\n",
      "Iteration: 1937, training loss: 0.4114042466472102\n",
      "Iteration: 1938, training loss: 0.4113868092683029\n",
      "Iteration: 1939, training loss: 0.4113693844526413\n",
      "Iteration: 1940, training loss: 0.41135197218435465\n",
      "Iteration: 1941, training loss: 0.4113345724476013\n",
      "Iteration: 1942, training loss: 0.4113171852265681\n",
      "Iteration: 1943, training loss: 0.4112998105054706\n",
      "Iteration: 1944, training loss: 0.411282448268553\n",
      "Iteration: 1945, training loss: 0.41126509850008774\n",
      "Iteration: 1946, training loss: 0.4112477611843764\n",
      "Iteration: 1947, training loss: 0.41123043630574807\n",
      "Iteration: 1948, training loss: 0.4112131238485608\n",
      "Iteration: 1949, training loss: 0.4111958237972006\n",
      "Iteration: 1950, training loss: 0.41117853613608185\n",
      "Iteration: 1951, training loss: 0.41116126084964694\n",
      "Iteration: 1952, training loss: 0.4111439979223664\n",
      "Iteration: 1953, training loss: 0.4111267473387386\n",
      "Iteration: 1954, training loss: 0.41110950908329014\n",
      "Iteration: 1955, training loss: 0.4110922831405752\n",
      "Iteration: 1956, training loss: 0.411075069495176\n",
      "Iteration: 1957, training loss: 0.4110578681317023\n",
      "Iteration: 1958, training loss: 0.41104067903479174\n",
      "Iteration: 1959, training loss: 0.4110235021891094\n",
      "Iteration: 1960, training loss: 0.4110063375793481\n",
      "Iteration: 1961, training loss: 0.410989185190228\n",
      "Iteration: 1962, training loss: 0.4109720450064968\n",
      "Iteration: 1963, training loss: 0.41095491701292947\n",
      "Iteration: 1964, training loss: 0.4109378011943283\n",
      "Iteration: 1965, training loss: 0.410920697535523\n",
      "Iteration: 1966, training loss: 0.41090360602137016\n",
      "Iteration: 1967, training loss: 0.4108865266367538\n",
      "Iteration: 1968, training loss: 0.4108694593665848\n",
      "Iteration: 1969, training loss: 0.4108524041958011\n",
      "Iteration: 1970, training loss: 0.41083536110936764\n",
      "Iteration: 1971, training loss: 0.410818330092276\n",
      "Iteration: 1972, training loss: 0.41080131112954477\n",
      "Iteration: 1973, training loss: 0.41078430420621936\n",
      "Iteration: 1974, training loss: 0.4107673093073719\n",
      "Iteration: 1975, training loss: 0.4107503264181007\n",
      "Iteration: 1976, training loss: 0.41073335552353113\n",
      "Iteration: 1977, training loss: 0.410716396608815\n",
      "Iteration: 1978, training loss: 0.4106994496591304\n",
      "Iteration: 1979, training loss: 0.4106825146596818\n",
      "Iteration: 1980, training loss: 0.41066559159570026\n",
      "Iteration: 1981, training loss: 0.4106486804524428\n",
      "Iteration: 1982, training loss: 0.410631781215193\n",
      "Iteration: 1983, training loss: 0.41061489386926014\n",
      "Iteration: 1984, training loss: 0.4105980183999801\n",
      "Iteration: 1985, training loss: 0.4105811547927144\n",
      "Iteration: 1986, training loss: 0.4105643030328506\n",
      "Iteration: 1987, training loss: 0.41054746310580237\n",
      "Iteration: 1988, training loss: 0.4105306349970092\n",
      "Iteration: 1989, training loss: 0.4105138186919361\n",
      "Iteration: 1990, training loss: 0.4104970141760742\n",
      "Iteration: 1991, training loss: 0.4104802214349401\n",
      "Iteration: 1992, training loss: 0.41046344045407607\n",
      "Iteration: 1993, training loss: 0.41044667121904993\n",
      "Iteration: 1994, training loss: 0.41042991371545506\n",
      "Iteration: 1995, training loss: 0.41041316792891025\n",
      "Iteration: 1996, training loss: 0.4103964338450597\n",
      "Iteration: 1997, training loss: 0.41037971144957297\n",
      "Iteration: 1998, training loss: 0.41036300072814486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1999, training loss: 0.41034630166649544\n",
      "Iteration: 2000, training loss: 0.41032961425036996\n",
      "Iteration: 2001, training loss: 0.4103129384655387\n",
      "Iteration: 2002, training loss: 0.41029627429779714\n",
      "Iteration: 2003, training loss: 0.4102796217329656\n",
      "Iteration: 2004, training loss: 0.41026298075688933\n",
      "Iteration: 2005, training loss: 0.4102463513554387\n",
      "Iteration: 2006, training loss: 0.4102297335145088\n",
      "Iteration: 2007, training loss: 0.41021312722001924\n",
      "Iteration: 2008, training loss: 0.4101965324579148\n",
      "Iteration: 2009, training loss: 0.41017994921416473\n",
      "Iteration: 2010, training loss: 0.41016337747476256\n",
      "Iteration: 2011, training loss: 0.41014681722572693\n",
      "Iteration: 2012, training loss: 0.4101302684531006\n",
      "Iteration: 2013, training loss: 0.4101137311429511\n",
      "Iteration: 2014, training loss: 0.4100972052813699\n",
      "Iteration: 2015, training loss: 0.41008069085447324\n",
      "Iteration: 2016, training loss: 0.41006418784840143\n",
      "Iteration: 2017, training loss: 0.410047696249319\n",
      "Iteration: 2018, training loss: 0.41003121604341475\n",
      "Iteration: 2019, training loss: 0.41001474721690145\n",
      "Iteration: 2020, training loss: 0.40999828975601627\n",
      "Iteration: 2021, training loss: 0.4099818436470199\n",
      "Iteration: 2022, training loss: 0.40996540887619737\n",
      "Iteration: 2023, training loss: 0.40994898542985747\n",
      "Iteration: 2024, training loss: 0.40993257329433286\n",
      "Iteration: 2025, training loss: 0.40991617245598005\n",
      "Iteration: 2026, training loss: 0.4098997829011792\n",
      "Iteration: 2027, training loss: 0.4098834046163343\n",
      "Iteration: 2028, training loss: 0.4098670375878728\n",
      "Iteration: 2029, training loss: 0.4098506818022459\n",
      "Iteration: 2030, training loss: 0.40983433724592827\n",
      "Iteration: 2031, training loss: 0.40981800390541817\n",
      "Iteration: 2032, training loss: 0.40980168176723697\n",
      "Iteration: 2033, training loss: 0.40978537081792976\n",
      "Iteration: 2034, training loss: 0.409769071044065\n",
      "Iteration: 2035, training loss: 0.40975278243223395\n",
      "Iteration: 2036, training loss: 0.4097365049690518\n",
      "Iteration: 2037, training loss: 0.40972023864115614\n",
      "Iteration: 2038, training loss: 0.40970398343520836\n",
      "Iteration: 2039, training loss: 0.40968773933789254\n",
      "Iteration: 2040, training loss: 0.4096715063359158\n",
      "Iteration: 2041, training loss: 0.40965528441600846\n",
      "Iteration: 2042, training loss: 0.4096390735649235\n",
      "Iteration: 2043, training loss: 0.4096228737694369\n",
      "Iteration: 2044, training loss: 0.40960668501634745\n",
      "Iteration: 2045, training loss: 0.4095905072924766\n",
      "Iteration: 2046, training loss: 0.4095743405846688\n",
      "Iteration: 2047, training loss: 0.4095581848797907\n",
      "Iteration: 2048, training loss: 0.40954204016473206\n",
      "Iteration: 2049, training loss: 0.40952590642640485\n",
      "Iteration: 2050, training loss: 0.4095097836517438\n",
      "Iteration: 2051, training loss: 0.4094936718277059\n",
      "Iteration: 2052, training loss: 0.4094775709412708\n",
      "Iteration: 2053, training loss: 0.4094614809794402\n",
      "Iteration: 2054, training loss: 0.40944540192923845\n",
      "Iteration: 2055, training loss: 0.40942933377771196\n",
      "Iteration: 2056, training loss: 0.4094132765119293\n",
      "Iteration: 2057, training loss: 0.4093972301189816\n",
      "Iteration: 2058, training loss: 0.40938119458598177\n",
      "Iteration: 2059, training loss: 0.4093651699000647\n",
      "Iteration: 2060, training loss: 0.4093491560483876\n",
      "Iteration: 2061, training loss: 0.40933315301812956\n",
      "Iteration: 2062, training loss: 0.4093171607964915\n",
      "Iteration: 2063, training loss: 0.40930117937069654\n",
      "Iteration: 2064, training loss: 0.40928520872798907\n",
      "Iteration: 2065, training loss: 0.40926924885563587\n",
      "Iteration: 2066, training loss: 0.40925329974092495\n",
      "Iteration: 2067, training loss: 0.40923736137116656\n",
      "Iteration: 2068, training loss: 0.40922143373369196\n",
      "Iteration: 2069, training loss: 0.4092055168158545\n",
      "Iteration: 2070, training loss: 0.40918961060502884\n",
      "Iteration: 2071, training loss: 0.4091737150886112\n",
      "Iteration: 2072, training loss: 0.4091578302540195\n",
      "Iteration: 2073, training loss: 0.40914195608869247\n",
      "Iteration: 2074, training loss: 0.4091260925800908\n",
      "Iteration: 2075, training loss: 0.4091102397156963\n",
      "Iteration: 2076, training loss: 0.4090943974830119\n",
      "Iteration: 2077, training loss: 0.4090785658695618\n",
      "Iteration: 2078, training loss: 0.4090627448628917\n",
      "Iteration: 2079, training loss: 0.40904693445056794\n",
      "Iteration: 2080, training loss: 0.40903113462017815\n",
      "Iteration: 2081, training loss: 0.4090153453593312\n",
      "Iteration: 2082, training loss: 0.40899956665565673\n",
      "Iteration: 2083, training loss: 0.4089837984968054\n",
      "Iteration: 2084, training loss: 0.4089680408704486\n",
      "Iteration: 2085, training loss: 0.40895229376427883\n",
      "Iteration: 2086, training loss: 0.40893655716600935\n",
      "Iteration: 2087, training loss: 0.40892083106337407\n",
      "Iteration: 2088, training loss: 0.4089051154441276\n",
      "Iteration: 2089, training loss: 0.4088894102960454\n",
      "Iteration: 2090, training loss: 0.4088737156069235\n",
      "Iteration: 2091, training loss: 0.4088580313645783\n",
      "Iteration: 2092, training loss: 0.4088423575568472\n",
      "Iteration: 2093, training loss: 0.4088266941715875\n",
      "Iteration: 2094, training loss: 0.40881104119667766\n",
      "Iteration: 2095, training loss: 0.4087953986200158\n",
      "Iteration: 2096, training loss: 0.40877976642952085\n",
      "Iteration: 2097, training loss: 0.40876414461313215\n",
      "Iteration: 2098, training loss: 0.408748533158809\n",
      "Iteration: 2099, training loss: 0.40873293205453115\n",
      "Iteration: 2100, training loss: 0.4087173412882983\n",
      "Iteration: 2101, training loss: 0.4087017608481305\n",
      "Iteration: 2102, training loss: 0.408686190722068\n",
      "Iteration: 2103, training loss: 0.4086706308981709\n",
      "Iteration: 2104, training loss: 0.4086550813645194\n",
      "Iteration: 2105, training loss: 0.4086395421092135\n",
      "Iteration: 2106, training loss: 0.4086240131203734\n",
      "Iteration: 2107, training loss: 0.4086084943861391\n",
      "Iteration: 2108, training loss: 0.40859298589467036\n",
      "Iteration: 2109, training loss: 0.40857748763414675\n",
      "Iteration: 2110, training loss: 0.40856199959276773\n",
      "Iteration: 2111, training loss: 0.4085465217587523\n",
      "Iteration: 2112, training loss: 0.40853105412033924\n",
      "Iteration: 2113, training loss: 0.40851559666578696\n",
      "Iteration: 2114, training loss: 0.4085001493833735\n",
      "Iteration: 2115, training loss: 0.40848471226139615\n",
      "Iteration: 2116, training loss: 0.4084692852881721\n",
      "Iteration: 2117, training loss: 0.4084538684520377\n",
      "Iteration: 2118, training loss: 0.4084384617413492\n",
      "Iteration: 2119, training loss: 0.4084230651444814\n",
      "Iteration: 2120, training loss: 0.4084076786498293\n",
      "Iteration: 2121, training loss: 0.4083923022458065\n",
      "Iteration: 2122, training loss: 0.40837693592084656\n",
      "Iteration: 2123, training loss: 0.4083615796634016\n",
      "Iteration: 2124, training loss: 0.40834623346194315\n",
      "Iteration: 2125, training loss: 0.40833089730496197\n",
      "Iteration: 2126, training loss: 0.4083155711809678\n",
      "Iteration: 2127, training loss: 0.40830025507848955\n",
      "Iteration: 2128, training loss: 0.40828494898607504\n",
      "Iteration: 2129, training loss: 0.408269652892291\n",
      "Iteration: 2130, training loss: 0.4082543667857232\n",
      "Iteration: 2131, training loss: 0.4082390906549763\n",
      "Iteration: 2132, training loss: 0.40822382448867367\n",
      "Iteration: 2133, training loss: 0.4082085682754577\n",
      "Iteration: 2134, training loss: 0.40819332200398933\n",
      "Iteration: 2135, training loss: 0.4081780856629483\n",
      "Iteration: 2136, training loss: 0.40816285924103324\n",
      "Iteration: 2137, training loss: 0.40814764272696097\n",
      "Iteration: 2138, training loss: 0.4081324361094674\n",
      "Iteration: 2139, training loss: 0.4081172393773067\n",
      "Iteration: 2140, training loss: 0.4081020525192519\n",
      "Iteration: 2141, training loss: 0.40808687552409395\n",
      "Iteration: 2142, training loss: 0.4080717083806429\n",
      "Iteration: 2143, training loss: 0.4080565510777266\n",
      "Iteration: 2144, training loss: 0.40804140360419183\n",
      "Iteration: 2145, training loss: 0.4080262659489033\n",
      "Iteration: 2146, training loss: 0.4080111381007441\n",
      "Iteration: 2147, training loss: 0.40799602004861585\n",
      "Iteration: 2148, training loss: 0.40798091178143786\n",
      "Iteration: 2149, training loss: 0.4079658132881483\n",
      "Iteration: 2150, training loss: 0.4079507245577028\n",
      "Iteration: 2151, training loss: 0.4079356455790754\n",
      "Iteration: 2152, training loss: 0.40792057634125845\n",
      "Iteration: 2153, training loss: 0.40790551683326187\n",
      "Iteration: 2154, training loss: 0.40789046704411375\n",
      "Iteration: 2155, training loss: 0.4078754269628601\n",
      "Iteration: 2156, training loss: 0.4078603965785651\n",
      "Iteration: 2157, training loss: 0.4078453758803104\n",
      "Iteration: 2158, training loss: 0.4078303648571957\n",
      "Iteration: 2159, training loss: 0.4078153634983385\n",
      "Iteration: 2160, training loss: 0.40780037179287393\n",
      "Iteration: 2161, training loss: 0.40778538972995504\n",
      "Iteration: 2162, training loss: 0.40777041729875246\n",
      "Iteration: 2163, training loss: 0.4077554544884543\n",
      "Iteration: 2164, training loss: 0.40774050128826655\n",
      "Iteration: 2165, training loss: 0.4077255576874128\n",
      "Iteration: 2166, training loss: 0.4077106236751339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2167, training loss: 0.4076956992406882\n",
      "Iteration: 2168, training loss: 0.4076807843733519\n",
      "Iteration: 2169, training loss: 0.40766587906241825\n",
      "Iteration: 2170, training loss: 0.407650983297198\n",
      "Iteration: 2171, training loss: 0.4076360970670194\n",
      "Iteration: 2172, training loss: 0.40762122036122767\n",
      "Iteration: 2173, training loss: 0.4076063531691857\n",
      "Iteration: 2174, training loss: 0.4075914954802733\n",
      "Iteration: 2175, training loss: 0.40757664728388776\n",
      "Iteration: 2176, training loss: 0.40756180856944335\n",
      "Iteration: 2177, training loss: 0.40754697932637146\n",
      "Iteration: 2178, training loss: 0.40753215954412075\n",
      "Iteration: 2179, training loss: 0.4075173492121568\n",
      "Iteration: 2180, training loss: 0.40750254831996247\n",
      "Iteration: 2181, training loss: 0.40748775685703703\n",
      "Iteration: 2182, training loss: 0.4074729748128973\n",
      "Iteration: 2183, training loss: 0.4074582021770767\n",
      "Iteration: 2184, training loss: 0.4074434389391257\n",
      "Iteration: 2185, training loss: 0.4074286850886117\n",
      "Iteration: 2186, training loss: 0.40741394061511854\n",
      "Iteration: 2187, training loss: 0.4073992055082471\n",
      "Iteration: 2188, training loss: 0.407384479757615\n",
      "Iteration: 2189, training loss: 0.4073697633528566\n",
      "Iteration: 2190, training loss: 0.40735505628362273\n",
      "Iteration: 2191, training loss: 0.4073403585395813\n",
      "Iteration: 2192, training loss: 0.4073256701104162\n",
      "Iteration: 2193, training loss: 0.4073109909858284\n",
      "Iteration: 2194, training loss: 0.4072963211555352\n",
      "Iteration: 2195, training loss: 0.4072816606092704\n",
      "Iteration: 2196, training loss: 0.4072670093367844\n",
      "Iteration: 2197, training loss: 0.4072523673278438\n",
      "Iteration: 2198, training loss: 0.4072377345722317\n",
      "Iteration: 2199, training loss: 0.40722311105974773\n",
      "Iteration: 2200, training loss: 0.4072084967802076\n",
      "Iteration: 2201, training loss: 0.40719389172344356\n",
      "Iteration: 2202, training loss: 0.4071792958793039\n",
      "Iteration: 2203, training loss: 0.40716470923765324\n",
      "Iteration: 2204, training loss: 0.40715013178837234\n",
      "Iteration: 2205, training loss: 0.4071355635213583\n",
      "Iteration: 2206, training loss: 0.40712100442652416\n",
      "Iteration: 2207, training loss: 0.407106454493799\n",
      "Iteration: 2208, training loss: 0.4070919137131283\n",
      "Iteration: 2209, training loss: 0.4070773820744732\n",
      "Iteration: 2210, training loss: 0.407062859567811\n",
      "Iteration: 2211, training loss: 0.4070483461831349\n",
      "Iteration: 2212, training loss: 0.4070338419104542\n",
      "Iteration: 2213, training loss: 0.4070193467397938\n",
      "Iteration: 2214, training loss: 0.4070048606611948\n",
      "Iteration: 2215, training loss: 0.40699038366471385\n",
      "Iteration: 2216, training loss: 0.4069759157404235\n",
      "Iteration: 2217, training loss: 0.4069614568784121\n",
      "Iteration: 2218, training loss: 0.40694700706878384\n",
      "Iteration: 2219, training loss: 0.40693256630165825\n",
      "Iteration: 2220, training loss: 0.40691813456717096\n",
      "Iteration: 2221, training loss: 0.40690371185547297\n",
      "Iteration: 2222, training loss: 0.40688929815673086\n",
      "Iteration: 2223, training loss: 0.40687489346112693\n",
      "Iteration: 2224, training loss: 0.40686049775885896\n",
      "Iteration: 2225, training loss: 0.40684611104014023\n",
      "Iteration: 2226, training loss: 0.4068317332951995\n",
      "Iteration: 2227, training loss: 0.4068173645142811\n",
      "Iteration: 2228, training loss: 0.4068030046876444\n",
      "Iteration: 2229, training loss: 0.40678865380556456\n",
      "Iteration: 2230, training loss: 0.40677431185833185\n",
      "Iteration: 2231, training loss: 0.4067599788362521\n",
      "Iteration: 2232, training loss: 0.406745654729646\n",
      "Iteration: 2233, training loss: 0.40673133952884993\n",
      "Iteration: 2234, training loss: 0.40671703322421526\n",
      "Iteration: 2235, training loss: 0.40670273580610883\n",
      "Iteration: 2236, training loss: 0.4066884472649121\n",
      "Iteration: 2237, training loss: 0.40667416759102193\n",
      "Iteration: 2238, training loss: 0.4066598967748507\n",
      "Iteration: 2239, training loss: 0.40664563480682514\n",
      "Iteration: 2240, training loss: 0.40663138167738766\n",
      "Iteration: 2241, training loss: 0.40661713737699506\n",
      "Iteration: 2242, training loss: 0.4066029018961198\n",
      "Iteration: 2243, training loss: 0.4065886752252486\n",
      "Iteration: 2244, training loss: 0.4065744573548836\n",
      "Iteration: 2245, training loss: 0.40656024827554155\n",
      "Iteration: 2246, training loss: 0.4065460479777543\n",
      "Iteration: 2247, training loss: 0.40653185645206813\n",
      "Iteration: 2248, training loss: 0.40651767368904446\n",
      "Iteration: 2249, training loss: 0.4065034996792595\n",
      "Iteration: 2250, training loss: 0.4064893344133038\n",
      "Iteration: 2251, training loss: 0.4064751778817832\n",
      "Iteration: 2252, training loss: 0.4064610300753175\n",
      "Iteration: 2253, training loss: 0.4064468909845418\n",
      "Iteration: 2254, training loss: 0.40643276060010547\n",
      "Iteration: 2255, training loss: 0.4064186389126726\n",
      "Iteration: 2256, training loss: 0.40640452591292164\n",
      "Iteration: 2257, training loss: 0.4063904215915458\n",
      "Iteration: 2258, training loss: 0.40637632593925266\n",
      "Iteration: 2259, training loss: 0.40636223894676426\n",
      "Iteration: 2260, training loss: 0.4063481606048171\n",
      "Iteration: 2261, training loss: 0.40633409090416206\n",
      "Iteration: 2262, training loss: 0.4063200298355645\n",
      "Iteration: 2263, training loss: 0.406305977389804\n",
      "Iteration: 2264, training loss: 0.4062919335576746\n",
      "Iteration: 2265, training loss: 0.40627789832998445\n",
      "Iteration: 2266, training loss: 0.4062638716975561\n",
      "Iteration: 2267, training loss: 0.40624985365122634\n",
      "Iteration: 2268, training loss: 0.40623584418184605\n",
      "Iteration: 2269, training loss: 0.40622184328028027\n",
      "Iteration: 2270, training loss: 0.4062078509374085\n",
      "Iteration: 2271, training loss: 0.4061938671441239\n",
      "Iteration: 2272, training loss: 0.40617989189133413\n",
      "Iteration: 2273, training loss: 0.40616592516996064\n",
      "Iteration: 2274, training loss: 0.4061519669709388\n",
      "Iteration: 2275, training loss: 0.4061380172852186\n",
      "Iteration: 2276, training loss: 0.4061240761037634\n",
      "Iteration: 2277, training loss: 0.40611014341755053\n",
      "Iteration: 2278, training loss: 0.40609621921757166\n",
      "Iteration: 2279, training loss: 0.4060823034948319\n",
      "Iteration: 2280, training loss: 0.40606839624035057\n",
      "Iteration: 2281, training loss: 0.4060544974451606\n",
      "Iteration: 2282, training loss: 0.40604060710030887\n",
      "Iteration: 2283, training loss: 0.406026725196856\n",
      "Iteration: 2284, training loss: 0.40601285172587614\n",
      "Iteration: 2285, training loss: 0.40599898667845774\n",
      "Iteration: 2286, training loss: 0.40598513004570225\n",
      "Iteration: 2287, training loss: 0.4059712818187252\n",
      "Iteration: 2288, training loss: 0.40595744198865574\n",
      "Iteration: 2289, training loss: 0.40594361054663664\n",
      "Iteration: 2290, training loss: 0.4059297874838239\n",
      "Iteration: 2291, training loss: 0.40591597279138775\n",
      "Iteration: 2292, training loss: 0.40590216646051136\n",
      "Iteration: 2293, training loss: 0.4058883684823916\n",
      "Iteration: 2294, training loss: 0.40587457884823896\n",
      "Iteration: 2295, training loss: 0.40586079754927723\n",
      "Iteration: 2296, training loss: 0.4058470245767436\n",
      "Iteration: 2297, training loss: 0.40583325992188873\n",
      "Iteration: 2298, training loss: 0.40581950357597674\n",
      "Iteration: 2299, training loss: 0.4058057555302849\n",
      "Iteration: 2300, training loss: 0.405792015776104\n",
      "Iteration: 2301, training loss: 0.40577828430473784\n",
      "Iteration: 2302, training loss: 0.40576456110750386\n",
      "Iteration: 2303, training loss: 0.40575084617573237\n",
      "Iteration: 2304, training loss: 0.4057371395007672\n",
      "Iteration: 2305, training loss: 0.4057234410739652\n",
      "Iteration: 2306, training loss: 0.4057097508866964\n",
      "Iteration: 2307, training loss: 0.40569606893034393\n",
      "Iteration: 2308, training loss: 0.4056823951963041\n",
      "Iteration: 2309, training loss: 0.40566872967598633\n",
      "Iteration: 2310, training loss: 0.405655072360813\n",
      "Iteration: 2311, training loss: 0.40564142324221947\n",
      "Iteration: 2312, training loss: 0.4056277823116545\n",
      "Iteration: 2313, training loss: 0.40561414956057906\n",
      "Iteration: 2314, training loss: 0.4056005249804679\n",
      "Iteration: 2315, training loss: 0.4055869085628082\n",
      "Iteration: 2316, training loss: 0.4055733002991001\n",
      "Iteration: 2317, training loss: 0.40555970018085685\n",
      "Iteration: 2318, training loss: 0.4055461081996043\n",
      "Iteration: 2319, training loss: 0.40553252434688114\n",
      "Iteration: 2320, training loss: 0.40551894861423904\n",
      "Iteration: 2321, training loss: 0.4055053809932423\n",
      "Iteration: 2322, training loss: 0.4054918214754679\n",
      "Iteration: 2323, training loss: 0.4054782700525058\n",
      "Iteration: 2324, training loss: 0.40546472671595846\n",
      "Iteration: 2325, training loss: 0.405451191457441\n",
      "Iteration: 2326, training loss: 0.40543766426858113\n",
      "Iteration: 2327, training loss: 0.4054241451410196\n",
      "Iteration: 2328, training loss: 0.4054106340664092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2329, training loss: 0.4053971310364155\n",
      "Iteration: 2330, training loss: 0.405383636042717\n",
      "Iteration: 2331, training loss: 0.405370149077004\n",
      "Iteration: 2332, training loss: 0.4053566701309799\n",
      "Iteration: 2333, training loss: 0.40534319919636025\n",
      "Iteration: 2334, training loss: 0.40532973626487323\n",
      "Iteration: 2335, training loss: 0.40531628132825936\n",
      "Iteration: 2336, training loss: 0.40530283437827147\n",
      "Iteration: 2337, training loss: 0.40528939540667497\n",
      "Iteration: 2338, training loss: 0.4052759644052475\n",
      "Iteration: 2339, training loss: 0.40526254136577916\n",
      "Iteration: 2340, training loss: 0.40524912628007187\n",
      "Iteration: 2341, training loss: 0.40523571913994055\n",
      "Iteration: 2342, training loss: 0.4052223199372119\n",
      "Iteration: 2343, training loss: 0.405208928663725\n",
      "Iteration: 2344, training loss: 0.4051955453113311\n",
      "Iteration: 2345, training loss: 0.40518216987189354\n",
      "Iteration: 2346, training loss: 0.40516880233728797\n",
      "Iteration: 2347, training loss: 0.40515544269940224\n",
      "Iteration: 2348, training loss: 0.4051420909501361\n",
      "Iteration: 2349, training loss: 0.40512874708140145\n",
      "Iteration: 2350, training loss: 0.4051154110851223\n",
      "Iteration: 2351, training loss: 0.4051020829532348\n",
      "Iteration: 2352, training loss: 0.4050887626776868\n",
      "Iteration: 2353, training loss: 0.4050754502504384\n",
      "Iteration: 2354, training loss: 0.40506214566346177\n",
      "Iteration: 2355, training loss: 0.40504884890874077\n",
      "Iteration: 2356, training loss: 0.4050355599782712\n",
      "Iteration: 2357, training loss: 0.405022278864061\n",
      "Iteration: 2358, training loss: 0.4050090055581298\n",
      "Iteration: 2359, training loss: 0.4049957400525089\n",
      "Iteration: 2360, training loss: 0.40498248233924183\n",
      "Iteration: 2361, training loss: 0.40496923241038374\n",
      "Iteration: 2362, training loss: 0.40495599025800155\n",
      "Iteration: 2363, training loss: 0.40494275587417394\n",
      "Iteration: 2364, training loss: 0.40492952925099135\n",
      "Iteration: 2365, training loss: 0.40491631038055587\n",
      "Iteration: 2366, training loss: 0.40490309925498136\n",
      "Iteration: 2367, training loss: 0.4048898958663934\n",
      "Iteration: 2368, training loss: 0.404876700206929\n",
      "Iteration: 2369, training loss: 0.4048635122687371\n",
      "Iteration: 2370, training loss: 0.4048503320439781\n",
      "Iteration: 2371, training loss: 0.4048371595248238\n",
      "Iteration: 2372, training loss: 0.4048239947034577\n",
      "Iteration: 2373, training loss: 0.4048108375720751\n",
      "Iteration: 2374, training loss: 0.40479768812288236\n",
      "Iteration: 2375, training loss: 0.4047845463480975\n",
      "Iteration: 2376, training loss: 0.40477141223995017\n",
      "Iteration: 2377, training loss: 0.40475828579068135\n",
      "Iteration: 2378, training loss: 0.4047451669925433\n",
      "Iteration: 2379, training loss: 0.4047320558377999\n",
      "Iteration: 2380, training loss: 0.4047189523187263\n",
      "Iteration: 2381, training loss: 0.404705856427609\n",
      "Iteration: 2382, training loss: 0.40469276815674593\n",
      "Iteration: 2383, training loss: 0.4046796874984461\n",
      "Iteration: 2384, training loss: 0.4046666144450302\n",
      "Iteration: 2385, training loss: 0.40465354898882977\n",
      "Iteration: 2386, training loss: 0.4046404911221879\n",
      "Iteration: 2387, training loss: 0.4046274408374587\n",
      "Iteration: 2388, training loss: 0.4046143981270078\n",
      "Iteration: 2389, training loss: 0.40460136298321153\n",
      "Iteration: 2390, training loss: 0.4045883353984578\n",
      "Iteration: 2391, training loss: 0.40457531536514535\n",
      "Iteration: 2392, training loss: 0.4045623028756844\n",
      "Iteration: 2393, training loss: 0.4045492979224959\n",
      "Iteration: 2394, training loss: 0.40453630049801215\n",
      "Iteration: 2395, training loss: 0.4045233105946762\n",
      "Iteration: 2396, training loss: 0.40451032820494254\n",
      "Iteration: 2397, training loss: 0.40449735332127634\n",
      "Iteration: 2398, training loss: 0.4044843859361539\n",
      "Iteration: 2399, training loss: 0.4044714260420623\n",
      "Iteration: 2400, training loss: 0.4044584736315001\n",
      "Iteration: 2401, training loss: 0.4044455286969761\n",
      "Iteration: 2402, training loss: 0.40443259123101033\n",
      "Iteration: 2403, training loss: 0.4044196612261338\n",
      "Iteration: 2404, training loss: 0.40440673867488836\n",
      "Iteration: 2405, training loss: 0.40439382356982645\n",
      "Iteration: 2406, training loss: 0.4043809159035116\n",
      "Iteration: 2407, training loss: 0.40436801566851804\n",
      "Iteration: 2408, training loss: 0.4043551228574308\n",
      "Iteration: 2409, training loss: 0.4043422374628456\n",
      "Iteration: 2410, training loss: 0.40432935947736903\n",
      "Iteration: 2411, training loss: 0.4043164888936184\n",
      "Iteration: 2412, training loss: 0.4043036257042214\n",
      "Iteration: 2413, training loss: 0.4042907699018169\n",
      "Iteration: 2414, training loss: 0.40427792147905406\n",
      "Iteration: 2415, training loss: 0.40426508042859277\n",
      "Iteration: 2416, training loss: 0.4042522467431037\n",
      "Iteration: 2417, training loss: 0.40423942041526795\n",
      "Iteration: 2418, training loss: 0.4042266014377771\n",
      "Iteration: 2419, training loss: 0.40421378980333356\n",
      "Iteration: 2420, training loss: 0.40420098550464995\n",
      "Iteration: 2421, training loss: 0.40418818853444977\n",
      "Iteration: 2422, training loss: 0.40417539888546683\n",
      "Iteration: 2423, training loss: 0.40416261655044533\n",
      "Iteration: 2424, training loss: 0.40414984152214\n",
      "Iteration: 2425, training loss: 0.4041370737933162\n",
      "Iteration: 2426, training loss: 0.4041243133567493\n",
      "Iteration: 2427, training loss: 0.40411156020522554\n",
      "Iteration: 2428, training loss: 0.40409881433154116\n",
      "Iteration: 2429, training loss: 0.40408607572850286\n",
      "Iteration: 2430, training loss: 0.4040733443889278\n",
      "Iteration: 2431, training loss: 0.4040606203056433\n",
      "Iteration: 2432, training loss: 0.4040479034714872\n",
      "Iteration: 2433, training loss: 0.4040351938793075\n",
      "Iteration: 2434, training loss: 0.4040224915219621\n",
      "Iteration: 2435, training loss: 0.4040097963923198\n",
      "Iteration: 2436, training loss: 0.40399710848325926\n",
      "Iteration: 2437, training loss: 0.4039844277876692\n",
      "Iteration: 2438, training loss: 0.40397175429844884\n",
      "Iteration: 2439, training loss: 0.40395908800850744\n",
      "Iteration: 2440, training loss: 0.4039464289107644\n",
      "Iteration: 2441, training loss: 0.40393377699814914\n",
      "Iteration: 2442, training loss: 0.4039211322636013\n",
      "Iteration: 2443, training loss: 0.4039084947000706\n",
      "Iteration: 2444, training loss: 0.4038958643005169\n",
      "Iteration: 2445, training loss: 0.40388324105790996\n",
      "Iteration: 2446, training loss: 0.40387062496522963\n",
      "Iteration: 2447, training loss: 0.4038580160154657\n",
      "Iteration: 2448, training loss: 0.4038454142016183\n",
      "Iteration: 2449, training loss: 0.403832819516697\n",
      "Iteration: 2450, training loss: 0.40382023195372163\n",
      "Iteration: 2451, training loss: 0.40380765150572195\n",
      "Iteration: 2452, training loss: 0.4037950781657377\n",
      "Iteration: 2453, training loss: 0.40378251192681813\n",
      "Iteration: 2454, training loss: 0.403769952782023\n",
      "Iteration: 2455, training loss: 0.4037574007244214\n",
      "Iteration: 2456, training loss: 0.40374485574709257\n",
      "Iteration: 2457, training loss: 0.40373231784312524\n",
      "Iteration: 2458, training loss: 0.4037197870056184\n",
      "Iteration: 2459, training loss: 0.4037072632276805\n",
      "Iteration: 2460, training loss: 0.40369474650242987\n",
      "Iteration: 2461, training loss: 0.40368223682299464\n",
      "Iteration: 2462, training loss: 0.4036697341825124\n",
      "Iteration: 2463, training loss: 0.40365723857413083\n",
      "Iteration: 2464, training loss: 0.403644749991007\n",
      "Iteration: 2465, training loss: 0.40363226842630795\n",
      "Iteration: 2466, training loss: 0.40361979387321023\n",
      "Iteration: 2467, training loss: 0.4036073263248999\n",
      "Iteration: 2468, training loss: 0.40359486577457276\n",
      "Iteration: 2469, training loss: 0.4035824122154344\n",
      "Iteration: 2470, training loss: 0.4035699656406997\n",
      "Iteration: 2471, training loss: 0.4035575260435933\n",
      "Iteration: 2472, training loss: 0.40354509341734923\n",
      "Iteration: 2473, training loss: 0.4035326677552111\n",
      "Iteration: 2474, training loss: 0.4035202490504323\n",
      "Iteration: 2475, training loss: 0.4035078372962754\n",
      "Iteration: 2476, training loss: 0.4034954324860126\n",
      "Iteration: 2477, training loss: 0.40348303461292545\n",
      "Iteration: 2478, training loss: 0.403470643670305\n",
      "Iteration: 2479, training loss: 0.403458259651452\n",
      "Iteration: 2480, training loss: 0.40344588254967595\n",
      "Iteration: 2481, training loss: 0.4034335123582964\n",
      "Iteration: 2482, training loss: 0.403421149070642\n",
      "Iteration: 2483, training loss: 0.40340879268005086\n",
      "Iteration: 2484, training loss: 0.4033964431798702\n",
      "Iteration: 2485, training loss: 0.4033841005634569\n",
      "Iteration: 2486, training loss: 0.40337176482417675\n",
      "Iteration: 2487, training loss: 0.4033594359554052\n",
      "Iteration: 2488, training loss: 0.40334711395052675\n",
      "Iteration: 2489, training loss: 0.4033347988029354\n",
      "Iteration: 2490, training loss: 0.4033224905060339\n",
      "Iteration: 2491, training loss: 0.40331018905323485\n",
      "Iteration: 2492, training loss: 0.40329789443795955\n",
      "Iteration: 2493, training loss: 0.40328560665363866\n",
      "Iteration: 2494, training loss: 0.4032733256937121\n",
      "Iteration: 2495, training loss: 0.4032610515516289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2496, training loss: 0.40324878422084703\n",
      "Iteration: 2497, training loss: 0.4032365236948339\n",
      "Iteration: 2498, training loss: 0.40322426996706584\n",
      "Iteration: 2499, training loss: 0.40321202303102827\n",
      "Iteration: 2500, training loss: 0.4031997828802157\n",
      "Iteration: 2501, training loss: 0.40318754950813157\n",
      "Iteration: 2502, training loss: 0.40317532290828867\n",
      "Iteration: 2503, training loss: 0.40316310307420844\n",
      "Iteration: 2504, training loss: 0.4031508899994216\n",
      "Iteration: 2505, training loss: 0.4031386836774677\n",
      "Iteration: 2506, training loss: 0.40312648410189533\n",
      "Iteration: 2507, training loss: 0.4031142912662619\n",
      "Iteration: 2508, training loss: 0.4031021051641341\n",
      "Iteration: 2509, training loss: 0.4030899257890871\n",
      "Iteration: 2510, training loss: 0.4030777531347052\n",
      "Iteration: 2511, training loss: 0.40306558719458174\n",
      "Iteration: 2512, training loss: 0.40305342796231847\n",
      "Iteration: 2513, training loss: 0.40304127543152646\n",
      "Iteration: 2514, training loss: 0.40302912959582554\n",
      "Iteration: 2515, training loss: 0.403016990448844\n",
      "Iteration: 2516, training loss: 0.40300485798421926\n",
      "Iteration: 2517, training loss: 0.4029927321955975\n",
      "Iteration: 2518, training loss: 0.4029806130766337\n",
      "Iteration: 2519, training loss: 0.4029685006209915\n",
      "Iteration: 2520, training loss: 0.4029563948223432\n",
      "Iteration: 2521, training loss: 0.40294429567437007\n",
      "Iteration: 2522, training loss: 0.40293220317076184\n",
      "Iteration: 2523, training loss: 0.40292011730521715\n",
      "Iteration: 2524, training loss: 0.40290803807144315\n",
      "Iteration: 2525, training loss: 0.40289596546315576\n",
      "Iteration: 2526, training loss: 0.4028838994740795\n",
      "Iteration: 2527, training loss: 0.4028718400979474\n",
      "Iteration: 2528, training loss: 0.4028597873285015\n",
      "Iteration: 2529, training loss: 0.4028477411594918\n",
      "Iteration: 2530, training loss: 0.4028357015846775\n",
      "Iteration: 2531, training loss: 0.40282366859782615\n",
      "Iteration: 2532, training loss: 0.4028116421927136\n",
      "Iteration: 2533, training loss: 0.4027996223631246\n",
      "Iteration: 2534, training loss: 0.4027876091028523\n",
      "Iteration: 2535, training loss: 0.4027756024056982\n",
      "Iteration: 2536, training loss: 0.4027636022654724\n",
      "Iteration: 2537, training loss: 0.4027516086759937\n",
      "Iteration: 2538, training loss: 0.40273962163108895\n",
      "Iteration: 2539, training loss: 0.4027276411245937\n",
      "Iteration: 2540, training loss: 0.4027156671503519\n",
      "Iteration: 2541, training loss: 0.4027036997022158\n",
      "Iteration: 2542, training loss: 0.40269173877404624\n",
      "Iteration: 2543, training loss: 0.4026797843597122\n",
      "Iteration: 2544, training loss: 0.4026678364530913\n",
      "Iteration: 2545, training loss: 0.40265589504806915\n",
      "Iteration: 2546, training loss: 0.40264396013854015\n",
      "Iteration: 2547, training loss: 0.40263203171840667\n",
      "Iteration: 2548, training loss: 0.4026201097815795\n",
      "Iteration: 2549, training loss: 0.40260819432197775\n",
      "Iteration: 2550, training loss: 0.4025962853335288\n",
      "Iteration: 2551, training loss: 0.40258438281016823\n",
      "Iteration: 2552, training loss: 0.4025724867458399\n",
      "Iteration: 2553, training loss: 0.4025605971344959\n",
      "Iteration: 2554, training loss: 0.4025487139700965\n",
      "Iteration: 2555, training loss: 0.40253683724661043\n",
      "Iteration: 2556, training loss: 0.40252496695801415\n",
      "Iteration: 2557, training loss: 0.4025131030982926\n",
      "Iteration: 2558, training loss: 0.40250124566143886\n",
      "Iteration: 2559, training loss: 0.40248939464145417\n",
      "Iteration: 2560, training loss: 0.40247755003234764\n",
      "Iteration: 2561, training loss: 0.4024657118281369\n",
      "Iteration: 2562, training loss: 0.4024538800228474\n",
      "Iteration: 2563, training loss: 0.4024420546105125\n",
      "Iteration: 2564, training loss: 0.4024302355851742\n",
      "Iteration: 2565, training loss: 0.4024184229408821\n",
      "Iteration: 2566, training loss: 0.40240661667169403\n",
      "Iteration: 2567, training loss: 0.4023948167716755\n",
      "Iteration: 2568, training loss: 0.4023830232349007\n",
      "Iteration: 2569, training loss: 0.4023712360554513\n",
      "Iteration: 2570, training loss: 0.4023594552274169\n",
      "Iteration: 2571, training loss: 0.4023476807448954\n",
      "Iteration: 2572, training loss: 0.40233591260199253\n",
      "Iteration: 2573, training loss: 0.40232415079282197\n",
      "Iteration: 2574, training loss: 0.40231239531150514\n",
      "Iteration: 2575, training loss: 0.40230064615217165\n",
      "Iteration: 2576, training loss: 0.40228890330895895\n",
      "Iteration: 2577, training loss: 0.4022771667760121\n",
      "Iteration: 2578, training loss: 0.40226543654748437\n",
      "Iteration: 2579, training loss: 0.4022537126175367\n",
      "Iteration: 2580, training loss: 0.40224199498033797\n",
      "Iteration: 2581, training loss: 0.40223028363006486\n",
      "Iteration: 2582, training loss: 0.40221857856090176\n",
      "Iteration: 2583, training loss: 0.40220687976704095\n",
      "Iteration: 2584, training loss: 0.40219518724268266\n",
      "Iteration: 2585, training loss: 0.4021835009820345\n",
      "Iteration: 2586, training loss: 0.40217182097931214\n",
      "Iteration: 2587, training loss: 0.40216014722873894\n",
      "Iteration: 2588, training loss: 0.4021484797245458\n",
      "Iteration: 2589, training loss: 0.4021368184609718\n",
      "Iteration: 2590, training loss: 0.4021251634322632\n",
      "Iteration: 2591, training loss: 0.4021135146326742\n",
      "Iteration: 2592, training loss: 0.40210187205646675\n",
      "Iteration: 2593, training loss: 0.4020902356979102\n",
      "Iteration: 2594, training loss: 0.40207860555128183\n",
      "Iteration: 2595, training loss: 0.40206698161086635\n",
      "Iteration: 2596, training loss: 0.4020553638709562\n",
      "Iteration: 2597, training loss: 0.4020437523258514\n",
      "Iteration: 2598, training loss: 0.40203214696985956\n",
      "Iteration: 2599, training loss: 0.40202054779729585\n",
      "Iteration: 2600, training loss: 0.40200895480248316\n",
      "Iteration: 2601, training loss: 0.4019973679797515\n",
      "Iteration: 2602, training loss: 0.40198578732343904\n",
      "Iteration: 2603, training loss: 0.40197421282789103\n",
      "Iteration: 2604, training loss: 0.40196264448746033\n",
      "Iteration: 2605, training loss: 0.4019510822965074\n",
      "Iteration: 2606, training loss: 0.40193952624939994\n",
      "Iteration: 2607, training loss: 0.4019279763405135\n",
      "Iteration: 2608, training loss: 0.4019164325642309\n",
      "Iteration: 2609, training loss: 0.40190489491494236\n",
      "Iteration: 2610, training loss: 0.40189336338704545\n",
      "Iteration: 2611, training loss: 0.4018818379749454\n",
      "Iteration: 2612, training loss: 0.40187031867305484\n",
      "Iteration: 2613, training loss: 0.4018588054757935\n",
      "Iteration: 2614, training loss: 0.40184729837758865\n",
      "Iteration: 2615, training loss: 0.40183579737287506\n",
      "Iteration: 2616, training loss: 0.4018243024560949\n",
      "Iteration: 2617, training loss: 0.40181281362169724\n",
      "Iteration: 2618, training loss: 0.40180133086413894\n",
      "Iteration: 2619, training loss: 0.401789854177884\n",
      "Iteration: 2620, training loss: 0.40177838355740375\n",
      "Iteration: 2621, training loss: 0.4017669189971767\n",
      "Iteration: 2622, training loss: 0.40175546049168886\n",
      "Iteration: 2623, training loss: 0.40174400803543336\n",
      "Iteration: 2624, training loss: 0.4017325616229106\n",
      "Iteration: 2625, training loss: 0.40172112124862813\n",
      "Iteration: 2626, training loss: 0.40170968690710107\n",
      "Iteration: 2627, training loss: 0.4016982585928512\n",
      "Iteration: 2628, training loss: 0.4016868363004079\n",
      "Iteration: 2629, training loss: 0.40167542002430784\n",
      "Iteration: 2630, training loss: 0.40166400975909444\n",
      "Iteration: 2631, training loss: 0.40165260549931864\n",
      "Iteration: 2632, training loss: 0.4016412072395384\n",
      "Iteration: 2633, training loss: 0.40162981497431877\n",
      "Iteration: 2634, training loss: 0.4016184286982321\n",
      "Iteration: 2635, training loss: 0.4016070484058577\n",
      "Iteration: 2636, training loss: 0.4015956740917819\n",
      "Iteration: 2637, training loss: 0.4015843057505985\n",
      "Iteration: 2638, training loss: 0.4015729433769079\n",
      "Iteration: 2639, training loss: 0.40156158696531785\n",
      "Iteration: 2640, training loss: 0.4015502365104431\n",
      "Iteration: 2641, training loss: 0.40153889200690546\n",
      "Iteration: 2642, training loss: 0.40152755344933366\n",
      "Iteration: 2643, training loss: 0.40151622083236366\n",
      "Iteration: 2644, training loss: 0.40150489415063806\n",
      "Iteration: 2645, training loss: 0.40149357339880687\n",
      "Iteration: 2646, training loss: 0.4014822585715269\n",
      "Iteration: 2647, training loss: 0.4014709496634617\n",
      "Iteration: 2648, training loss: 0.4014596466692822\n",
      "Iteration: 2649, training loss: 0.401448349583666\n",
      "Iteration: 2650, training loss: 0.4014370584012976\n",
      "Iteration: 2651, training loss: 0.4014257731168687\n",
      "Iteration: 2652, training loss: 0.40141449372507754\n",
      "Iteration: 2653, training loss: 0.4014032202206295\n",
      "Iteration: 2654, training loss: 0.40139195259823673\n",
      "Iteration: 2655, training loss: 0.4013806908526184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2656, training loss: 0.40136943497850036\n",
      "Iteration: 2657, training loss: 0.4013581849706153\n",
      "Iteration: 2658, training loss: 0.401346940823703\n",
      "Iteration: 2659, training loss: 0.4013357025325098\n",
      "Iteration: 2660, training loss: 0.40132447009178884\n",
      "Iteration: 2661, training loss: 0.40131324349630026\n",
      "Iteration: 2662, training loss: 0.4013020227408109\n",
      "Iteration: 2663, training loss: 0.40129080782009435\n",
      "Iteration: 2664, training loss: 0.40127959872893093\n",
      "Iteration: 2665, training loss: 0.4012683954621078\n",
      "Iteration: 2666, training loss: 0.40125719801441884\n",
      "Iteration: 2667, training loss: 0.40124600638066454\n",
      "Iteration: 2668, training loss: 0.4012348205556524\n",
      "Iteration: 2669, training loss: 0.4012236405341963\n",
      "Iteration: 2670, training loss: 0.40121246631111684\n",
      "Iteration: 2671, training loss: 0.4012012978812416\n",
      "Iteration: 2672, training loss: 0.40119013523940456\n",
      "Iteration: 2673, training loss: 0.40117897838044647\n",
      "Iteration: 2674, training loss: 0.4011678272992146\n",
      "Iteration: 2675, training loss: 0.40115668199056315\n",
      "Iteration: 2676, training loss: 0.4011455424493526\n",
      "Iteration: 2677, training loss: 0.4011344086704502\n",
      "Iteration: 2678, training loss: 0.4011232806487299\n",
      "Iteration: 2679, training loss: 0.4011121583790721\n",
      "Iteration: 2680, training loss: 0.4011010418563637\n",
      "Iteration: 2681, training loss: 0.40108993107549856\n",
      "Iteration: 2682, training loss: 0.4010788260313766\n",
      "Iteration: 2683, training loss: 0.40106772671890456\n",
      "Iteration: 2684, training loss: 0.4010566331329957\n",
      "Iteration: 2685, training loss: 0.40104554526856984\n",
      "Iteration: 2686, training loss: 0.40103446312055313\n",
      "Iteration: 2687, training loss: 0.4010233866838783\n",
      "Iteration: 2688, training loss: 0.4010123159534849\n",
      "Iteration: 2689, training loss: 0.4010012509243183\n",
      "Iteration: 2690, training loss: 0.4009901915913309\n",
      "Iteration: 2691, training loss: 0.4009791379494814\n",
      "Iteration: 2692, training loss: 0.4009680899937349\n",
      "Iteration: 2693, training loss: 0.4009570477190628\n",
      "Iteration: 2694, training loss: 0.40094601112044326\n",
      "Iteration: 2695, training loss: 0.40093498019286056\n",
      "Iteration: 2696, training loss: 0.4009239549313054\n",
      "Iteration: 2697, training loss: 0.4009129353307751\n",
      "Iteration: 2698, training loss: 0.4009019213862733\n",
      "Iteration: 2699, training loss: 0.4008909130928097\n",
      "Iteration: 2700, training loss: 0.4008799104454007\n",
      "Iteration: 2701, training loss: 0.400868913439069\n",
      "Iteration: 2702, training loss: 0.4008579220688436\n",
      "Iteration: 2703, training loss: 0.4008469363297595\n",
      "Iteration: 2704, training loss: 0.4008359562168586\n",
      "Iteration: 2705, training loss: 0.4008249817251887\n",
      "Iteration: 2706, training loss: 0.4008140128498041\n",
      "Iteration: 2707, training loss: 0.4008030495857653\n",
      "Iteration: 2708, training loss: 0.40079209192813886\n",
      "Iteration: 2709, training loss: 0.40078113987199815\n",
      "Iteration: 2710, training loss: 0.4007701934124222\n",
      "Iteration: 2711, training loss: 0.40075925254449657\n",
      "Iteration: 2712, training loss: 0.4007483172633131\n",
      "Iteration: 2713, training loss: 0.40073738756396976\n",
      "Iteration: 2714, training loss: 0.40072646344157065\n",
      "Iteration: 2715, training loss: 0.4007155448912263\n",
      "Iteration: 2716, training loss: 0.40070463190805305\n",
      "Iteration: 2717, training loss: 0.40069372448717383\n",
      "Iteration: 2718, training loss: 0.4006828226237175\n",
      "Iteration: 2719, training loss: 0.4006719263128191\n",
      "Iteration: 2720, training loss: 0.4006610355496199\n",
      "Iteration: 2721, training loss: 0.40065015032926726\n",
      "Iteration: 2722, training loss: 0.4006392706469147\n",
      "Iteration: 2723, training loss: 0.4006283964977217\n",
      "Iteration: 2724, training loss: 0.400617527876854\n",
      "Iteration: 2725, training loss: 0.4006066647794834\n",
      "Iteration: 2726, training loss: 0.40059580720078786\n",
      "Iteration: 2727, training loss: 0.4005849551359513\n",
      "Iteration: 2728, training loss: 0.4005741085801636\n",
      "Iteration: 2729, training loss: 0.40056326752862115\n",
      "Iteration: 2730, training loss: 0.4005524319765256\n",
      "Iteration: 2731, training loss: 0.40054160191908555\n",
      "Iteration: 2732, training loss: 0.40053077735151504\n",
      "Iteration: 2733, training loss: 0.4005199582690341\n",
      "Iteration: 2734, training loss: 0.400509144666869\n",
      "Iteration: 2735, training loss: 0.400498336540252\n",
      "Iteration: 2736, training loss: 0.4004875338844212\n",
      "Iteration: 2737, training loss: 0.40047673669462075\n",
      "Iteration: 2738, training loss: 0.4004659449661008\n",
      "Iteration: 2739, training loss: 0.40045515869411724\n",
      "Iteration: 2740, training loss: 0.4004443778739323\n",
      "Iteration: 2741, training loss: 0.4004336025008137\n",
      "Iteration: 2742, training loss: 0.4004228325700355\n",
      "Iteration: 2743, training loss: 0.4004120680768773\n",
      "Iteration: 2744, training loss: 0.4004013090166248\n",
      "Iteration: 2745, training loss: 0.4003905553845696\n",
      "Iteration: 2746, training loss: 0.40037980717600913\n",
      "Iteration: 2747, training loss: 0.4003690643862467\n",
      "Iteration: 2748, training loss: 0.40035832701059154\n",
      "Iteration: 2749, training loss: 0.40034759504435863\n",
      "Iteration: 2750, training loss: 0.40033686848286887\n",
      "Iteration: 2751, training loss: 0.400326147321449\n",
      "Iteration: 2752, training loss: 0.4003154315554314\n",
      "Iteration: 2753, training loss: 0.40030472118015464\n",
      "Iteration: 2754, training loss: 0.40029401619096283\n",
      "Iteration: 2755, training loss: 0.4002833165832059\n",
      "Iteration: 2756, training loss: 0.4002726223522395\n",
      "Iteration: 2757, training loss: 0.4002619334934253\n",
      "Iteration: 2758, training loss: 0.4002512500021305\n",
      "Iteration: 2759, training loss: 0.40024057187372825\n",
      "Iteration: 2760, training loss: 0.4002298991035972\n",
      "Iteration: 2761, training loss: 0.40021923168712187\n",
      "Iteration: 2762, training loss: 0.4002085696196926\n",
      "Iteration: 2763, training loss: 0.40019791289670537\n",
      "Iteration: 2764, training loss: 0.4001872615135618\n",
      "Iteration: 2765, training loss: 0.4001766154656693\n",
      "Iteration: 2766, training loss: 0.40016597474844096\n",
      "Iteration: 2767, training loss: 0.4001553393572955\n",
      "Iteration: 2768, training loss: 0.40014470928765744\n",
      "Iteration: 2769, training loss: 0.4001340845349567\n",
      "Iteration: 2770, training loss: 0.4001234650946292\n",
      "Iteration: 2771, training loss: 0.4001128509621162\n",
      "Iteration: 2772, training loss: 0.40010224213286477\n",
      "Iteration: 2773, training loss: 0.40009163860232755\n",
      "Iteration: 2774, training loss: 0.4000810403659628\n",
      "Iteration: 2775, training loss: 0.4000704474192344\n",
      "Iteration: 2776, training loss: 0.40005985975761177\n",
      "Iteration: 2777, training loss: 0.4000492773765701\n",
      "Iteration: 2778, training loss: 0.4000387002715898\n",
      "Iteration: 2779, training loss: 0.40002812843815716\n",
      "Iteration: 2780, training loss: 0.400017561871764\n",
      "Iteration: 2781, training loss: 0.4000070005679075\n",
      "Iteration: 2782, training loss: 0.3999964445220906\n",
      "Iteration: 2783, training loss: 0.39998589372982174\n",
      "Iteration: 2784, training loss: 0.39997534818661473\n",
      "Iteration: 2785, training loss: 0.399964807887989\n",
      "Iteration: 2786, training loss: 0.39995427282946955\n",
      "Iteration: 2787, training loss: 0.3999437430065867\n",
      "Iteration: 2788, training loss: 0.39993321841487645\n",
      "Iteration: 2789, training loss: 0.39992269904988004\n",
      "Iteration: 2790, training loss: 0.39991218490714453\n",
      "Iteration: 2791, training loss: 0.39990167598222215\n",
      "Iteration: 2792, training loss: 0.39989117227067067\n",
      "Iteration: 2793, training loss: 0.39988067376805314\n",
      "Iteration: 2794, training loss: 0.39987018046993844\n",
      "Iteration: 2795, training loss: 0.39985969237190055\n",
      "Iteration: 2796, training loss: 0.3998492094695189\n",
      "Iteration: 2797, training loss: 0.39983873175837853\n",
      "Iteration: 2798, training loss: 0.3998282592340696\n",
      "Iteration: 2799, training loss: 0.39981779189218786\n",
      "Iteration: 2800, training loss: 0.3998073297283342\n",
      "Iteration: 2801, training loss: 0.3997968727381152\n",
      "Iteration: 2802, training loss: 0.39978642091714267\n",
      "Iteration: 2803, training loss: 0.39977597426103373\n",
      "Iteration: 2804, training loss: 0.39976553276541077\n",
      "Iteration: 2805, training loss: 0.3997550964259017\n",
      "Iteration: 2806, training loss: 0.39974466523813973\n",
      "Iteration: 2807, training loss: 0.39973423919776324\n",
      "Iteration: 2808, training loss: 0.399723818300416\n",
      "Iteration: 2809, training loss: 0.39971340254174714\n",
      "Iteration: 2810, training loss: 0.3997029919174111\n",
      "Iteration: 2811, training loss: 0.39969258642306743\n",
      "Iteration: 2812, training loss: 0.399682186054381\n",
      "Iteration: 2813, training loss: 0.3996717908070222\n",
      "Iteration: 2814, training loss: 0.39966140067666645\n",
      "Iteration: 2815, training loss: 0.39965101565899447\n",
      "Iteration: 2816, training loss: 0.39964063574969205\n",
      "Iteration: 2817, training loss: 0.3996302609444506\n",
      "Iteration: 2818, training loss: 0.39961989123896624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2819, training loss: 0.39960952662894095\n",
      "Iteration: 2820, training loss: 0.3995991671100813\n",
      "Iteration: 2821, training loss: 0.3995888126780994\n",
      "Iteration: 2822, training loss: 0.3995784633287126\n",
      "Iteration: 2823, training loss: 0.3995681190576431\n",
      "Iteration: 2824, training loss: 0.39955777986061863\n",
      "Iteration: 2825, training loss: 0.39954744573337175\n",
      "Iteration: 2826, training loss: 0.3995371166716406\n",
      "Iteration: 2827, training loss: 0.39952679267116814\n",
      "Iteration: 2828, training loss: 0.3995164737277025\n",
      "Iteration: 2829, training loss: 0.399506159836997\n",
      "Iteration: 2830, training loss: 0.3994958509948101\n",
      "Iteration: 2831, training loss: 0.39948554719690543\n",
      "Iteration: 2832, training loss: 0.3994752484390515\n",
      "Iteration: 2833, training loss: 0.39946495471702226\n",
      "Iteration: 2834, training loss: 0.3994546660265965\n",
      "Iteration: 2835, training loss: 0.39944438236355817\n",
      "Iteration: 2836, training loss: 0.39943410372369625\n",
      "Iteration: 2837, training loss: 0.3994238301028047\n",
      "Iteration: 2838, training loss: 0.3994135614966829\n",
      "Iteration: 2839, training loss: 0.3994032979011348\n",
      "Iteration: 2840, training loss: 0.3993930393119699\n",
      "Iteration: 2841, training loss: 0.39938278572500224\n",
      "Iteration: 2842, training loss: 0.3993725371360511\n",
      "Iteration: 2843, training loss: 0.39936229354094094\n",
      "Iteration: 2844, training loss: 0.39935205493550097\n",
      "Iteration: 2845, training loss: 0.39934182131556545\n",
      "Iteration: 2846, training loss: 0.3993315926769738\n",
      "Iteration: 2847, training loss: 0.3993213690155703\n",
      "Iteration: 2848, training loss: 0.3993111503272042\n",
      "Iteration: 2849, training loss: 0.3993009366077297\n",
      "Iteration: 2850, training loss: 0.399290727853006\n",
      "Iteration: 2851, training loss: 0.39928052405889725\n",
      "Iteration: 2852, training loss: 0.39927032522127254\n",
      "Iteration: 2853, training loss: 0.39926013133600596\n",
      "Iteration: 2854, training loss: 0.3992499423989765\n",
      "Iteration: 2855, training loss: 0.3992397584060679\n",
      "Iteration: 2856, training loss: 0.3992295793531691\n",
      "Iteration: 2857, training loss: 0.3992194052361737\n",
      "Iteration: 2858, training loss: 0.39920923605098035\n",
      "Iteration: 2859, training loss: 0.3991990717934925\n",
      "Iteration: 2860, training loss: 0.39918891245961863\n",
      "Iteration: 2861, training loss: 0.3991787580452718\n",
      "Iteration: 2862, training loss: 0.3991686085463704\n",
      "Iteration: 2863, training loss: 0.3991584639588371\n",
      "Iteration: 2864, training loss: 0.39914832427859986\n",
      "Iteration: 2865, training loss: 0.3991381895015914\n",
      "Iteration: 2866, training loss: 0.39912805962374914\n",
      "Iteration: 2867, training loss: 0.39911793464101547\n",
      "Iteration: 2868, training loss: 0.39910781454933747\n",
      "Iteration: 2869, training loss: 0.3990976993446671\n",
      "Iteration: 2870, training loss: 0.3990875890229612\n",
      "Iteration: 2871, training loss: 0.39907748358018136\n",
      "Iteration: 2872, training loss: 0.39906738301229383\n",
      "Iteration: 2873, training loss: 0.3990572873152697\n",
      "Iteration: 2874, training loss: 0.3990471964850851\n",
      "Iteration: 2875, training loss: 0.3990371105177204\n",
      "Iteration: 2876, training loss: 0.3990270294091614\n",
      "Iteration: 2877, training loss: 0.399016953155398\n",
      "Iteration: 2878, training loss: 0.3990068817524252\n",
      "Iteration: 2879, training loss: 0.3989968151962427\n",
      "Iteration: 2880, training loss: 0.39898675348285484\n",
      "Iteration: 2881, training loss: 0.3989766966082708\n",
      "Iteration: 2882, training loss: 0.39896664456850445\n",
      "Iteration: 2883, training loss: 0.3989565973595742\n",
      "Iteration: 2884, training loss: 0.39894655497750336\n",
      "Iteration: 2885, training loss: 0.3989365174183199\n",
      "Iteration: 2886, training loss: 0.3989264846780564\n",
      "Iteration: 2887, training loss: 0.39891645675275017\n",
      "Iteration: 2888, training loss: 0.398906433638443\n",
      "Iteration: 2889, training loss: 0.3988964153311818\n",
      "Iteration: 2890, training loss: 0.39888640182701757\n",
      "Iteration: 2891, training loss: 0.39887639312200646\n",
      "Iteration: 2892, training loss: 0.3988663892122089\n",
      "Iteration: 2893, training loss: 0.3988563900936901\n",
      "Iteration: 2894, training loss: 0.39884639576251985\n",
      "Iteration: 2895, training loss: 0.39883640621477273\n",
      "Iteration: 2896, training loss: 0.3988264214465276\n",
      "Iteration: 2897, training loss: 0.39881644145386824\n",
      "Iteration: 2898, training loss: 0.39880646623288285\n",
      "Iteration: 2899, training loss: 0.39879649577966436\n",
      "Iteration: 2900, training loss: 0.39878653009031\n",
      "Iteration: 2901, training loss: 0.3987765691609219\n",
      "Iteration: 2902, training loss: 0.3987666129876067\n",
      "Iteration: 2903, training loss: 0.39875666156647527\n",
      "Iteration: 2904, training loss: 0.3987467148936436\n",
      "Iteration: 2905, training loss: 0.39873677296523163\n",
      "Iteration: 2906, training loss: 0.3987268357773643\n",
      "Iteration: 2907, training loss: 0.39871690332617077\n",
      "Iteration: 2908, training loss: 0.39870697560778495\n",
      "Iteration: 2909, training loss: 0.3986970526183452\n",
      "Iteration: 2910, training loss: 0.3986871343539942\n",
      "Iteration: 2911, training loss: 0.3986772208108796\n",
      "Iteration: 2912, training loss: 0.39866731198515293\n",
      "Iteration: 2913, training loss: 0.3986574078729706\n",
      "Iteration: 2914, training loss: 0.3986475084704935\n",
      "Iteration: 2915, training loss: 0.39863761377388696\n",
      "Iteration: 2916, training loss: 0.3986277237793207\n",
      "Iteration: 2917, training loss: 0.3986178384829688\n",
      "Iteration: 2918, training loss: 0.39860795788101006\n",
      "Iteration: 2919, training loss: 0.3985980819696276\n",
      "Iteration: 2920, training loss: 0.39858821074500894\n",
      "Iteration: 2921, training loss: 0.3985783442033461\n",
      "Iteration: 2922, training loss: 0.39856848234083553\n",
      "Iteration: 2923, training loss: 0.398558625153678\n",
      "Iteration: 2924, training loss: 0.3985487726380788\n",
      "Iteration: 2925, training loss: 0.3985389247902476\n",
      "Iteration: 2926, training loss: 0.3985290816063984\n",
      "Iteration: 2927, training loss: 0.3985192430827498\n",
      "Iteration: 2928, training loss: 0.3985094092155244\n",
      "Iteration: 2929, training loss: 0.39849958000094965\n",
      "Iteration: 2930, training loss: 0.39848975543525694\n",
      "Iteration: 2931, training loss: 0.39847993551468236\n",
      "Iteration: 2932, training loss: 0.3984701202354662\n",
      "Iteration: 2933, training loss: 0.398460309593853\n",
      "Iteration: 2934, training loss: 0.39845050358609185\n",
      "Iteration: 2935, training loss: 0.3984407022084362\n",
      "Iteration: 2936, training loss: 0.3984309054571435\n",
      "Iteration: 2937, training loss: 0.39842111332847585\n",
      "Iteration: 2938, training loss: 0.39841132581869965\n",
      "Iteration: 2939, training loss: 0.39840154292408536\n",
      "Iteration: 2940, training loss: 0.39839176464090814\n",
      "Iteration: 2941, training loss: 0.39838199096544696\n",
      "Iteration: 2942, training loss: 0.39837222189398547\n",
      "Iteration: 2943, training loss: 0.3983624574228114\n",
      "Iteration: 2944, training loss: 0.39835269754821695\n",
      "Iteration: 2945, training loss: 0.39834294226649836\n",
      "Iteration: 2946, training loss: 0.39833319157395625\n",
      "Iteration: 2947, training loss: 0.3983234454668956\n",
      "Iteration: 2948, training loss: 0.3983137039416254\n",
      "Iteration: 2949, training loss: 0.3983039669944592\n",
      "Iteration: 2950, training loss: 0.39829423462171437\n",
      "Iteration: 2951, training loss: 0.3982845068197129\n",
      "Iteration: 2952, training loss: 0.3982747835847809\n",
      "Iteration: 2953, training loss: 0.3982650649132485\n",
      "Iteration: 2954, training loss: 0.3982553508014503\n",
      "Iteration: 2955, training loss: 0.39824564124572487\n",
      "Iteration: 2956, training loss: 0.39823593624241543\n",
      "Iteration: 2957, training loss: 0.39822623578786864\n",
      "Iteration: 2958, training loss: 0.3982165398784361\n",
      "Iteration: 2959, training loss: 0.39820684851047317\n",
      "Iteration: 2960, training loss: 0.39819716168033936\n",
      "Iteration: 2961, training loss: 0.39818747938439863\n",
      "Iteration: 2962, training loss: 0.3981778016190189\n",
      "Iteration: 2963, training loss: 0.3981681283805722\n",
      "Iteration: 2964, training loss: 0.398158459665435\n",
      "Iteration: 2965, training loss: 0.39814879546998744\n",
      "Iteration: 2966, training loss: 0.3981391357906143\n",
      "Iteration: 2967, training loss: 0.39812948062370407\n",
      "Iteration: 2968, training loss: 0.39811982996564965\n",
      "Iteration: 2969, training loss: 0.3981101838128478\n",
      "Iteration: 2970, training loss: 0.3981005421616996\n",
      "Iteration: 2971, training loss: 0.39809090500861033\n",
      "Iteration: 2972, training loss: 0.3980812723499889\n",
      "Iteration: 2973, training loss: 0.3980716441822489\n",
      "Iteration: 2974, training loss: 0.3980620205018076\n",
      "Iteration: 2975, training loss: 0.3980524013050864\n",
      "Iteration: 2976, training loss: 0.398042786588511\n",
      "Iteration: 2977, training loss: 0.3980331763485109\n",
      "Iteration: 2978, training loss: 0.39802357058151977\n",
      "Iteration: 2979, training loss: 0.3980139692839755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2980, training loss: 0.3980043724523195\n",
      "Iteration: 2981, training loss: 0.3979947800829979\n",
      "Iteration: 2982, training loss: 0.3979851921724604\n",
      "Iteration: 2983, training loss: 0.3979756087171609\n",
      "Iteration: 2984, training loss: 0.3979660297135573\n",
      "Iteration: 2985, training loss: 0.3979564551581116\n",
      "Iteration: 2986, training loss: 0.39794688504728964\n",
      "Iteration: 2987, training loss: 0.3979373193775613\n",
      "Iteration: 2988, training loss: 0.3979277581454006\n",
      "Iteration: 2989, training loss: 0.39791820134728545\n",
      "Iteration: 2990, training loss: 0.3979086489796977\n",
      "Iteration: 2991, training loss: 0.3978991010391233\n",
      "Iteration: 2992, training loss: 0.397889557522052\n",
      "Iteration: 2993, training loss: 0.3978800184249777\n",
      "Iteration: 2994, training loss: 0.3978704837443982\n",
      "Iteration: 2995, training loss: 0.3978609534768153\n",
      "Iteration: 2996, training loss: 0.39785142761873465\n",
      "Iteration: 2997, training loss: 0.39784190616666587\n",
      "Iteration: 2998, training loss: 0.39783238911712254\n",
      "Iteration: 2999, training loss: 0.3978228764666221\n",
      "Iteration: 3000, training loss: 0.3978133682116862\n",
      "Iteration: 3001, training loss: 0.39780386434883996\n",
      "Iteration: 3002, training loss: 0.39779436487461284\n",
      "Iteration: 3003, training loss: 0.3977848697855379\n",
      "Iteration: 3004, training loss: 0.39777537907815225\n",
      "Iteration: 3005, training loss: 0.397765892748997\n",
      "Iteration: 3006, training loss: 0.3977564107946168\n",
      "Iteration: 3007, training loss: 0.3977469332115605\n",
      "Iteration: 3008, training loss: 0.39773745999638077\n",
      "Iteration: 3009, training loss: 0.39772799114563423\n",
      "Iteration: 3010, training loss: 0.3977185266558809\n",
      "Iteration: 3011, training loss: 0.39770906652368543\n",
      "Iteration: 3012, training loss: 0.39769961074561566\n",
      "Iteration: 3013, training loss: 0.3976901593182436\n",
      "Iteration: 3014, training loss: 0.39768071223814494\n",
      "Iteration: 3015, training loss: 0.3976712695018995\n",
      "Iteration: 3016, training loss: 0.39766183110609066\n",
      "Iteration: 3017, training loss: 0.3976523970473057\n",
      "Iteration: 3018, training loss: 0.3976429673221357\n",
      "Iteration: 3019, training loss: 0.3976335419271756\n",
      "Iteration: 3020, training loss: 0.3976241208590241\n",
      "Iteration: 3021, training loss: 0.3976147041142837\n",
      "Iteration: 3022, training loss: 0.39760529168956094\n",
      "Iteration: 3023, training loss: 0.39759588358146564\n",
      "Iteration: 3024, training loss: 0.39758647978661205\n",
      "Iteration: 3025, training loss: 0.3975770803016176\n",
      "Iteration: 3026, training loss: 0.39756768512310386\n",
      "Iteration: 3027, training loss: 0.39755829424769606\n",
      "Iteration: 3028, training loss: 0.3975489076720232\n",
      "Iteration: 3029, training loss: 0.39753952539271814\n",
      "Iteration: 3030, training loss: 0.3975301474064172\n",
      "Iteration: 3031, training loss: 0.39752077370976074\n",
      "Iteration: 3032, training loss: 0.39751140429939286\n",
      "Iteration: 3033, training loss: 0.3975020391719611\n",
      "Iteration: 3034, training loss: 0.39749267832411705\n",
      "Iteration: 3035, training loss: 0.39748332175251605\n",
      "Iteration: 3036, training loss: 0.39747396945381674\n",
      "Iteration: 3037, training loss: 0.39746462142468186\n",
      "Iteration: 3038, training loss: 0.3974552776617779\n",
      "Iteration: 3039, training loss: 0.39744593816177465\n",
      "Iteration: 3040, training loss: 0.397436602921346\n",
      "Iteration: 3041, training loss: 0.39742727193716937\n",
      "Iteration: 3042, training loss: 0.3974179452059258\n",
      "Iteration: 3043, training loss: 0.3974086227243002\n",
      "Iteration: 3044, training loss: 0.3973993044889809\n",
      "Iteration: 3045, training loss: 0.3973899904966602\n",
      "Iteration: 3046, training loss: 0.3973806807440338\n",
      "Iteration: 3047, training loss: 0.3973713752278012\n",
      "Iteration: 3048, training loss: 0.39736207394466555\n",
      "Iteration: 3049, training loss: 0.3973527768913336\n",
      "Iteration: 3050, training loss: 0.3973434840645157\n",
      "Iteration: 3051, training loss: 0.3973341954609259\n",
      "Iteration: 3052, training loss: 0.397324911077282\n",
      "Iteration: 3053, training loss: 0.3973156309103052\n",
      "Iteration: 3054, training loss: 0.3973063549567205\n",
      "Iteration: 3055, training loss: 0.39729708321325635\n",
      "Iteration: 3056, training loss: 0.39728781567664506\n",
      "Iteration: 3057, training loss: 0.3972785523436222\n",
      "Iteration: 3058, training loss: 0.3972692932109273\n",
      "Iteration: 3059, training loss: 0.3972600382753031\n",
      "Iteration: 3060, training loss: 0.3972507875334964\n",
      "Iteration: 3061, training loss: 0.39724154098225717\n",
      "Iteration: 3062, training loss: 0.3972322986183391\n",
      "Iteration: 3063, training loss: 0.3972230604384995\n",
      "Iteration: 3064, training loss: 0.39721382643949926\n",
      "Iteration: 3065, training loss: 0.3972045966181028\n",
      "Iteration: 3066, training loss: 0.3971953709710781\n",
      "Iteration: 3067, training loss: 0.39718614949519654\n",
      "Iteration: 3068, training loss: 0.3971769321872332\n",
      "Iteration: 3069, training loss: 0.39716771904396697\n",
      "Iteration: 3070, training loss: 0.39715851006217967\n",
      "Iteration: 3071, training loss: 0.3971493052386571\n",
      "Iteration: 3072, training loss: 0.3971401045701885\n",
      "Iteration: 3073, training loss: 0.3971309080535665\n",
      "Iteration: 3074, training loss: 0.39712171568558735\n",
      "Iteration: 3075, training loss: 0.3971125274630509\n",
      "Iteration: 3076, training loss: 0.3971033433827603\n",
      "Iteration: 3077, training loss: 0.39709416344152243\n",
      "Iteration: 3078, training loss: 0.39708498763614747\n",
      "Iteration: 3079, training loss: 0.39707581596344904\n",
      "Iteration: 3080, training loss: 0.3970666484202446\n",
      "Iteration: 3081, training loss: 0.39705748500335475\n",
      "Iteration: 3082, training loss: 0.39704832570960347\n",
      "Iteration: 3083, training loss: 0.39703917053581866\n",
      "Iteration: 3084, training loss: 0.39703001947883154\n",
      "Iteration: 3085, training loss: 0.39702087253547635\n",
      "Iteration: 3086, training loss: 0.3970117297025913\n",
      "Iteration: 3087, training loss: 0.397002590977018\n",
      "Iteration: 3088, training loss: 0.39699345635560107\n",
      "Iteration: 3089, training loss: 0.3969843258351891\n",
      "Iteration: 3090, training loss: 0.39697519941263376\n",
      "Iteration: 3091, training loss: 0.3969660770847904\n",
      "Iteration: 3092, training loss: 0.39695695884851756\n",
      "Iteration: 3093, training loss: 0.39694784470067734\n",
      "Iteration: 3094, training loss: 0.3969387346381353\n",
      "Iteration: 3095, training loss: 0.39692962865776027\n",
      "Iteration: 3096, training loss: 0.3969205267564245\n",
      "Iteration: 3097, training loss: 0.39691142893100373\n",
      "Iteration: 3098, training loss: 0.39690233517837714\n",
      "Iteration: 3099, training loss: 0.39689324549542704\n",
      "Iteration: 3100, training loss: 0.39688415987903936\n",
      "Iteration: 3101, training loss: 0.3968750783261034\n",
      "Iteration: 3102, training loss: 0.39686600083351176\n",
      "Iteration: 3103, training loss: 0.3968569273981604\n",
      "Iteration: 3104, training loss: 0.3968478580169487\n",
      "Iteration: 3105, training loss: 0.3968387926867793\n",
      "Iteration: 3106, training loss: 0.3968297314045585\n",
      "Iteration: 3107, training loss: 0.3968206741671955\n",
      "Iteration: 3108, training loss: 0.39681162097160305\n",
      "Iteration: 3109, training loss: 0.39680257181469736\n",
      "Iteration: 3110, training loss: 0.3967935266933978\n",
      "Iteration: 3111, training loss: 0.3967844856046273\n",
      "Iteration: 3112, training loss: 0.3967754485453117\n",
      "Iteration: 3113, training loss: 0.3967664155123808\n",
      "Iteration: 3114, training loss: 0.39675738650276676\n",
      "Iteration: 3115, training loss: 0.3967483615134062\n",
      "Iteration: 3116, training loss: 0.3967393405412382\n",
      "Iteration: 3117, training loss: 0.39673032358320554\n",
      "Iteration: 3118, training loss: 0.396721310636254\n",
      "Iteration: 3119, training loss: 0.3967123016973329\n",
      "Iteration: 3120, training loss: 0.3967032967633949\n",
      "Iteration: 3121, training loss: 0.3966942958313957\n",
      "Iteration: 3122, training loss: 0.39668529889829435\n",
      "Iteration: 3123, training loss: 0.3966763059610533\n",
      "Iteration: 3124, training loss: 0.3966673170166382\n",
      "Iteration: 3125, training loss: 0.39665833206201784\n",
      "Iteration: 3126, training loss: 0.39664935109416455\n",
      "Iteration: 3127, training loss: 0.39664037411005376\n",
      "Iteration: 3128, training loss: 0.39663140110666395\n",
      "Iteration: 3129, training loss: 0.39662243208097714\n",
      "Iteration: 3130, training loss: 0.39661346702997846\n",
      "Iteration: 3131, training loss: 0.3966045059506563\n",
      "Iteration: 3132, training loss: 0.3965955488400023\n",
      "Iteration: 3133, training loss: 0.39658659569501137\n",
      "Iteration: 3134, training loss: 0.3965776465126815\n",
      "Iteration: 3135, training loss: 0.3965687012900139\n",
      "Iteration: 3136, training loss: 0.3965597600240133\n",
      "Iteration: 3137, training loss: 0.39655082271168723\n",
      "Iteration: 3138, training loss: 0.3965418893500466\n",
      "Iteration: 3139, training loss: 0.39653295993610554\n",
      "Iteration: 3140, training loss: 0.39652403446688145\n",
      "Iteration: 3141, training loss: 0.39651511293939473\n",
      "Iteration: 3142, training loss: 0.39650619535066917\n",
      "Iteration: 3143, training loss: 0.3964972816977314\n",
      "Iteration: 3144, training loss: 0.3964883719776117\n",
      "Iteration: 3145, training loss: 0.3964794661873432\n",
      "Iteration: 3146, training loss: 0.39647056432396227\n",
      "Iteration: 3147, training loss: 0.3964616663845085\n",
      "Iteration: 3148, training loss: 0.3964527723660245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3149, training loss: 0.39644388226555616\n",
      "Iteration: 3150, training loss: 0.3964349960801525\n",
      "Iteration: 3151, training loss: 0.39642611380686565\n",
      "Iteration: 3152, training loss: 0.39641723544275076\n",
      "Iteration: 3153, training loss: 0.3964083609848666\n",
      "Iteration: 3154, training loss: 0.3963994904302745\n",
      "Iteration: 3155, training loss: 0.396390623776039\n",
      "Iteration: 3156, training loss: 0.3963817610192281\n",
      "Iteration: 3157, training loss: 0.39637290215691273\n",
      "Iteration: 3158, training loss: 0.3963640471861669\n",
      "Iteration: 3159, training loss: 0.39635519610406766\n",
      "Iteration: 3160, training loss: 0.3963463489076953\n",
      "Iteration: 3161, training loss: 0.3963375055941333\n",
      "Iteration: 3162, training loss: 0.39632866616046797\n",
      "Iteration: 3163, training loss: 0.3963198306037887\n",
      "Iteration: 3164, training loss: 0.3963109989211885\n",
      "Iteration: 3165, training loss: 0.39630217110976274\n",
      "Iteration: 3166, training loss: 0.3962933471666103\n",
      "Iteration: 3167, training loss: 0.39628452708883305\n",
      "Iteration: 3168, training loss: 0.3962757108735359\n",
      "Iteration: 3169, training loss: 0.39626689851782687\n",
      "Iteration: 3170, training loss: 0.39625809001881696\n",
      "Iteration: 3171, training loss: 0.39624928537362036\n",
      "Iteration: 3172, training loss: 0.39624048457935424\n",
      "Iteration: 3173, training loss: 0.3962316876331387\n",
      "Iteration: 3174, training loss: 0.39622289453209697\n",
      "Iteration: 3175, training loss: 0.3962141052733555\n",
      "Iteration: 3176, training loss: 0.3962053198540435\n",
      "Iteration: 3177, training loss: 0.39619653827129336\n",
      "Iteration: 3178, training loss: 0.3961877605222405\n",
      "Iteration: 3179, training loss: 0.3961789866040232\n",
      "Iteration: 3180, training loss: 0.396170216513783\n",
      "Iteration: 3181, training loss: 0.3961614502486644\n",
      "Iteration: 3182, training loss: 0.39615268780581453\n",
      "Iteration: 3183, training loss: 0.3961439291823842\n",
      "Iteration: 3184, training loss: 0.3961351743755267\n",
      "Iteration: 3185, training loss: 0.3961264233823985\n",
      "Iteration: 3186, training loss: 0.3961176762001589\n",
      "Iteration: 3187, training loss: 0.3961089328259706\n",
      "Iteration: 3188, training loss: 0.39610019325699886\n",
      "Iteration: 3189, training loss: 0.39609145749041214\n",
      "Iteration: 3190, training loss: 0.3960827255233816\n",
      "Iteration: 3191, training loss: 0.3960739973530819\n",
      "Iteration: 3192, training loss: 0.3960652729766901\n",
      "Iteration: 3193, training loss: 0.39605655239138654\n",
      "Iteration: 3194, training loss: 0.3960478355943544\n",
      "Iteration: 3195, training loss: 0.39603912258278007\n",
      "Iteration: 3196, training loss: 0.3960304133538524\n",
      "Iteration: 3197, training loss: 0.3960217079047636\n",
      "Iteration: 3198, training loss: 0.3960130062327088\n",
      "Iteration: 3199, training loss: 0.3960043083348858\n",
      "Iteration: 3200, training loss: 0.3959956142084955\n",
      "Iteration: 3201, training loss: 0.39598692385074186\n",
      "Iteration: 3202, training loss: 0.3959782372588314\n",
      "Iteration: 3203, training loss: 0.395969554429974\n",
      "Iteration: 3204, training loss: 0.3959608753613823\n",
      "Iteration: 3205, training loss: 0.39595220005027154\n",
      "Iteration: 3206, training loss: 0.3959435284938604\n",
      "Iteration: 3207, training loss: 0.39593486068937\n",
      "Iteration: 3208, training loss: 0.3959261966340247\n",
      "Iteration: 3209, training loss: 0.39591753632505144\n",
      "Iteration: 3210, training loss: 0.39590887975968037\n",
      "Iteration: 3211, training loss: 0.39590022693514443\n",
      "Iteration: 3212, training loss: 0.39589157784867934\n",
      "Iteration: 3213, training loss: 0.39588293249752377\n",
      "Iteration: 3214, training loss: 0.3958742908789192\n",
      "Iteration: 3215, training loss: 0.3958656529901101\n",
      "Iteration: 3216, training loss: 0.39585701882834384\n",
      "Iteration: 3217, training loss: 0.3958483883908705\n",
      "Iteration: 3218, training loss: 0.3958397616749431\n",
      "Iteration: 3219, training loss: 0.3958311386778175\n",
      "Iteration: 3220, training loss: 0.3958225193967523\n",
      "Iteration: 3221, training loss: 0.39581390382900933\n",
      "Iteration: 3222, training loss: 0.3958052919718527\n",
      "Iteration: 3223, training loss: 0.39579668382255\n",
      "Iteration: 3224, training loss: 0.3957880793783711\n",
      "Iteration: 3225, training loss: 0.3957794786365891\n",
      "Iteration: 3226, training loss: 0.3957708815944796\n",
      "Iteration: 3227, training loss: 0.3957622882493212\n",
      "Iteration: 3228, training loss: 0.3957536985983954\n",
      "Iteration: 3229, training loss: 0.39574511263898626\n",
      "Iteration: 3230, training loss: 0.39573653036838113\n",
      "Iteration: 3231, training loss: 0.3957279517838696\n",
      "Iteration: 3232, training loss: 0.39571937688274433\n",
      "Iteration: 3233, training loss: 0.39571080566230094\n",
      "Iteration: 3234, training loss: 0.39570223811983746\n",
      "Iteration: 3235, training loss: 0.3956936742526551\n",
      "Iteration: 3236, training loss: 0.39568511405805773\n",
      "Iteration: 3237, training loss: 0.39567655753335185\n",
      "Iteration: 3238, training loss: 0.395668004675847\n",
      "Iteration: 3239, training loss: 0.3956594554828553\n",
      "Iteration: 3240, training loss: 0.3956509099516917\n",
      "Iteration: 3241, training loss: 0.39564236807967407\n",
      "Iteration: 3242, training loss: 0.3956338298641226\n",
      "Iteration: 3243, training loss: 0.39562529530236096\n",
      "Iteration: 3244, training loss: 0.3956167643917149\n",
      "Iteration: 3245, training loss: 0.3956082371295134\n",
      "Iteration: 3246, training loss: 0.3955997135130878\n",
      "Iteration: 3247, training loss: 0.3955911935397724\n",
      "Iteration: 3248, training loss: 0.39558267720690443\n",
      "Iteration: 3249, training loss: 0.39557416451182353\n",
      "Iteration: 3250, training loss: 0.3955656554518722\n",
      "Iteration: 3251, training loss: 0.39555715002439573\n",
      "Iteration: 3252, training loss: 0.395548648226742\n",
      "Iteration: 3253, training loss: 0.39554015005626186\n",
      "Iteration: 3254, training loss: 0.39553165551030867\n",
      "Iteration: 3255, training loss: 0.39552316458623854\n",
      "Iteration: 3256, training loss: 0.39551467728141027\n",
      "Iteration: 3257, training loss: 0.3955061935931855\n",
      "Iteration: 3258, training loss: 0.3954977135189285\n",
      "Iteration: 3259, training loss: 0.3954892370560063\n",
      "Iteration: 3260, training loss: 0.3954807642017885\n",
      "Iteration: 3261, training loss: 0.39547229495364755\n",
      "Iteration: 3262, training loss: 0.39546382930895846\n",
      "Iteration: 3263, training loss: 0.39545536726509894\n",
      "Iteration: 3264, training loss: 0.39544690881944944\n",
      "Iteration: 3265, training loss: 0.3954384539693932\n",
      "Iteration: 3266, training loss: 0.39543000271231604\n",
      "Iteration: 3267, training loss: 0.39542155504560617\n",
      "Iteration: 3268, training loss: 0.395413110966655\n",
      "Iteration: 3269, training loss: 0.3954046704728562\n",
      "Iteration: 3270, training loss: 0.39539623356160636\n",
      "Iteration: 3271, training loss: 0.3953878002303045\n",
      "Iteration: 3272, training loss: 0.3953793704763525\n",
      "Iteration: 3273, training loss: 0.39537094429715475\n",
      "Iteration: 3274, training loss: 0.3953625216901185\n",
      "Iteration: 3275, training loss: 0.3953541026526531\n",
      "Iteration: 3276, training loss: 0.3953456871821714\n",
      "Iteration: 3277, training loss: 0.3953372752760881\n",
      "Iteration: 3278, training loss: 0.395328866931821\n",
      "Iteration: 3279, training loss: 0.3953204621467903\n",
      "Iteration: 3280, training loss: 0.3953120609184189\n",
      "Iteration: 3281, training loss: 0.3953036632441326\n",
      "Iteration: 3282, training loss: 0.39529526912135915\n",
      "Iteration: 3283, training loss: 0.39528687854752964\n",
      "Iteration: 3284, training loss: 0.3952784915200772\n",
      "Iteration: 3285, training loss: 0.3952701080364381\n",
      "Iteration: 3286, training loss: 0.3952617280940507\n",
      "Iteration: 3287, training loss: 0.3952533516903564\n",
      "Iteration: 3288, training loss: 0.39524497882279885\n",
      "Iteration: 3289, training loss: 0.3952366094888245\n",
      "Iteration: 3290, training loss: 0.3952282436858823\n",
      "Iteration: 3291, training loss: 0.39521988141142395\n",
      "Iteration: 3292, training loss: 0.39521152266290344\n",
      "Iteration: 3293, training loss: 0.39520316743777767\n",
      "Iteration: 3294, training loss: 0.39519481573350596\n",
      "Iteration: 3295, training loss: 0.39518646754755\n",
      "Iteration: 3296, training loss: 0.39517812287737447\n",
      "Iteration: 3297, training loss: 0.39516978172044637\n",
      "Iteration: 3298, training loss: 0.3951614440742353\n",
      "Iteration: 3299, training loss: 0.3951531099362134\n",
      "Iteration: 3300, training loss: 0.3951447793038554\n",
      "Iteration: 3301, training loss: 0.3951364521746385\n",
      "Iteration: 3302, training loss: 0.39512812854604246\n",
      "Iteration: 3303, training loss: 0.3951198084155499\n",
      "Iteration: 3304, training loss: 0.3951114917806455\n",
      "Iteration: 3305, training loss: 0.39510317863881694\n",
      "Iteration: 3306, training loss: 0.39509486898755397\n",
      "Iteration: 3307, training loss: 0.3950865628243493\n",
      "Iteration: 3308, training loss: 0.3950782601466977\n",
      "Iteration: 3309, training loss: 0.3950699609520972\n",
      "Iteration: 3310, training loss: 0.3950616652380475\n",
      "Iteration: 3311, training loss: 0.39505337300205134\n",
      "Iteration: 3312, training loss: 0.39504508424161394\n",
      "Iteration: 3313, training loss: 0.39503679895424293\n",
      "Iteration: 3314, training loss: 0.3950285171374483\n",
      "Iteration: 3315, training loss: 0.3950202387887429\n",
      "Iteration: 3316, training loss: 0.3950119639056418\n",
      "Iteration: 3317, training loss: 0.3950036924856628\n",
      "Iteration: 3318, training loss: 0.3949954245263258\n",
      "Iteration: 3319, training loss: 0.3949871600251537\n",
      "Iteration: 3320, training loss: 0.3949788989796716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3321, training loss: 0.3949706413874071\n",
      "Iteration: 3322, training loss: 0.3949623872458902\n",
      "Iteration: 3323, training loss: 0.3949541365526537\n",
      "Iteration: 3324, training loss: 0.3949458893052325\n",
      "Iteration: 3325, training loss: 0.3949376455011642\n",
      "Iteration: 3326, training loss: 0.3949294051379889\n",
      "Iteration: 3327, training loss: 0.39492116821324896\n",
      "Iteration: 3328, training loss: 0.3949129347244893\n",
      "Iteration: 3329, training loss: 0.3949047046692574\n",
      "Iteration: 3330, training loss: 0.3948964780451031\n",
      "Iteration: 3331, training loss: 0.3948882548495787\n",
      "Iteration: 3332, training loss: 0.39488003508023883\n",
      "Iteration: 3333, training loss: 0.394871818734641\n",
      "Iteration: 3334, training loss: 0.39486360581034446\n",
      "Iteration: 3335, training loss: 0.3948553963049117\n",
      "Iteration: 3336, training loss: 0.3948471902159069\n",
      "Iteration: 3337, training loss: 0.39483898754089725\n",
      "Iteration: 3338, training loss: 0.394830788277452\n",
      "Iteration: 3339, training loss: 0.39482259242314305\n",
      "Iteration: 3340, training loss: 0.39481439997554457\n",
      "Iteration: 3341, training loss: 0.3948062109322333\n",
      "Iteration: 3342, training loss: 0.39479802529078817\n",
      "Iteration: 3343, training loss: 0.3947898430487909\n",
      "Iteration: 3344, training loss: 0.39478166420382527\n",
      "Iteration: 3345, training loss: 0.3947734887534774\n",
      "Iteration: 3346, training loss: 0.3947653166953363\n",
      "Iteration: 3347, training loss: 0.394757148026993\n",
      "Iteration: 3348, training loss: 0.39474898274604087\n",
      "Iteration: 3349, training loss: 0.39474082085007595\n",
      "Iteration: 3350, training loss: 0.3947326623366965\n",
      "Iteration: 3351, training loss: 0.3947245072035033\n",
      "Iteration: 3352, training loss: 0.3947163554480993\n",
      "Iteration: 3353, training loss: 0.39470820706809\n",
      "Iteration: 3354, training loss: 0.3947000620610832\n",
      "Iteration: 3355, training loss: 0.3946919204246891\n",
      "Iteration: 3356, training loss: 0.3946837821565204\n",
      "Iteration: 3357, training loss: 0.3946756472541919\n",
      "Iteration: 3358, training loss: 0.3946675157153211\n",
      "Iteration: 3359, training loss: 0.3946593875375276\n",
      "Iteration: 3360, training loss: 0.3946512627184333\n",
      "Iteration: 3361, training loss: 0.3946431412556628\n",
      "Iteration: 3362, training loss: 0.39463502314684273\n",
      "Iteration: 3363, training loss: 0.3946269083896023\n",
      "Iteration: 3364, training loss: 0.3946187969815729\n",
      "Iteration: 3365, training loss: 0.39461068892038825\n",
      "Iteration: 3366, training loss: 0.39460258420368466\n",
      "Iteration: 3367, training loss: 0.39459448282910053\n",
      "Iteration: 3368, training loss: 0.3945863847942766\n",
      "Iteration: 3369, training loss: 0.3945782900968562\n",
      "Iteration: 3370, training loss: 0.39457019873448457\n",
      "Iteration: 3371, training loss: 0.3945621107048097\n",
      "Iteration: 3372, training loss: 0.3945540260054816\n",
      "Iteration: 3373, training loss: 0.3945459446341529\n",
      "Iteration: 3374, training loss: 0.39453786658847806\n",
      "Iteration: 3375, training loss: 0.39452979186611453\n",
      "Iteration: 3376, training loss: 0.39452172046472134\n",
      "Iteration: 3377, training loss: 0.39451365238196046\n",
      "Iteration: 3378, training loss: 0.3945055876154957\n",
      "Iteration: 3379, training loss: 0.3944975261629934\n",
      "Iteration: 3380, training loss: 0.3944894680221223\n",
      "Iteration: 3381, training loss: 0.3944814131905532\n",
      "Iteration: 3382, training loss: 0.39447336166595937\n",
      "Iteration: 3383, training loss: 0.39446531344601604\n",
      "Iteration: 3384, training loss: 0.3944572685284013\n",
      "Iteration: 3385, training loss: 0.394449226910795\n",
      "Iteration: 3386, training loss: 0.39444118859087957\n",
      "Iteration: 3387, training loss: 0.3944331535663397\n",
      "Iteration: 3388, training loss: 0.39442512183486206\n",
      "Iteration: 3389, training loss: 0.39441709339413594\n",
      "Iteration: 3390, training loss: 0.3944090682418528\n",
      "Iteration: 3391, training loss: 0.3944010463757062\n",
      "Iteration: 3392, training loss: 0.3943930277933923\n",
      "Iteration: 3393, training loss: 0.3943850124926091\n",
      "Iteration: 3394, training loss: 0.3943770004710574\n",
      "Iteration: 3395, training loss: 0.3943689917264396\n",
      "Iteration: 3396, training loss: 0.3943609862564607\n",
      "Iteration: 3397, training loss: 0.39435298405882807\n",
      "Iteration: 3398, training loss: 0.39434498513125127\n",
      "Iteration: 3399, training loss: 0.3943369894714418\n",
      "Iteration: 3400, training loss: 0.3943289970771138\n",
      "Iteration: 3401, training loss: 0.39432100794598335\n",
      "Iteration: 3402, training loss: 0.394313022075769\n",
      "Iteration: 3403, training loss: 0.39430503946419126\n",
      "Iteration: 3404, training loss: 0.3942970601089731\n",
      "Iteration: 3405, training loss: 0.39428908400783963\n",
      "Iteration: 3406, training loss: 0.3942811111585182\n",
      "Iteration: 3407, training loss: 0.39427314155873827\n",
      "Iteration: 3408, training loss: 0.39426517520623183\n",
      "Iteration: 3409, training loss: 0.39425721209873266\n",
      "Iteration: 3410, training loss: 0.394249252233977\n",
      "Iteration: 3411, training loss: 0.39424129560970345\n",
      "Iteration: 3412, training loss: 0.39423334222365236\n",
      "Iteration: 3413, training loss: 0.3942253920735666\n",
      "Iteration: 3414, training loss: 0.3942174451571913\n",
      "Iteration: 3415, training loss: 0.3942095014722736\n",
      "Iteration: 3416, training loss: 0.394201561016563\n",
      "Iteration: 3417, training loss: 0.39419362378781087\n",
      "Iteration: 3418, training loss: 0.3941856897837712\n",
      "Iteration: 3419, training loss: 0.3941777590021999\n",
      "Iteration: 3420, training loss: 0.39416983144085516\n",
      "Iteration: 3421, training loss: 0.3941619070974971\n",
      "Iteration: 3422, training loss: 0.3941539859698886\n",
      "Iteration: 3423, training loss: 0.39414606805579394\n",
      "Iteration: 3424, training loss: 0.39413815335298025\n",
      "Iteration: 3425, training loss: 0.3941302418592165\n",
      "Iteration: 3426, training loss: 0.39412233357227383\n",
      "Iteration: 3427, training loss: 0.3941144284899256\n",
      "Iteration: 3428, training loss: 0.3941065266099474\n",
      "Iteration: 3429, training loss: 0.3940986279301167\n",
      "Iteration: 3430, training loss: 0.3940907324482136\n",
      "Iteration: 3431, training loss: 0.3940828401620198\n",
      "Iteration: 3432, training loss: 0.39407495106931956\n",
      "Iteration: 3433, training loss: 0.3940670651678992\n",
      "Iteration: 3434, training loss: 0.39405918245554705\n",
      "Iteration: 3435, training loss: 0.3940513029300537\n",
      "Iteration: 3436, training loss: 0.39404342658921176\n",
      "Iteration: 3437, training loss: 0.39403555343081625\n",
      "Iteration: 3438, training loss: 0.3940276834526639\n",
      "Iteration: 3439, training loss: 0.394019816652554\n",
      "Iteration: 3440, training loss: 0.3940119530282876\n",
      "Iteration: 3441, training loss: 0.3940040925776681\n",
      "Iteration: 3442, training loss: 0.3939962352985011\n",
      "Iteration: 3443, training loss: 0.393988381188594\n",
      "Iteration: 3444, training loss: 0.3939805302457565\n",
      "Iteration: 3445, training loss: 0.39397268246780054\n",
      "Iteration: 3446, training loss: 0.39396483785254\n",
      "Iteration: 3447, training loss: 0.3939569963977908\n",
      "Iteration: 3448, training loss: 0.3939491581013712\n",
      "Iteration: 3449, training loss: 0.3939413229611014\n",
      "Iteration: 3450, training loss: 0.39393349097480373\n",
      "Iteration: 3451, training loss: 0.3939256621403026\n",
      "Iteration: 3452, training loss: 0.3939178364554246\n",
      "Iteration: 3453, training loss: 0.3939100139179983\n",
      "Iteration: 3454, training loss: 0.3939021945258545\n",
      "Iteration: 3455, training loss: 0.3938943782768259\n",
      "Iteration: 3456, training loss: 0.3938865651687475\n",
      "Iteration: 3457, training loss: 0.3938787551994561\n",
      "Iteration: 3458, training loss: 0.3938709483667908\n",
      "Iteration: 3459, training loss: 0.39386314466859274\n",
      "Iteration: 3460, training loss: 0.3938553441027051\n",
      "Iteration: 3461, training loss: 0.39384754666697336\n",
      "Iteration: 3462, training loss: 0.3938397523592444\n",
      "Iteration: 3463, training loss: 0.393831961177368\n",
      "Iteration: 3464, training loss: 0.39382417311919543\n",
      "Iteration: 3465, training loss: 0.39381638818258036\n",
      "Iteration: 3466, training loss: 0.3938086063653782\n",
      "Iteration: 3467, training loss: 0.3938008276654466\n",
      "Iteration: 3468, training loss: 0.39379305208064536\n",
      "Iteration: 3469, training loss: 0.3937852796088361\n",
      "Iteration: 3470, training loss: 0.3937775102478828\n",
      "Iteration: 3471, training loss: 0.393769743995651\n",
      "Iteration: 3472, training loss: 0.3937619808500088\n",
      "Iteration: 3473, training loss: 0.39375422080882594\n",
      "Iteration: 3474, training loss: 0.3937464638699745\n",
      "Iteration: 3475, training loss: 0.39373871003132843\n",
      "Iteration: 3476, training loss: 0.3937309592907637\n",
      "Iteration: 3477, training loss: 0.3937232116461583\n",
      "Iteration: 3478, training loss: 0.3937154670953925\n",
      "Iteration: 3479, training loss: 0.3937077256363481\n",
      "Iteration: 3480, training loss: 0.39369998726690936\n",
      "Iteration: 3481, training loss: 0.39369225198496244\n",
      "Iteration: 3482, training loss: 0.3936845197883955\n",
      "Iteration: 3483, training loss: 0.39367679067509875\n",
      "Iteration: 3484, training loss: 0.3936690646429641\n",
      "Iteration: 3485, training loss: 0.393661341689886\n",
      "Iteration: 3486, training loss: 0.39365362181376057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3487, training loss: 0.39364590501248586\n",
      "Iteration: 3488, training loss: 0.3936381912839624\n",
      "Iteration: 3489, training loss: 0.39363048062609196\n",
      "Iteration: 3490, training loss: 0.393622773036779\n",
      "Iteration: 3491, training loss: 0.39361506851392963\n",
      "Iteration: 3492, training loss: 0.39360736705545196\n",
      "Iteration: 3493, training loss: 0.39359966865925633\n",
      "Iteration: 3494, training loss: 0.3935919733232547\n",
      "Iteration: 3495, training loss: 0.39358428104536125\n",
      "Iteration: 3496, training loss: 0.39357659182349214\n",
      "Iteration: 3497, training loss: 0.3935689056555654\n",
      "Iteration: 3498, training loss: 0.3935612225395012\n",
      "Iteration: 3499, training loss: 0.3935535424732214\n",
      "Iteration: 3500, training loss: 0.3935458654546502\n",
      "Iteration: 3501, training loss: 0.3935381914817133\n",
      "Iteration: 3502, training loss: 0.3935305205523391\n",
      "Iteration: 3503, training loss: 0.3935228526644572\n",
      "Iteration: 3504, training loss: 0.39351518781599937\n",
      "Iteration: 3505, training loss: 0.3935075260048998\n",
      "Iteration: 3506, training loss: 0.3934998672290939\n",
      "Iteration: 3507, training loss: 0.3934922114865197\n",
      "Iteration: 3508, training loss: 0.3934845587751166\n",
      "Iteration: 3509, training loss: 0.3934769090928266\n",
      "Iteration: 3510, training loss: 0.39346926243759306\n",
      "Iteration: 3511, training loss: 0.3934616188073614\n",
      "Iteration: 3512, training loss: 0.39345397820007944\n",
      "Iteration: 3513, training loss: 0.3934463406136963\n",
      "Iteration: 3514, training loss: 0.39343870604616343\n",
      "Iteration: 3515, training loss: 0.39343107449543413\n",
      "Iteration: 3516, training loss: 0.39342344595946366\n",
      "Iteration: 3517, training loss: 0.39341582043620904\n",
      "Iteration: 3518, training loss: 0.39340819792362935\n",
      "Iteration: 3519, training loss: 0.3934005784196858\n",
      "Iteration: 3520, training loss: 0.3933929619223411\n",
      "Iteration: 3521, training loss: 0.39338534842956\n",
      "Iteration: 3522, training loss: 0.3933777379393096\n",
      "Iteration: 3523, training loss: 0.39337013044955826\n",
      "Iteration: 3524, training loss: 0.3933625259582767\n",
      "Iteration: 3525, training loss: 0.3933549244634375\n",
      "Iteration: 3526, training loss: 0.39334732596301486\n",
      "Iteration: 3527, training loss: 0.3933397304549852\n",
      "Iteration: 3528, training loss: 0.39333213793732685\n",
      "Iteration: 3529, training loss: 0.3933245484080197\n",
      "Iteration: 3530, training loss: 0.393316961865046\n",
      "Iteration: 3531, training loss: 0.3933093783063895\n",
      "Iteration: 3532, training loss: 0.393301797730036\n",
      "Iteration: 3533, training loss: 0.3932942201339733\n",
      "Iteration: 3534, training loss: 0.39328664551619086\n",
      "Iteration: 3535, training loss: 0.3932790738746803\n",
      "Iteration: 3536, training loss: 0.3932715052074348\n",
      "Iteration: 3537, training loss: 0.39326393951244987\n",
      "Iteration: 3538, training loss: 0.3932563767877223\n",
      "Iteration: 3539, training loss: 0.3932488170312513\n",
      "Iteration: 3540, training loss: 0.39324126024103767\n",
      "Iteration: 3541, training loss: 0.39323370641508426\n",
      "Iteration: 3542, training loss: 0.39322615555139556\n",
      "Iteration: 3543, training loss: 0.3932186076479781\n",
      "Iteration: 3544, training loss: 0.3932110627028402\n",
      "Iteration: 3545, training loss: 0.39320352071399217\n",
      "Iteration: 3546, training loss: 0.39319598167944597\n",
      "Iteration: 3547, training loss: 0.3931884455972156\n",
      "Iteration: 3548, training loss: 0.3931809124653169\n",
      "Iteration: 3549, training loss: 0.39317338228176735\n",
      "Iteration: 3550, training loss: 0.3931658550445866\n",
      "Iteration: 3551, training loss: 0.393158330751796\n",
      "Iteration: 3552, training loss: 0.3931508094014187\n",
      "Iteration: 3553, training loss: 0.39314329099147977\n",
      "Iteration: 3554, training loss: 0.39313577552000606\n",
      "Iteration: 3555, training loss: 0.39312826298502634\n",
      "Iteration: 3556, training loss: 0.39312075338457103\n",
      "Iteration: 3557, training loss: 0.39311324671667275\n",
      "Iteration: 3558, training loss: 0.39310574297936557\n",
      "Iteration: 3559, training loss: 0.39309824217068573\n",
      "Iteration: 3560, training loss: 0.3930907442886709\n",
      "Iteration: 3561, training loss: 0.39308324933136113\n",
      "Iteration: 3562, training loss: 0.3930757572967976\n",
      "Iteration: 3563, training loss: 0.39306826818302393\n",
      "Iteration: 3564, training loss: 0.39306078198808514\n",
      "Iteration: 3565, training loss: 0.3930532987100284\n",
      "Iteration: 3566, training loss: 0.39304581834690244\n",
      "Iteration: 3567, training loss: 0.39303834089675793\n",
      "Iteration: 3568, training loss: 0.39303086635764745\n",
      "Iteration: 3569, training loss: 0.39302339472762504\n",
      "Iteration: 3570, training loss: 0.3930159260047469\n",
      "Iteration: 3571, training loss: 0.3930084601870709\n",
      "Iteration: 3572, training loss: 0.3930009972726567\n",
      "Iteration: 3573, training loss: 0.3929935372595658\n",
      "Iteration: 3574, training loss: 0.3929860801458615\n",
      "Iteration: 3575, training loss: 0.3929786259296088\n",
      "Iteration: 3576, training loss: 0.39297117460887454\n",
      "Iteration: 3577, training loss: 0.3929637261817276\n",
      "Iteration: 3578, training loss: 0.3929562806462381\n",
      "Iteration: 3579, training loss: 0.39294883800047853\n",
      "Iteration: 3580, training loss: 0.3929413982425229\n",
      "Iteration: 3581, training loss: 0.3929339613704469\n",
      "Iteration: 3582, training loss: 0.39292652738232825\n",
      "Iteration: 3583, training loss: 0.3929190962762461\n",
      "Iteration: 3584, training loss: 0.3929116680502819\n",
      "Iteration: 3585, training loss: 0.3929042427025183\n",
      "Iteration: 3586, training loss: 0.39289682023104017\n",
      "Iteration: 3587, training loss: 0.3928894006339338\n",
      "Iteration: 3588, training loss: 0.3928819839092875\n",
      "Iteration: 3589, training loss: 0.3928745700551914\n",
      "Iteration: 3590, training loss: 0.3928671590697371\n",
      "Iteration: 3591, training loss: 0.3928597509510181\n",
      "Iteration: 3592, training loss: 0.3928523456971298\n",
      "Iteration: 3593, training loss: 0.39284494330616915\n",
      "Iteration: 3594, training loss: 0.392837543776235\n",
      "Iteration: 3595, training loss: 0.39283014710542774\n",
      "Iteration: 3596, training loss: 0.39282275329184985\n",
      "Iteration: 3597, training loss: 0.39281536233360526\n",
      "Iteration: 3598, training loss: 0.3928079742287999\n",
      "Iteration: 3599, training loss: 0.39280058897554115\n",
      "Iteration: 3600, training loss: 0.3927932065719384\n",
      "Iteration: 3601, training loss: 0.39278582701610265\n",
      "Iteration: 3602, training loss: 0.3927784503061465\n",
      "Iteration: 3603, training loss: 0.39277107644018466\n",
      "Iteration: 3604, training loss: 0.39276370541633326\n",
      "Iteration: 3605, training loss: 0.39275633723271025\n",
      "Iteration: 3606, training loss: 0.3927489718874354\n",
      "Iteration: 3607, training loss: 0.39274160937863006\n",
      "Iteration: 3608, training loss: 0.39273424970441734\n",
      "Iteration: 3609, training loss: 0.3927268928629221\n",
      "Iteration: 3610, training loss: 0.392719538852271\n",
      "Iteration: 3611, training loss: 0.39271218767059235\n",
      "Iteration: 3612, training loss: 0.3927048393160161\n",
      "Iteration: 3613, training loss: 0.392697493786674\n",
      "Iteration: 3614, training loss: 0.39269015108069966\n",
      "Iteration: 3615, training loss: 0.392682811196228\n",
      "Iteration: 3616, training loss: 0.3926754741313961\n",
      "Iteration: 3617, training loss: 0.3926681398843424\n",
      "Iteration: 3618, training loss: 0.39266080845320717\n",
      "Iteration: 3619, training loss: 0.3926534798361326\n",
      "Iteration: 3620, training loss: 0.39264615403126213\n",
      "Iteration: 3621, training loss: 0.39263883103674135\n",
      "Iteration: 3622, training loss: 0.3926315108507172\n",
      "Iteration: 3623, training loss: 0.3926241934713385\n",
      "Iteration: 3624, training loss: 0.3926168788967557\n",
      "Iteration: 3625, training loss: 0.3926095671251211\n",
      "Iteration: 3626, training loss: 0.39260225815458843\n",
      "Iteration: 3627, training loss: 0.39259495198331323\n",
      "Iteration: 3628, training loss: 0.3925876486094527\n",
      "Iteration: 3629, training loss: 0.3925803480311658\n",
      "Iteration: 3630, training loss: 0.39257305024661304\n",
      "Iteration: 3631, training loss: 0.3925657552539569\n",
      "Iteration: 3632, training loss: 0.3925584630513611\n",
      "Iteration: 3633, training loss: 0.3925511736369912\n",
      "Iteration: 3634, training loss: 0.3925438870090147\n",
      "Iteration: 3635, training loss: 0.39253660316560046\n",
      "Iteration: 3636, training loss: 0.3925293221049192\n",
      "Iteration: 3637, training loss: 0.392522043825143\n",
      "Iteration: 3638, training loss: 0.392514768324446\n",
      "Iteration: 3639, training loss: 0.39250749560100373\n",
      "Iteration: 3640, training loss: 0.39250022565299353\n",
      "Iteration: 3641, training loss: 0.3924929584785943\n",
      "Iteration: 3642, training loss: 0.3924856940759866\n",
      "Iteration: 3643, training loss: 0.3924784324433528\n",
      "Iteration: 3644, training loss: 0.3924711735788767\n",
      "Iteration: 3645, training loss: 0.39246391748074383\n",
      "Iteration: 3646, training loss: 0.39245666414714153\n",
      "Iteration: 3647, training loss: 0.3924494135762585\n",
      "Iteration: 3648, training loss: 0.39244216576628527\n",
      "Iteration: 3649, training loss: 0.39243492071541414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3650, training loss: 0.3924276784218387\n",
      "Iteration: 3651, training loss: 0.3924204388837544\n",
      "Iteration: 3652, training loss: 0.3924132020993582\n",
      "Iteration: 3653, training loss: 0.39240596806684913\n",
      "Iteration: 3654, training loss: 0.39239873678442716\n",
      "Iteration: 3655, training loss: 0.39239150825029434\n",
      "Iteration: 3656, training loss: 0.3923842824626543\n",
      "Iteration: 3657, training loss: 0.3923770594197123\n",
      "Iteration: 3658, training loss: 0.39236983911967493\n",
      "Iteration: 3659, training loss: 0.39236262156075097\n",
      "Iteration: 3660, training loss: 0.3923554067411502\n",
      "Iteration: 3661, training loss: 0.39234819465908455\n",
      "Iteration: 3662, training loss: 0.3923409853127673\n",
      "Iteration: 3663, training loss: 0.3923337787004132\n",
      "Iteration: 3664, training loss: 0.3923265748202391\n",
      "Iteration: 3665, training loss: 0.3923193736704628\n",
      "Iteration: 3666, training loss: 0.39231217524930434\n",
      "Iteration: 3667, training loss: 0.392304979554985\n",
      "Iteration: 3668, training loss: 0.3922977865857278\n",
      "Iteration: 3669, training loss: 0.39229059633975716\n",
      "Iteration: 3670, training loss: 0.39228340881529955\n",
      "Iteration: 3671, training loss: 0.39227622401058254\n",
      "Iteration: 3672, training loss: 0.3922690419238355\n",
      "Iteration: 3673, training loss: 0.39226186255328965\n",
      "Iteration: 3674, training loss: 0.3922546858971773\n",
      "Iteration: 3675, training loss: 0.3922475119537328\n",
      "Iteration: 3676, training loss: 0.3922403407211918\n",
      "Iteration: 3677, training loss: 0.3922331721977917\n",
      "Iteration: 3678, training loss: 0.39222600638177146\n",
      "Iteration: 3679, training loss: 0.3922188432713716\n",
      "Iteration: 3680, training loss: 0.39221168286483415\n",
      "Iteration: 3681, training loss: 0.39220452516040294\n",
      "Iteration: 3682, training loss: 0.392197370156323\n",
      "Iteration: 3683, training loss: 0.39219021785084146\n",
      "Iteration: 3684, training loss: 0.3921830682422066\n",
      "Iteration: 3685, training loss: 0.39217592132866835\n",
      "Iteration: 3686, training loss: 0.3921687771084784\n",
      "Iteration: 3687, training loss: 0.39216163557988976\n",
      "Iteration: 3688, training loss: 0.3921544967411574\n",
      "Iteration: 3689, training loss: 0.3921473605905373\n",
      "Iteration: 3690, training loss: 0.39214022712628743\n",
      "Iteration: 3691, training loss: 0.39213309634666715\n",
      "Iteration: 3692, training loss: 0.3921259682499375\n",
      "Iteration: 3693, training loss: 0.392118842834361\n",
      "Iteration: 3694, training loss: 0.3921117200982015\n",
      "Iteration: 3695, training loss: 0.392104600039725\n",
      "Iteration: 3696, training loss: 0.39209748265719846\n",
      "Iteration: 3697, training loss: 0.3920903679488907\n",
      "Iteration: 3698, training loss: 0.39208325591307197\n",
      "Iteration: 3699, training loss: 0.39207614654801415\n",
      "Iteration: 3700, training loss: 0.39206903985199054\n",
      "Iteration: 3701, training loss: 0.3920619358232763\n",
      "Iteration: 3702, training loss: 0.39205483446014766\n",
      "Iteration: 3703, training loss: 0.39204773576088275\n",
      "Iteration: 3704, training loss: 0.392040639723761\n",
      "Iteration: 3705, training loss: 0.39203354634706367\n",
      "Iteration: 3706, training loss: 0.3920264556290734\n",
      "Iteration: 3707, training loss: 0.3920193675680742\n",
      "Iteration: 3708, training loss: 0.3920122821623519\n",
      "Iteration: 3709, training loss: 0.39200519941019357\n",
      "Iteration: 3710, training loss: 0.3919981193098882\n",
      "Iteration: 3711, training loss: 0.3919910418597259\n",
      "Iteration: 3712, training loss: 0.3919839670579985\n",
      "Iteration: 3713, training loss: 0.3919768949029994\n",
      "Iteration: 3714, training loss: 0.39196982539302333\n",
      "Iteration: 3715, training loss: 0.39196275852636686\n",
      "Iteration: 3716, training loss: 0.3919556943013276\n",
      "Iteration: 3717, training loss: 0.3919486327162052\n",
      "Iteration: 3718, training loss: 0.3919415737693005\n",
      "Iteration: 3719, training loss: 0.39193451745891594\n",
      "Iteration: 3720, training loss: 0.3919274637833555\n",
      "Iteration: 3721, training loss: 0.3919204127409245\n",
      "Iteration: 3722, training loss: 0.39191336432993\n",
      "Iteration: 3723, training loss: 0.3919063185486805\n",
      "Iteration: 3724, training loss: 0.3918992753954859\n",
      "Iteration: 3725, training loss: 0.39189223486865765\n",
      "Iteration: 3726, training loss: 0.39188519696650886\n",
      "Iteration: 3727, training loss: 0.3918781616873539\n",
      "Iteration: 3728, training loss: 0.3918711290295088\n",
      "Iteration: 3729, training loss: 0.39186409899129093\n",
      "Iteration: 3730, training loss: 0.39185707157101934\n",
      "Iteration: 3731, training loss: 0.39185004676701435\n",
      "Iteration: 3732, training loss: 0.3918430245775981\n",
      "Iteration: 3733, training loss: 0.3918360050010939\n",
      "Iteration: 3734, training loss: 0.39182898803582655\n",
      "Iteration: 3735, training loss: 0.3918219736801227\n",
      "Iteration: 3736, training loss: 0.3918149619323101\n",
      "Iteration: 3737, training loss: 0.391807952790718\n",
      "Iteration: 3738, training loss: 0.39180094625367745\n",
      "Iteration: 3739, training loss: 0.39179394231952075\n",
      "Iteration: 3740, training loss: 0.39178694098658157\n",
      "Iteration: 3741, training loss: 0.39177994225319523\n",
      "Iteration: 3742, training loss: 0.3917729461176985\n",
      "Iteration: 3743, training loss: 0.3917659525784295\n",
      "Iteration: 3744, training loss: 0.39175896163372814\n",
      "Iteration: 3745, training loss: 0.3917519732819354\n",
      "Iteration: 3746, training loss: 0.3917449875213939\n",
      "Iteration: 3747, training loss: 0.3917380043504478\n",
      "Iteration: 3748, training loss: 0.3917310237674426\n",
      "Iteration: 3749, training loss: 0.3917240457707253\n",
      "Iteration: 3750, training loss: 0.39171707035864445\n",
      "Iteration: 3751, training loss: 0.3917100975295498\n",
      "Iteration: 3752, training loss: 0.39170312728179296\n",
      "Iteration: 3753, training loss: 0.39169615961372656\n",
      "Iteration: 3754, training loss: 0.39168919452370504\n",
      "Iteration: 3755, training loss: 0.391682232010084\n",
      "Iteration: 3756, training loss: 0.39167527207122077\n",
      "Iteration: 3757, training loss: 0.39166831470547386\n",
      "Iteration: 3758, training loss: 0.3916613599112035\n",
      "Iteration: 3759, training loss: 0.391654407686771\n",
      "Iteration: 3760, training loss: 0.39164745803053946\n",
      "Iteration: 3761, training loss: 0.3916405109408734\n",
      "Iteration: 3762, training loss: 0.3916335664161385\n",
      "Iteration: 3763, training loss: 0.3916266244547021\n",
      "Iteration: 3764, training loss: 0.39161968505493283\n",
      "Iteration: 3765, training loss: 0.3916127482152011\n",
      "Iteration: 3766, training loss: 0.3916058139338782\n",
      "Iteration: 3767, training loss: 0.39159888220933736\n",
      "Iteration: 3768, training loss: 0.3915919530399529\n",
      "Iteration: 3769, training loss: 0.39158502642410076\n",
      "Iteration: 3770, training loss: 0.39157810236015833\n",
      "Iteration: 3771, training loss: 0.39157118084650416\n",
      "Iteration: 3772, training loss: 0.3915642618815185\n",
      "Iteration: 3773, training loss: 0.39155734546358295\n",
      "Iteration: 3774, training loss: 0.39155043159108044\n",
      "Iteration: 3775, training loss: 0.3915435202623954\n",
      "Iteration: 3776, training loss: 0.3915366114759137\n",
      "Iteration: 3777, training loss: 0.3915297052300225\n",
      "Iteration: 3778, training loss: 0.39152280152311053\n",
      "Iteration: 3779, training loss: 0.3915159003535678\n",
      "Iteration: 3780, training loss: 0.3915090017197858\n",
      "Iteration: 3781, training loss: 0.39150210562015747\n",
      "Iteration: 3782, training loss: 0.39149521205307697\n",
      "Iteration: 3783, training loss: 0.3914883210169402\n",
      "Iteration: 3784, training loss: 0.39148143251014406\n",
      "Iteration: 3785, training loss: 0.39147454653108715\n",
      "Iteration: 3786, training loss: 0.3914676630781693\n",
      "Iteration: 3787, training loss: 0.39146078214979196\n",
      "Iteration: 3788, training loss: 0.3914539037443578\n",
      "Iteration: 3789, training loss: 0.3914470278602707\n",
      "Iteration: 3790, training loss: 0.39144015449593633\n",
      "Iteration: 3791, training loss: 0.39143328364976154\n",
      "Iteration: 3792, training loss: 0.39142641532015465\n",
      "Iteration: 3793, training loss: 0.3914195495055252\n",
      "Iteration: 3794, training loss: 0.3914126862042844\n",
      "Iteration: 3795, training loss: 0.39140582541484453\n",
      "Iteration: 3796, training loss: 0.3913989671356194\n",
      "Iteration: 3797, training loss: 0.3913921113650244\n",
      "Iteration: 3798, training loss: 0.39138525810147584\n",
      "Iteration: 3799, training loss: 0.39137840734339197\n",
      "Iteration: 3800, training loss: 0.39137155908919186\n",
      "Iteration: 3801, training loss: 0.39136471333729644\n",
      "Iteration: 3802, training loss: 0.3913578700861277\n",
      "Iteration: 3803, training loss: 0.39135102933410915\n",
      "Iteration: 3804, training loss: 0.39134419107966556\n",
      "Iteration: 3805, training loss: 0.3913373553212232\n",
      "Iteration: 3806, training loss: 0.39133052205720964\n",
      "Iteration: 3807, training loss: 0.39132369128605377\n",
      "Iteration: 3808, training loss: 0.3913168630061859\n",
      "Iteration: 3809, training loss: 0.39131003721603785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3810, training loss: 0.3913032139140425\n",
      "Iteration: 3811, training loss: 0.3912963930986343\n",
      "Iteration: 3812, training loss: 0.391289574768249\n",
      "Iteration: 3813, training loss: 0.3912827589213237\n",
      "Iteration: 3814, training loss: 0.39127594555629686\n",
      "Iteration: 3815, training loss: 0.39126913467160834\n",
      "Iteration: 3816, training loss: 0.39126232626569946\n",
      "Iteration: 3817, training loss: 0.3912555203370125\n",
      "Iteration: 3818, training loss: 0.3912487168839914\n",
      "Iteration: 3819, training loss: 0.39124191590508156\n",
      "Iteration: 3820, training loss: 0.39123511739872935\n",
      "Iteration: 3821, training loss: 0.3912283213633828\n",
      "Iteration: 3822, training loss: 0.3912215277974912\n",
      "Iteration: 3823, training loss: 0.39121473669950513\n",
      "Iteration: 3824, training loss: 0.39120794806787657\n",
      "Iteration: 3825, training loss: 0.39120116190105864\n",
      "Iteration: 3826, training loss: 0.39119437819750624\n",
      "Iteration: 3827, training loss: 0.3911875969556752\n",
      "Iteration: 3828, training loss: 0.3911808181740227\n",
      "Iteration: 3829, training loss: 0.3911740418510077\n",
      "Iteration: 3830, training loss: 0.39116726798508994\n",
      "Iteration: 3831, training loss: 0.3911604965747307\n",
      "Iteration: 3832, training loss: 0.3911537276183927\n",
      "Iteration: 3833, training loss: 0.39114696111453984\n",
      "Iteration: 3834, training loss: 0.39114019706163744\n",
      "Iteration: 3835, training loss: 0.39113343545815216\n",
      "Iteration: 3836, training loss: 0.3911266763025518\n",
      "Iteration: 3837, training loss: 0.3911199195933057\n",
      "Iteration: 3838, training loss: 0.3911131653288844\n",
      "Iteration: 3839, training loss: 0.3911064135077599\n",
      "Iteration: 3840, training loss: 0.3910996641284053\n",
      "Iteration: 3841, training loss: 0.391092917189295\n",
      "Iteration: 3842, training loss: 0.3910861726889051\n",
      "Iteration: 3843, training loss: 0.3910794306257126\n",
      "Iteration: 3844, training loss: 0.39107269099819597\n",
      "Iteration: 3845, training loss: 0.39106595380483516\n",
      "Iteration: 3846, training loss: 0.39105921904411095\n",
      "Iteration: 3847, training loss: 0.39105248671450604\n",
      "Iteration: 3848, training loss: 0.3910457568145039\n",
      "Iteration: 3849, training loss: 0.39103902934258966\n",
      "Iteration: 3850, training loss: 0.3910323042972495\n",
      "Iteration: 3851, training loss: 0.39102558167697105\n",
      "Iteration: 3852, training loss: 0.3910188614802434\n",
      "Iteration: 3853, training loss: 0.3910121437055566\n",
      "Iteration: 3854, training loss: 0.3910054283514022\n",
      "Iteration: 3855, training loss: 0.3909987154162728\n",
      "Iteration: 3856, training loss: 0.39099200489866276\n",
      "Iteration: 3857, training loss: 0.39098529679706734\n",
      "Iteration: 3858, training loss: 0.3909785911099831\n",
      "Iteration: 3859, training loss: 0.39097188783590825\n",
      "Iteration: 3860, training loss: 0.3909651869733419\n",
      "Iteration: 3861, training loss: 0.3909584885207846\n",
      "Iteration: 3862, training loss: 0.39095179247673817\n",
      "Iteration: 3863, training loss: 0.3909450988397058\n",
      "Iteration: 3864, training loss: 0.3909384076081917\n",
      "Iteration: 3865, training loss: 0.3909317187807017\n",
      "Iteration: 3866, training loss: 0.3909250323557427\n",
      "Iteration: 3867, training loss: 0.39091834833182293\n",
      "Iteration: 3868, training loss: 0.39091166670745187\n",
      "Iteration: 3869, training loss: 0.3909049874811404\n",
      "Iteration: 3870, training loss: 0.3908983106514004\n",
      "Iteration: 3871, training loss: 0.3908916362167453\n",
      "Iteration: 3872, training loss: 0.3908849641756898\n",
      "Iteration: 3873, training loss: 0.3908782945267495\n",
      "Iteration: 3874, training loss: 0.3908716272684419\n",
      "Iteration: 3875, training loss: 0.3908649623992851\n",
      "Iteration: 3876, training loss: 0.3908582999177988\n",
      "Iteration: 3877, training loss: 0.3908516398225041\n",
      "Iteration: 3878, training loss: 0.39084498211192314\n",
      "Iteration: 3879, training loss: 0.39083832678457936\n",
      "Iteration: 3880, training loss: 0.39083167383899736\n",
      "Iteration: 3881, training loss: 0.3908250232737033\n",
      "Iteration: 3882, training loss: 0.3908183750872243\n",
      "Iteration: 3883, training loss: 0.39081172927808894\n",
      "Iteration: 3884, training loss: 0.39080508584482687\n",
      "Iteration: 3885, training loss: 0.39079844478596915\n",
      "Iteration: 3886, training loss: 0.390791806100048\n",
      "Iteration: 3887, training loss: 0.3907851697855969\n",
      "Iteration: 3888, training loss: 0.39077853584115074\n",
      "Iteration: 3889, training loss: 0.3907719042652452\n",
      "Iteration: 3890, training loss: 0.39076527505641795\n",
      "Iteration: 3891, training loss: 0.3907586482132071\n",
      "Iteration: 3892, training loss: 0.3907520237341526\n",
      "Iteration: 3893, training loss: 0.3907454016177954\n",
      "Iteration: 3894, training loss: 0.3907387818626776\n",
      "Iteration: 3895, training loss: 0.39073216446734277\n",
      "Iteration: 3896, training loss: 0.39072554943033544\n",
      "Iteration: 3897, training loss: 0.3907189367502017\n",
      "Iteration: 3898, training loss: 0.3907123264254887\n",
      "Iteration: 3899, training loss: 0.3907057184547448\n",
      "Iteration: 3900, training loss: 0.39069911283651954\n",
      "Iteration: 3901, training loss: 0.39069250956936397\n",
      "Iteration: 3902, training loss: 0.39068590865183\n",
      "Iteration: 3903, training loss: 0.3906793100824711\n",
      "Iteration: 3904, training loss: 0.39067271385984165\n",
      "Iteration: 3905, training loss: 0.39066611998249756\n",
      "Iteration: 3906, training loss: 0.39065952844899576\n",
      "Iteration: 3907, training loss: 0.39065293925789446\n",
      "Iteration: 3908, training loss: 0.3906463524077532\n",
      "Iteration: 3909, training loss: 0.39063976789713245\n",
      "Iteration: 3910, training loss: 0.3906331857245942\n",
      "Iteration: 3911, training loss: 0.39062660588870163\n",
      "Iteration: 3912, training loss: 0.3906200283880189\n",
      "Iteration: 3913, training loss: 0.3906134532211116\n",
      "Iteration: 3914, training loss: 0.3906068803865465\n",
      "Iteration: 3915, training loss: 0.39060030988289157\n",
      "Iteration: 3916, training loss: 0.3905937417087159\n",
      "Iteration: 3917, training loss: 0.3905871758625899\n",
      "Iteration: 3918, training loss: 0.3905806123430852\n",
      "Iteration: 3919, training loss: 0.3905740511487745\n",
      "Iteration: 3920, training loss: 0.39056749227823184\n",
      "Iteration: 3921, training loss: 0.3905609357300325\n",
      "Iteration: 3922, training loss: 0.3905543815027527\n",
      "Iteration: 3923, training loss: 0.39054782959497025\n",
      "Iteration: 3924, training loss: 0.3905412800052638\n",
      "Iteration: 3925, training loss: 0.39053473273221345\n",
      "Iteration: 3926, training loss: 0.3905281877744004\n",
      "Iteration: 3927, training loss: 0.390521645130407\n",
      "Iteration: 3928, training loss: 0.3905151047988168\n",
      "Iteration: 3929, training loss: 0.3905085667782147\n",
      "Iteration: 3930, training loss: 0.39050203106718656\n",
      "Iteration: 3931, training loss: 0.39049549766431974\n",
      "Iteration: 3932, training loss: 0.3904889665682025\n",
      "Iteration: 3933, training loss: 0.39048243777742425\n",
      "Iteration: 3934, training loss: 0.39047591129057596\n",
      "Iteration: 3935, training loss: 0.39046938710624945\n",
      "Iteration: 3936, training loss: 0.39046286522303775\n",
      "Iteration: 3937, training loss: 0.3904563456395354\n",
      "Iteration: 3938, training loss: 0.39044982835433756\n",
      "Iteration: 3939, training loss: 0.39044331336604116\n",
      "Iteration: 3940, training loss: 0.3904368006732439\n",
      "Iteration: 3941, training loss: 0.39043029027454473\n",
      "Iteration: 3942, training loss: 0.390423782168544\n",
      "Iteration: 3943, training loss: 0.3904172763538431\n",
      "Iteration: 3944, training loss: 0.3904107728290443\n",
      "Iteration: 3945, training loss: 0.39040427159275165\n",
      "Iteration: 3946, training loss: 0.3903977726435698\n",
      "Iteration: 3947, training loss: 0.39039127598010487\n",
      "Iteration: 3948, training loss: 0.3903847816009642\n",
      "Iteration: 3949, training loss: 0.39037828950475606\n",
      "Iteration: 3950, training loss: 0.3903717996900901\n",
      "Iteration: 3951, training loss: 0.39036531215557696\n",
      "Iteration: 3952, training loss: 0.3903588268998286\n",
      "Iteration: 3953, training loss: 0.39035234392145796\n",
      "Iteration: 3954, training loss: 0.39034586321907955\n",
      "Iteration: 3955, training loss: 0.3903393847913084\n",
      "Iteration: 3956, training loss: 0.39033290863676134\n",
      "Iteration: 3957, training loss: 0.39032643475405576\n",
      "Iteration: 3958, training loss: 0.39031996314181083\n",
      "Iteration: 3959, training loss: 0.39031349379864644\n",
      "Iteration: 3960, training loss: 0.39030702672318374\n",
      "Iteration: 3961, training loss: 0.39030056191404494\n",
      "Iteration: 3962, training loss: 0.3902940993698537\n",
      "Iteration: 3963, training loss: 0.39028763908923453\n",
      "Iteration: 3964, training loss: 0.3902811810708132\n",
      "Iteration: 3965, training loss: 0.3902747253132167\n",
      "Iteration: 3966, training loss: 0.39026827181507306\n",
      "Iteration: 3967, training loss: 0.3902618205750115\n",
      "Iteration: 3968, training loss: 0.39025537159166235\n",
      "Iteration: 3969, training loss: 0.3902489248636571\n",
      "Iteration: 3970, training loss: 0.3902424803896284\n",
      "Iteration: 3971, training loss: 0.39023603816821006\n",
      "Iteration: 3972, training loss: 0.39022959819803704\n",
      "Iteration: 3973, training loss: 0.3902231604777453\n",
      "Iteration: 3974, training loss: 0.39021672500597204\n",
      "Iteration: 3975, training loss: 0.39021029178135574\n",
      "Iteration: 3976, training loss: 0.3902038608025357\n",
      "Iteration: 3977, training loss: 0.39019743206815255\n",
      "Iteration: 3978, training loss: 0.39019100557684805\n",
      "Iteration: 3979, training loss: 0.3901845813272652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3980, training loss: 0.3901781593180478\n",
      "Iteration: 3981, training loss: 0.390171739547841\n",
      "Iteration: 3982, training loss: 0.3901653220152912\n",
      "Iteration: 3983, training loss: 0.3901589067190456\n",
      "Iteration: 3984, training loss: 0.3901524936577528\n",
      "Iteration: 3985, training loss: 0.39014608283006225\n",
      "Iteration: 3986, training loss: 0.390139674234625\n",
      "Iteration: 3987, training loss: 0.39013326787009284\n",
      "Iteration: 3988, training loss: 0.39012686373511846\n",
      "Iteration: 3989, training loss: 0.39012046182835636\n",
      "Iteration: 3990, training loss: 0.3901140621484615\n",
      "Iteration: 3991, training loss: 0.3901076646940904\n",
      "Iteration: 3992, training loss: 0.3901012694639005\n",
      "Iteration: 3993, training loss: 0.3900948764565502\n",
      "Iteration: 3994, training loss: 0.3900884856706994\n",
      "Iteration: 3995, training loss: 0.39008209710500874\n",
      "Iteration: 3996, training loss: 0.3900757107581402\n",
      "Iteration: 3997, training loss: 0.39006932662875676\n",
      "Iteration: 3998, training loss: 0.3900629447155226\n",
      "Iteration: 3999, training loss: 0.39005656501710295\n",
      "Iteration: 4000, training loss: 0.3900501875321641\n",
      "Iteration: 4001, training loss: 0.39004381225937346\n",
      "Iteration: 4002, training loss: 0.3900374391973997\n",
      "Iteration: 4003, training loss: 0.3900310683449124\n",
      "Iteration: 4004, training loss: 0.39002469970058223\n",
      "Iteration: 4005, training loss: 0.3900183332630812\n",
      "Iteration: 4006, training loss: 0.39001196903108215\n",
      "Iteration: 4007, training loss: 0.39000560700325926\n",
      "Iteration: 4008, training loss: 0.3899992471782876\n",
      "Iteration: 4009, training loss: 0.38999288955484324\n",
      "Iteration: 4010, training loss: 0.38998653413160367\n",
      "Iteration: 4011, training loss: 0.3899801809072474\n",
      "Iteration: 4012, training loss: 0.38997382988045387\n",
      "Iteration: 4013, training loss: 0.38996748104990353\n",
      "Iteration: 4014, training loss: 0.38996113441427827\n",
      "Iteration: 4015, training loss: 0.38995478997226085\n",
      "Iteration: 4016, training loss: 0.3899484477225352\n",
      "Iteration: 4017, training loss: 0.38994210766378606\n",
      "Iteration: 4018, training loss: 0.3899357697946997\n",
      "Iteration: 4019, training loss: 0.38992943411396314\n",
      "Iteration: 4020, training loss: 0.3899231006202646\n",
      "Iteration: 4021, training loss: 0.3899167693122934\n",
      "Iteration: 4022, training loss: 0.38991044018873994\n",
      "Iteration: 4023, training loss: 0.3899041132482956\n",
      "Iteration: 4024, training loss: 0.3898977884896529\n",
      "Iteration: 4025, training loss: 0.3898914659115055\n",
      "Iteration: 4026, training loss: 0.38988514551254816\n",
      "Iteration: 4027, training loss: 0.3898788272914765\n",
      "Iteration: 4028, training loss: 0.38987251124698735\n",
      "Iteration: 4029, training loss: 0.38986619737777867\n",
      "Iteration: 4030, training loss: 0.3898598856825495\n",
      "Iteration: 4031, training loss: 0.38985357615999977\n",
      "Iteration: 4032, training loss: 0.3898472688088306\n",
      "Iteration: 4033, training loss: 0.3898409636277443\n",
      "Iteration: 4034, training loss: 0.38983466061544386\n",
      "Iteration: 4035, training loss: 0.38982835977063385\n",
      "Iteration: 4036, training loss: 0.3898220610920196\n",
      "Iteration: 4037, training loss: 0.38981576457830747\n",
      "Iteration: 4038, training loss: 0.38980947022820495\n",
      "Iteration: 4039, training loss: 0.38980317804042064\n",
      "Iteration: 4040, training loss: 0.38979688801366424\n",
      "Iteration: 4041, training loss: 0.3897906001466464\n",
      "Iteration: 4042, training loss: 0.3897843144380787\n",
      "Iteration: 4043, training loss: 0.3897780308866741\n",
      "Iteration: 4044, training loss: 0.3897717494911465\n",
      "Iteration: 4045, training loss: 0.38976547025021074\n",
      "Iteration: 4046, training loss: 0.3897591931625826\n",
      "Iteration: 4047, training loss: 0.38975291822697944\n",
      "Iteration: 4048, training loss: 0.389746645442119\n",
      "Iteration: 4049, training loss: 0.3897403748067206\n",
      "Iteration: 4050, training loss: 0.3897341063195043\n",
      "Iteration: 4051, training loss: 0.38972783997919136\n",
      "Iteration: 4052, training loss: 0.389721575784504\n",
      "Iteration: 4053, training loss: 0.3897153137341655\n",
      "Iteration: 4054, training loss: 0.3897090538269003\n",
      "Iteration: 4055, training loss: 0.3897027960614337\n",
      "Iteration: 4056, training loss: 0.3896965404364921\n",
      "Iteration: 4057, training loss: 0.3896902869508032\n",
      "Iteration: 4058, training loss: 0.3896840356030952\n",
      "Iteration: 4059, training loss: 0.3896777863920979\n",
      "Iteration: 4060, training loss: 0.3896715393165418\n",
      "Iteration: 4061, training loss: 0.38966529437515846\n",
      "Iteration: 4062, training loss: 0.3896590515666805\n",
      "Iteration: 4063, training loss: 0.3896528108898419\n",
      "Iteration: 4064, training loss: 0.38964657234337713\n",
      "Iteration: 4065, training loss: 0.38964033592602204\n",
      "Iteration: 4066, training loss: 0.3896341016365133\n",
      "Iteration: 4067, training loss: 0.3896278694735889\n",
      "Iteration: 4068, training loss: 0.38962163943598765\n",
      "Iteration: 4069, training loss: 0.3896154115224495\n",
      "Iteration: 4070, training loss: 0.38960918573171516\n",
      "Iteration: 4071, training loss: 0.3896029620625267\n",
      "Iteration: 4072, training loss: 0.3895967405136271\n",
      "Iteration: 4073, training loss: 0.3895905210837603\n",
      "Iteration: 4074, training loss: 0.3895843037716712\n",
      "Iteration: 4075, training loss: 0.389578088576106\n",
      "Iteration: 4076, training loss: 0.3895718754958116\n",
      "Iteration: 4077, training loss: 0.3895656645295363\n",
      "Iteration: 4078, training loss: 0.3895594556760289\n",
      "Iteration: 4079, training loss: 0.3895532489340397\n",
      "Iteration: 4080, training loss: 0.3895470443023197\n",
      "Iteration: 4081, training loss: 0.3895408417796211\n",
      "Iteration: 4082, training loss: 0.38953464136469707\n",
      "Iteration: 4083, training loss: 0.38952844305630163\n",
      "Iteration: 4084, training loss: 0.38952224685319015\n",
      "Iteration: 4085, training loss: 0.38951605275411866\n",
      "Iteration: 4086, training loss: 0.38950986075784444\n",
      "Iteration: 4087, training loss: 0.38950367086312565\n",
      "Iteration: 4088, training loss: 0.3894974830687216\n",
      "Iteration: 4089, training loss: 0.38949129737339244\n",
      "Iteration: 4090, training loss: 0.38948511377589934\n",
      "Iteration: 4091, training loss: 0.3894789322750045\n",
      "Iteration: 4092, training loss: 0.38947275286947136\n",
      "Iteration: 4093, training loss: 0.3894665755580639\n",
      "Iteration: 4094, training loss: 0.3894604003395476\n",
      "Iteration: 4095, training loss: 0.3894542272126885\n",
      "Iteration: 4096, training loss: 0.38944805617625383\n",
      "Iteration: 4097, training loss: 0.38944188722901213\n",
      "Iteration: 4098, training loss: 0.38943572036973223\n",
      "Iteration: 4099, training loss: 0.38942955559718456\n",
      "Iteration: 4100, training loss: 0.38942339291014033\n",
      "Iteration: 4101, training loss: 0.3894172323073718\n",
      "Iteration: 4102, training loss: 0.3894110737876521\n",
      "Iteration: 4103, training loss: 0.3894049173497555\n",
      "Iteration: 4104, training loss: 0.3893987629924572\n",
      "Iteration: 4105, training loss: 0.38939261071453324\n",
      "Iteration: 4106, training loss: 0.38938646051476095\n",
      "Iteration: 4107, training loss: 0.3893803123919185\n",
      "Iteration: 4108, training loss: 0.38937416634478506\n",
      "Iteration: 4109, training loss: 0.38936802237214063\n",
      "Iteration: 4110, training loss: 0.38936188047276626\n",
      "Iteration: 4111, training loss: 0.3893557406454445\n",
      "Iteration: 4112, training loss: 0.3893496028889579\n",
      "Iteration: 4113, training loss: 0.3893434672020908\n",
      "Iteration: 4114, training loss: 0.3893373335836283\n",
      "Iteration: 4115, training loss: 0.3893312020323563\n",
      "Iteration: 4116, training loss: 0.3893250725470619\n",
      "Iteration: 4117, training loss: 0.38931894512653303\n",
      "Iteration: 4118, training loss: 0.3893128197695586\n",
      "Iteration: 4119, training loss: 0.3893066964749288\n",
      "Iteration: 4120, training loss: 0.3893005752414343\n",
      "Iteration: 4121, training loss: 0.389294456067867\n",
      "Iteration: 4122, training loss: 0.38928833895302\n",
      "Iteration: 4123, training loss: 0.38928222389568684\n",
      "Iteration: 4124, training loss: 0.3892761108946625\n",
      "Iteration: 4125, training loss: 0.3892699999487428\n",
      "Iteration: 4126, training loss: 0.3892638910567244\n",
      "Iteration: 4127, training loss: 0.3892577842174049\n",
      "Iteration: 4128, training loss: 0.38925167942958316\n",
      "Iteration: 4129, training loss: 0.3892455766920587\n",
      "Iteration: 4130, training loss: 0.38923947600363235\n",
      "Iteration: 4131, training loss: 0.38923337736310537\n",
      "Iteration: 4132, training loss: 0.38922728076928054\n",
      "Iteration: 4133, training loss: 0.3892211862209613\n",
      "Iteration: 4134, training loss: 0.38921509371695207\n",
      "Iteration: 4135, training loss: 0.3892090032560584\n",
      "Iteration: 4136, training loss: 0.38920291483708647\n",
      "Iteration: 4137, training loss: 0.38919682845884374\n",
      "Iteration: 4138, training loss: 0.3891907441201385\n",
      "Iteration: 4139, training loss: 0.38918466181978\n",
      "Iteration: 4140, training loss: 0.3891785815565785\n",
      "Iteration: 4141, training loss: 0.38917250332934494\n",
      "Iteration: 4142, training loss: 0.38916642713689176\n",
      "Iteration: 4143, training loss: 0.38916035297803175\n",
      "Iteration: 4144, training loss: 0.38915428085157916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4145, training loss: 0.38914821075634876\n",
      "Iteration: 4146, training loss: 0.38914214269115655\n",
      "Iteration: 4147, training loss: 0.3891360766548195\n",
      "Iteration: 4148, training loss: 0.38913001264615515\n",
      "Iteration: 4149, training loss: 0.38912395066398253\n",
      "Iteration: 4150, training loss: 0.3891178907071213\n",
      "Iteration: 4151, training loss: 0.38911183277439193\n",
      "Iteration: 4152, training loss: 0.38910577686461617\n",
      "Iteration: 4153, training loss: 0.3890997229766165\n",
      "Iteration: 4154, training loss: 0.3890936711092165\n",
      "Iteration: 4155, training loss: 0.38908762126124047\n",
      "Iteration: 4156, training loss: 0.3890815734315139\n",
      "Iteration: 4157, training loss: 0.3890755276188629\n",
      "Iteration: 4158, training loss: 0.38906948382211476\n",
      "Iteration: 4159, training loss: 0.3890634420400978\n",
      "Iteration: 4160, training loss: 0.3890574022716409\n",
      "Iteration: 4161, training loss: 0.38905136451557437\n",
      "Iteration: 4162, training loss: 0.3890453287707289\n",
      "Iteration: 4163, training loss: 0.3890392950359365\n",
      "Iteration: 4164, training loss: 0.3890332633100302\n",
      "Iteration: 4165, training loss: 0.3890272335918434\n",
      "Iteration: 4166, training loss: 0.38902120588021116\n",
      "Iteration: 4167, training loss: 0.3890151801739689\n",
      "Iteration: 4168, training loss: 0.3890091564719532\n",
      "Iteration: 4169, training loss: 0.38900313477300175\n",
      "Iteration: 4170, training loss: 0.38899711507595264\n",
      "Iteration: 4171, training loss: 0.3889910973796454\n",
      "Iteration: 4172, training loss: 0.38898508168292023\n",
      "Iteration: 4173, training loss: 0.3889790679846184\n",
      "Iteration: 4174, training loss: 0.3889730562835818\n",
      "Iteration: 4175, training loss: 0.3889670465786538\n",
      "Iteration: 4176, training loss: 0.38896103886867806\n",
      "Iteration: 4177, training loss: 0.3889550331524996\n",
      "Iteration: 4178, training loss: 0.38894902942896414\n",
      "Iteration: 4179, training loss: 0.38894302769691846\n",
      "Iteration: 4180, training loss: 0.38893702795521\n",
      "Iteration: 4181, training loss: 0.38893103020268754\n",
      "Iteration: 4182, training loss: 0.3889250344382004\n",
      "Iteration: 4183, training loss: 0.388919040660599\n",
      "Iteration: 4184, training loss: 0.3889130488687345\n",
      "Iteration: 4185, training loss: 0.3889070590614593\n",
      "Iteration: 4186, training loss: 0.38890107123762624\n",
      "Iteration: 4187, training loss: 0.3888950853960896\n",
      "Iteration: 4188, training loss: 0.3888891015357041\n",
      "Iteration: 4189, training loss: 0.3888831196553258\n",
      "Iteration: 4190, training loss: 0.3888771397538111\n",
      "Iteration: 4191, training loss: 0.3888711618300181\n",
      "Iteration: 4192, training loss: 0.38886518588280483\n",
      "Iteration: 4193, training loss: 0.38885921191103123\n",
      "Iteration: 4194, training loss: 0.3888532399135573\n",
      "Iteration: 4195, training loss: 0.3888472698892445\n",
      "Iteration: 4196, training loss: 0.38884130183695503\n",
      "Iteration: 4197, training loss: 0.3888353357555518\n",
      "Iteration: 4198, training loss: 0.3888293716438989\n",
      "Iteration: 4199, training loss: 0.38882340950086125\n",
      "Iteration: 4200, training loss: 0.38881744932530443\n",
      "Iteration: 4201, training loss: 0.3888114911160952\n",
      "Iteration: 4202, training loss: 0.3888055348721012\n",
      "Iteration: 4203, training loss: 0.38879958059219066\n",
      "Iteration: 4204, training loss: 0.3887936282752331\n",
      "Iteration: 4205, training loss: 0.38878767792009883\n",
      "Iteration: 4206, training loss: 0.38878172952565887\n",
      "Iteration: 4207, training loss: 0.38877578309078537\n",
      "Iteration: 4208, training loss: 0.3887698386143511\n",
      "Iteration: 4209, training loss: 0.38876389609522993\n",
      "Iteration: 4210, training loss: 0.38875795553229653\n",
      "Iteration: 4211, training loss: 0.3887520169244266\n",
      "Iteration: 4212, training loss: 0.38874608027049656\n",
      "Iteration: 4213, training loss: 0.3887401455693838\n",
      "Iteration: 4214, training loss: 0.3887342128199665\n",
      "Iteration: 4215, training loss: 0.38872828202112386\n",
      "Iteration: 4216, training loss: 0.38872235317173587\n",
      "Iteration: 4217, training loss: 0.38871642627068337\n",
      "Iteration: 4218, training loss: 0.3887105013168483\n",
      "Iteration: 4219, training loss: 0.3887045783091132\n",
      "Iteration: 4220, training loss: 0.3886986572463617\n",
      "Iteration: 4221, training loss: 0.38869273812747823\n",
      "Iteration: 4222, training loss: 0.388686820951348\n",
      "Iteration: 4223, training loss: 0.3886809057168572\n",
      "Iteration: 4224, training loss: 0.388674992422893\n",
      "Iteration: 4225, training loss: 0.38866908106834325\n",
      "Iteration: 4226, training loss: 0.3886631716520969\n",
      "Iteration: 4227, training loss: 0.3886572641730435\n",
      "Iteration: 4228, training loss: 0.38865135863007355\n",
      "Iteration: 4229, training loss: 0.3886454550220787\n",
      "Iteration: 4230, training loss: 0.388639553347951\n",
      "Iteration: 4231, training loss: 0.3886336536065838\n",
      "Iteration: 4232, training loss: 0.3886277557968712\n",
      "Iteration: 4233, training loss: 0.38862185991770803\n",
      "Iteration: 4234, training loss: 0.38861596596799\n",
      "Iteration: 4235, training loss: 0.38861007394661373\n",
      "Iteration: 4236, training loss: 0.38860418385247697\n",
      "Iteration: 4237, training loss: 0.38859829568447796\n",
      "Iteration: 4238, training loss: 0.38859240944151585\n",
      "Iteration: 4239, training loss: 0.3885865251224909\n",
      "Iteration: 4240, training loss: 0.38858064272630416\n",
      "Iteration: 4241, training loss: 0.3885747622518573\n",
      "Iteration: 4242, training loss: 0.3885688836980531\n",
      "Iteration: 4243, training loss: 0.38856300706379504\n",
      "Iteration: 4244, training loss: 0.38855713234798755\n",
      "Iteration: 4245, training loss: 0.388551259549536\n",
      "Iteration: 4246, training loss: 0.38854538866734645\n",
      "Iteration: 4247, training loss: 0.388539519700326\n",
      "Iteration: 4248, training loss: 0.3885336526473824\n",
      "Iteration: 4249, training loss: 0.38852778750742434\n",
      "Iteration: 4250, training loss: 0.3885219242793614\n",
      "Iteration: 4251, training loss: 0.3885160629621041\n",
      "Iteration: 4252, training loss: 0.3885102035545635\n",
      "Iteration: 4253, training loss: 0.38850434605565193\n",
      "Iteration: 4254, training loss: 0.3884984904642822\n",
      "Iteration: 4255, training loss: 0.3884926367793682\n",
      "Iteration: 4256, training loss: 0.3884867849998247\n",
      "Iteration: 4257, training loss: 0.388480935124567\n",
      "Iteration: 4258, training loss: 0.38847508715251156\n",
      "Iteration: 4259, training loss: 0.38846924108257563\n",
      "Iteration: 4260, training loss: 0.38846339691367726\n",
      "Iteration: 4261, training loss: 0.3884575546447353\n",
      "Iteration: 4262, training loss: 0.3884517142746696\n",
      "Iteration: 4263, training loss: 0.3884458758024006\n",
      "Iteration: 4264, training loss: 0.38844003922684994\n",
      "Iteration: 4265, training loss: 0.3884342045469397\n",
      "Iteration: 4266, training loss: 0.38842837176159306\n",
      "Iteration: 4267, training loss: 0.38842254086973405\n",
      "Iteration: 4268, training loss: 0.3884167118702875\n",
      "Iteration: 4269, training loss: 0.3884108847621788\n",
      "Iteration: 4270, training loss: 0.3884050595443346\n",
      "Iteration: 4271, training loss: 0.38839923621568223\n",
      "Iteration: 4272, training loss: 0.38839341477514977\n",
      "Iteration: 4273, training loss: 0.3883875952216662\n",
      "Iteration: 4274, training loss: 0.38838177755416137\n",
      "Iteration: 4275, training loss: 0.3883759617715659\n",
      "Iteration: 4276, training loss: 0.3883701478728114\n",
      "Iteration: 4277, training loss: 0.38836433585683\n",
      "Iteration: 4278, training loss: 0.3883585257225549\n",
      "Iteration: 4279, training loss: 0.38835271746892014\n",
      "Iteration: 4280, training loss: 0.38834691109486047\n",
      "Iteration: 4281, training loss: 0.38834110659931154\n",
      "Iteration: 4282, training loss: 0.38833530398120975\n",
      "Iteration: 4283, training loss: 0.3883295032394925\n",
      "Iteration: 4284, training loss: 0.3883237043730978\n",
      "Iteration: 4285, training loss: 0.3883179073809647\n",
      "Iteration: 4286, training loss: 0.3883121122620329\n",
      "Iteration: 4287, training loss: 0.38830631901524293\n",
      "Iteration: 4288, training loss: 0.38830052763953626\n",
      "Iteration: 4289, training loss: 0.3882947381338552\n",
      "Iteration: 4290, training loss: 0.3882889504971426\n",
      "Iteration: 4291, training loss: 0.38828316472834257\n",
      "Iteration: 4292, training loss: 0.3882773808263996\n",
      "Iteration: 4293, training loss: 0.3882715987902594\n",
      "Iteration: 4294, training loss: 0.38826581861886816\n",
      "Iteration: 4295, training loss: 0.3882600403111731\n",
      "Iteration: 4296, training loss: 0.3882542638661221\n",
      "Iteration: 4297, training loss: 0.388248489282664\n",
      "Iteration: 4298, training loss: 0.38824271655974846\n",
      "Iteration: 4299, training loss: 0.3882369456963258\n",
      "Iteration: 4300, training loss: 0.3882311766913472\n",
      "Iteration: 4301, training loss: 0.3882254095437647\n",
      "Iteration: 4302, training loss: 0.38821964425253136\n",
      "Iteration: 4303, training loss: 0.38821388081660047\n",
      "Iteration: 4304, training loss: 0.38820811923492693\n",
      "Iteration: 4305, training loss: 0.38820235950646576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4306, training loss: 0.38819660163017305\n",
      "Iteration: 4307, training loss: 0.38819084560500566\n",
      "Iteration: 4308, training loss: 0.3881850914299214\n",
      "Iteration: 4309, training loss: 0.38817933910387875\n",
      "Iteration: 4310, training loss: 0.38817358862583695\n",
      "Iteration: 4311, training loss: 0.3881678399947563\n",
      "Iteration: 4312, training loss: 0.3881620932095975\n",
      "Iteration: 4313, training loss: 0.3881563482693223\n",
      "Iteration: 4314, training loss: 0.38815060517289335\n",
      "Iteration: 4315, training loss: 0.3881448639192739\n",
      "Iteration: 4316, training loss: 0.3881391245074281\n",
      "Iteration: 4317, training loss: 0.3881333869363209\n",
      "Iteration: 4318, training loss: 0.388127651204918\n",
      "Iteration: 4319, training loss: 0.38812191731218576\n",
      "Iteration: 4320, training loss: 0.38811618525709174\n",
      "Iteration: 4321, training loss: 0.388110455038604\n",
      "Iteration: 4322, training loss: 0.38810472665569146\n",
      "Iteration: 4323, training loss: 0.3880990001073237\n",
      "Iteration: 4324, training loss: 0.38809327539247135\n",
      "Iteration: 4325, training loss: 0.38808755251010574\n",
      "Iteration: 4326, training loss: 0.3880818314591988\n",
      "Iteration: 4327, training loss: 0.3880761122387235\n",
      "Iteration: 4328, training loss: 0.38807039484765365\n",
      "Iteration: 4329, training loss: 0.38806467928496347\n",
      "Iteration: 4330, training loss: 0.38805896554962827\n",
      "Iteration: 4331, training loss: 0.3880532536406242\n",
      "Iteration: 4332, training loss: 0.38804754355692805\n",
      "Iteration: 4333, training loss: 0.38804183529751746\n",
      "Iteration: 4334, training loss: 0.3880361288613707\n",
      "Iteration: 4335, training loss: 0.3880304242474671\n",
      "Iteration: 4336, training loss: 0.3880247214547865\n",
      "Iteration: 4337, training loss: 0.38801902048230985\n",
      "Iteration: 4338, training loss: 0.38801332132901856\n",
      "Iteration: 4339, training loss: 0.38800762399389493\n",
      "Iteration: 4340, training loss: 0.3880019284759222\n",
      "Iteration: 4341, training loss: 0.3879962347740841\n",
      "Iteration: 4342, training loss: 0.38799054288736534\n",
      "Iteration: 4343, training loss: 0.3879848528147516\n",
      "Iteration: 4344, training loss: 0.3879791645552287\n",
      "Iteration: 4345, training loss: 0.3879734781077839\n",
      "Iteration: 4346, training loss: 0.38796779347140503\n",
      "Iteration: 4347, training loss: 0.38796211064508046\n",
      "Iteration: 4348, training loss: 0.3879564296277997\n",
      "Iteration: 4349, training loss: 0.38795075041855276\n",
      "Iteration: 4350, training loss: 0.38794507301633047\n",
      "Iteration: 4351, training loss: 0.3879393974201246\n",
      "Iteration: 4352, training loss: 0.38793372362892764\n",
      "Iteration: 4353, training loss: 0.38792805164173255\n",
      "Iteration: 4354, training loss: 0.38792238145753355\n",
      "Iteration: 4355, training loss: 0.3879167130753253\n",
      "Iteration: 4356, training loss: 0.3879110464941033\n",
      "Iteration: 4357, training loss: 0.3879053817128639\n",
      "Iteration: 4358, training loss: 0.387899718730604\n",
      "Iteration: 4359, training loss: 0.38789405754632156\n",
      "Iteration: 4360, training loss: 0.3878883981590151\n",
      "Iteration: 4361, training loss: 0.387882740567684\n",
      "Iteration: 4362, training loss: 0.38787708477132843\n",
      "Iteration: 4363, training loss: 0.3878714307689492\n",
      "Iteration: 4364, training loss: 0.387865778559548\n",
      "Iteration: 4365, training loss: 0.3878601281421272\n",
      "Iteration: 4366, training loss: 0.38785447951569\n",
      "Iteration: 4367, training loss: 0.38784883267924036\n",
      "Iteration: 4368, training loss: 0.38784318763178294\n",
      "Iteration: 4369, training loss: 0.38783754437232315\n",
      "Iteration: 4370, training loss: 0.3878319028998673\n",
      "Iteration: 4371, training loss: 0.38782626321342245\n",
      "Iteration: 4372, training loss: 0.38782062531199607\n",
      "Iteration: 4373, training loss: 0.38781498919459684\n",
      "Iteration: 4374, training loss: 0.38780935486023393\n",
      "Iteration: 4375, training loss: 0.3878037223079174\n",
      "Iteration: 4376, training loss: 0.387798091536658\n",
      "Iteration: 4377, training loss: 0.38779246254546723\n",
      "Iteration: 4378, training loss: 0.3877868353333574\n",
      "Iteration: 4379, training loss: 0.3877812098993414\n",
      "Iteration: 4380, training loss: 0.38777558624243313\n",
      "Iteration: 4381, training loss: 0.387769964361647\n",
      "Iteration: 4382, training loss: 0.3877643442559984\n",
      "Iteration: 4383, training loss: 0.38775872592450333\n",
      "Iteration: 4384, training loss: 0.38775310936617857\n",
      "Iteration: 4385, training loss: 0.38774749458004154\n",
      "Iteration: 4386, training loss: 0.38774188156511064\n",
      "Iteration: 4387, training loss: 0.38773627032040486\n",
      "Iteration: 4388, training loss: 0.3877306608449439\n",
      "Iteration: 4389, training loss: 0.3877250531377482\n",
      "Iteration: 4390, training loss: 0.38771944719783924\n",
      "Iteration: 4391, training loss: 0.3877138430242389\n",
      "Iteration: 4392, training loss: 0.3877082406159698\n",
      "Iteration: 4393, training loss: 0.3877026399720557\n",
      "Iteration: 4394, training loss: 0.3876970410915206\n",
      "Iteration: 4395, training loss: 0.3876914439733895\n",
      "Iteration: 4396, training loss: 0.3876858486166882\n",
      "Iteration: 4397, training loss: 0.3876802550204431\n",
      "Iteration: 4398, training loss: 0.3876746631836814\n",
      "Iteration: 4399, training loss: 0.38766907310543103\n",
      "Iteration: 4400, training loss: 0.3876634847847207\n",
      "Iteration: 4401, training loss: 0.38765789822057967\n",
      "Iteration: 4402, training loss: 0.38765231341203815\n",
      "Iteration: 4403, training loss: 0.387646730358127\n",
      "Iteration: 4404, training loss: 0.3876411490578779\n",
      "Iteration: 4405, training loss: 0.3876355695103232\n",
      "Iteration: 4406, training loss: 0.38762999171449597\n",
      "Iteration: 4407, training loss: 0.3876244156694299\n",
      "Iteration: 4408, training loss: 0.3876188413741596\n",
      "Iteration: 4409, training loss: 0.3876132688277204\n",
      "Iteration: 4410, training loss: 0.38760769802914835\n",
      "Iteration: 4411, training loss: 0.3876021289774799\n",
      "Iteration: 4412, training loss: 0.3875965616717529\n",
      "Iteration: 4413, training loss: 0.38759099611100534\n",
      "Iteration: 4414, training loss: 0.38758543229427606\n",
      "Iteration: 4415, training loss: 0.38757987022060486\n",
      "Iteration: 4416, training loss: 0.38757430988903213\n",
      "Iteration: 4417, training loss: 0.3875687512985989\n",
      "Iteration: 4418, training loss: 0.387563194448347\n",
      "Iteration: 4419, training loss: 0.387557639337319\n",
      "Iteration: 4420, training loss: 0.38755208596455815\n",
      "Iteration: 4421, training loss: 0.38754653432910846\n",
      "Iteration: 4422, training loss: 0.38754098443001467\n",
      "Iteration: 4423, training loss: 0.3875354362663222\n",
      "Iteration: 4424, training loss: 0.3875298898370772\n",
      "Iteration: 4425, training loss: 0.3875243451413266\n",
      "Iteration: 4426, training loss: 0.387518802178118\n",
      "Iteration: 4427, training loss: 0.38751326094649974\n",
      "Iteration: 4428, training loss: 0.3875077214455208\n",
      "Iteration: 4429, training loss: 0.38750218367423095\n",
      "Iteration: 4430, training loss: 0.38749664763168074\n",
      "Iteration: 4431, training loss: 0.38749111331692127\n",
      "Iteration: 4432, training loss: 0.3874855807290044\n",
      "Iteration: 4433, training loss: 0.387480049866983\n",
      "Iteration: 4434, training loss: 0.3874745207299102\n",
      "Iteration: 4435, training loss: 0.3874689933168401\n",
      "Iteration: 4436, training loss: 0.38746346762682743\n",
      "Iteration: 4437, training loss: 0.3874579436589278\n",
      "Iteration: 4438, training loss: 0.3874524214121972\n",
      "Iteration: 4439, training loss: 0.3874469008856927\n",
      "Iteration: 4440, training loss: 0.38744138207847173\n",
      "Iteration: 4441, training loss: 0.3874358649895928\n",
      "Iteration: 4442, training loss: 0.3874303496181149\n",
      "Iteration: 4443, training loss: 0.3874248359630977\n",
      "Iteration: 4444, training loss: 0.38741932402360174\n",
      "Iteration: 4445, training loss: 0.387413813798688\n",
      "Iteration: 4446, training loss: 0.38740830528741843\n",
      "Iteration: 4447, training loss: 0.3874027984888556\n",
      "Iteration: 4448, training loss: 0.3873972934020628\n",
      "Iteration: 4449, training loss: 0.38739179002610397\n",
      "Iteration: 4450, training loss: 0.3873862883600438\n",
      "Iteration: 4451, training loss: 0.38738078840294765\n",
      "Iteration: 4452, training loss: 0.38737529015388156\n",
      "Iteration: 4453, training loss: 0.3873697936119124\n",
      "Iteration: 4454, training loss: 0.38736429877610756\n",
      "Iteration: 4455, training loss: 0.3873588056455354\n",
      "Iteration: 4456, training loss: 0.3873533142192646\n",
      "Iteration: 4457, training loss: 0.3873478244963648\n",
      "Iteration: 4458, training loss: 0.3873423364759063\n",
      "Iteration: 4459, training loss: 0.3873368501569601\n",
      "Iteration: 4460, training loss: 0.38733136553859787\n",
      "Iteration: 4461, training loss: 0.38732588261989187\n",
      "Iteration: 4462, training loss: 0.38732040139991536\n",
      "Iteration: 4463, training loss: 0.3873149218777418\n",
      "Iteration: 4464, training loss: 0.3873094440524459\n",
      "Iteration: 4465, training loss: 0.3873039679231028\n",
      "Iteration: 4466, training loss: 0.3872984934887883\n",
      "Iteration: 4467, training loss: 0.3872930207485788\n",
      "Iteration: 4468, training loss: 0.3872875497015517\n",
      "Iteration: 4469, training loss: 0.3872820803467848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4470, training loss: 0.38727661268335684\n",
      "Iteration: 4471, training loss: 0.38727114671034707\n",
      "Iteration: 4472, training loss: 0.38726568242683534\n",
      "Iteration: 4473, training loss: 0.3872602198319024\n",
      "Iteration: 4474, training loss: 0.38725475892462957\n",
      "Iteration: 4475, training loss: 0.38724929970409916\n",
      "Iteration: 4476, training loss: 0.3872438421693935\n",
      "Iteration: 4477, training loss: 0.3872383863195963\n",
      "Iteration: 4478, training loss: 0.38723293215379156\n",
      "Iteration: 4479, training loss: 0.38722747967106413\n",
      "Iteration: 4480, training loss: 0.38722202887049934\n",
      "Iteration: 4481, training loss: 0.38721657975118345\n",
      "Iteration: 4482, training loss: 0.38721113231220333\n",
      "Iteration: 4483, training loss: 0.3872056865526465\n",
      "Iteration: 4484, training loss: 0.387200242471601\n",
      "Iteration: 4485, training loss: 0.387194800068156\n",
      "Iteration: 4486, training loss: 0.3871893593414008\n",
      "Iteration: 4487, training loss: 0.38718392029042564\n",
      "Iteration: 4488, training loss: 0.38717848291432166\n",
      "Iteration: 4489, training loss: 0.3871730472121803\n",
      "Iteration: 4490, training loss: 0.38716761318309384\n",
      "Iteration: 4491, training loss: 0.3871621808261553\n",
      "Iteration: 4492, training loss: 0.3871567501404583\n",
      "Iteration: 4493, training loss: 0.38715132112509704\n",
      "Iteration: 4494, training loss: 0.3871458937791666\n",
      "Iteration: 4495, training loss: 0.38714046810176267\n",
      "Iteration: 4496, training loss: 0.38713504409198146\n",
      "Iteration: 4497, training loss: 0.38712962174892\n",
      "Iteration: 4498, training loss: 0.38712420107167606\n",
      "Iteration: 4499, training loss: 0.38711878205934785\n",
      "Iteration: 4500, training loss: 0.38711336471103447\n",
      "Iteration: 4501, training loss: 0.38710794902583556\n",
      "Iteration: 4502, training loss: 0.3871025350028515\n",
      "Iteration: 4503, training loss: 0.3870971226411833\n",
      "Iteration: 4504, training loss: 0.3870917119399327\n",
      "Iteration: 4505, training loss: 0.38708630289820195\n",
      "Iteration: 4506, training loss: 0.3870808955150943\n",
      "Iteration: 4507, training loss: 0.3870754897897132\n",
      "Iteration: 4508, training loss: 0.3870700857211631\n",
      "Iteration: 4509, training loss: 0.38706468330854904\n",
      "Iteration: 4510, training loss: 0.38705928255097666\n",
      "Iteration: 4511, training loss: 0.38705388344755237\n",
      "Iteration: 4512, training loss: 0.3870484859973831\n",
      "Iteration: 4513, training loss: 0.38704309019957683\n",
      "Iteration: 4514, training loss: 0.38703769605324156\n",
      "Iteration: 4515, training loss: 0.38703230355748636\n",
      "Iteration: 4516, training loss: 0.3870269127114211\n",
      "Iteration: 4517, training loss: 0.38702152351415586\n",
      "Iteration: 4518, training loss: 0.3870161359648018\n",
      "Iteration: 4519, training loss: 0.38701075006247043\n",
      "Iteration: 4520, training loss: 0.38700536580627415\n",
      "Iteration: 4521, training loss: 0.3869999831953259\n",
      "Iteration: 4522, training loss: 0.38699460222873927\n",
      "Iteration: 4523, training loss: 0.38698922290562865\n",
      "Iteration: 4524, training loss: 0.38698384522510887\n",
      "Iteration: 4525, training loss: 0.38697846918629547\n",
      "Iteration: 4526, training loss: 0.3869730947883048\n",
      "Iteration: 4527, training loss: 0.3869677220302537\n",
      "Iteration: 4528, training loss: 0.38696235091125974\n",
      "Iteration: 4529, training loss: 0.38695698143044116\n",
      "Iteration: 4530, training loss: 0.3869516135869167\n",
      "Iteration: 4531, training loss: 0.38694624737980615\n",
      "Iteration: 4532, training loss: 0.3869408828082293\n",
      "Iteration: 4533, training loss: 0.3869355198713072\n",
      "Iteration: 4534, training loss: 0.3869301585681612\n",
      "Iteration: 4535, training loss: 0.38692479889791354\n",
      "Iteration: 4536, training loss: 0.3869194408596869\n",
      "Iteration: 4537, training loss: 0.3869140844526046\n",
      "Iteration: 4538, training loss: 0.3869087296757909\n",
      "Iteration: 4539, training loss: 0.3869033765283702\n",
      "Iteration: 4540, training loss: 0.3868980250094683\n",
      "Iteration: 4541, training loss: 0.3868926751182108\n",
      "Iteration: 4542, training loss: 0.38688732685372446\n",
      "Iteration: 4543, training loss: 0.3868819802151366\n",
      "Iteration: 4544, training loss: 0.38687663520157506\n",
      "Iteration: 4545, training loss: 0.38687129181216867\n",
      "Iteration: 4546, training loss: 0.3868659500460463\n",
      "Iteration: 4547, training loss: 0.38686060990233795\n",
      "Iteration: 4548, training loss: 0.3868552713801742\n",
      "Iteration: 4549, training loss: 0.3868499344786862\n",
      "Iteration: 4550, training loss: 0.3868445991970056\n",
      "Iteration: 4551, training loss: 0.3868392655342649\n",
      "Iteration: 4552, training loss: 0.386833933489597\n",
      "Iteration: 4553, training loss: 0.38682860306213596\n",
      "Iteration: 4554, training loss: 0.3868232742510157\n",
      "Iteration: 4555, training loss: 0.38681794705537154\n",
      "Iteration: 4556, training loss: 0.3868126214743389\n",
      "Iteration: 4557, training loss: 0.38680729750705406\n",
      "Iteration: 4558, training loss: 0.38680197515265385\n",
      "Iteration: 4559, training loss: 0.38679665441027605\n",
      "Iteration: 4560, training loss: 0.38679133527905846\n",
      "Iteration: 4561, training loss: 0.3867860177581401\n",
      "Iteration: 4562, training loss: 0.38678070184666025\n",
      "Iteration: 4563, training loss: 0.38677538754375906\n",
      "Iteration: 4564, training loss: 0.3867700748485772\n",
      "Iteration: 4565, training loss: 0.386764763760256\n",
      "Iteration: 4566, training loss: 0.38675945427793734\n",
      "Iteration: 4567, training loss: 0.3867541464007639\n",
      "Iteration: 4568, training loss: 0.3867488401278789\n",
      "Iteration: 4569, training loss: 0.386743535458426\n",
      "Iteration: 4570, training loss: 0.3867382323915498\n",
      "Iteration: 4571, training loss: 0.3867329309263954\n",
      "Iteration: 4572, training loss: 0.38672763106210845\n",
      "Iteration: 4573, training loss: 0.3867223327978355\n",
      "Iteration: 4574, training loss: 0.3867170361327233\n",
      "Iteration: 4575, training loss: 0.38671174106591943\n",
      "Iteration: 4576, training loss: 0.38670644759657236\n",
      "Iteration: 4577, training loss: 0.3867011557238307\n",
      "Iteration: 4578, training loss: 0.3866958654468441\n",
      "Iteration: 4579, training loss: 0.3866905767647625\n",
      "Iteration: 4580, training loss: 0.3866852896767368\n",
      "Iteration: 4581, training loss: 0.38668000418191817\n",
      "Iteration: 4582, training loss: 0.3866747202794588\n",
      "Iteration: 4583, training loss: 0.38666943796851105\n",
      "Iteration: 4584, training loss: 0.3866641572482282\n",
      "Iteration: 4585, training loss: 0.38665887811776417\n",
      "Iteration: 4586, training loss: 0.3866536005762733\n",
      "Iteration: 4587, training loss: 0.3866483246229107\n",
      "Iteration: 4588, training loss: 0.3866430502568321\n",
      "Iteration: 4589, training loss: 0.3866377774771937\n",
      "Iteration: 4590, training loss: 0.38663250628315254\n",
      "Iteration: 4591, training loss: 0.38662723667386606\n",
      "Iteration: 4592, training loss: 0.3866219686484925\n",
      "Iteration: 4593, training loss: 0.3866167022061905\n",
      "Iteration: 4594, training loss: 0.3866114373461196\n",
      "Iteration: 4595, training loss: 0.3866061740674397\n",
      "Iteration: 4596, training loss: 0.38660091236931143\n",
      "Iteration: 4597, training loss: 0.38659565225089604\n",
      "Iteration: 4598, training loss: 0.38659039371135534\n",
      "Iteration: 4599, training loss: 0.38658513674985184\n",
      "Iteration: 4600, training loss: 0.38657988136554855\n",
      "Iteration: 4601, training loss: 0.38657462755760913\n",
      "Iteration: 4602, training loss: 0.3865693753251979\n",
      "Iteration: 4603, training loss: 0.38656412466747975\n",
      "Iteration: 4604, training loss: 0.38655887558362007\n",
      "Iteration: 4605, training loss: 0.38655362807278526\n",
      "Iteration: 4606, training loss: 0.3865483821341418\n",
      "Iteration: 4607, training loss: 0.38654313776685706\n",
      "Iteration: 4608, training loss: 0.386537894970099\n",
      "Iteration: 4609, training loss: 0.38653265374303614\n",
      "Iteration: 4610, training loss: 0.3865274140848376\n",
      "Iteration: 4611, training loss: 0.3865221759946733\n",
      "Iteration: 4612, training loss: 0.38651693947171345\n",
      "Iteration: 4613, training loss: 0.38651170451512895\n",
      "Iteration: 4614, training loss: 0.38650647112409153\n",
      "Iteration: 4615, training loss: 0.3865012392977733\n",
      "Iteration: 4616, training loss: 0.386496009035347\n",
      "Iteration: 4617, training loss: 0.386490780335986\n",
      "Iteration: 4618, training loss: 0.38648555319886435\n",
      "Iteration: 4619, training loss: 0.3864803276231566\n",
      "Iteration: 4620, training loss: 0.3864751036080379\n",
      "Iteration: 4621, training loss: 0.386469881152684\n",
      "Iteration: 4622, training loss: 0.3864646602562713\n",
      "Iteration: 4623, training loss: 0.38645944091797685\n",
      "Iteration: 4624, training loss: 0.38645422313697814\n",
      "Iteration: 4625, training loss: 0.38644900691245343\n",
      "Iteration: 4626, training loss: 0.38644379224358133\n",
      "Iteration: 4627, training loss: 0.3864385791295413\n",
      "Iteration: 4628, training loss: 0.38643336756951346\n",
      "Iteration: 4629, training loss: 0.3864281575626782\n",
      "Iteration: 4630, training loss: 0.3864229491082166\n",
      "Iteration: 4631, training loss: 0.3864177422053105\n",
      "Iteration: 4632, training loss: 0.38641253685314236\n",
      "Iteration: 4633, training loss: 0.38640733305089486\n",
      "Iteration: 4634, training loss: 0.38640213079775176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4635, training loss: 0.386396930092897\n",
      "Iteration: 4636, training loss: 0.3863917309355155\n",
      "Iteration: 4637, training loss: 0.38638653332479234\n",
      "Iteration: 4638, training loss: 0.38638133725991364\n",
      "Iteration: 4639, training loss: 0.3863761427400656\n",
      "Iteration: 4640, training loss: 0.38637094976443553\n",
      "Iteration: 4641, training loss: 0.3863657583322111\n",
      "Iteration: 4642, training loss: 0.38636056844258043\n",
      "Iteration: 4643, training loss: 0.3863553800947324\n",
      "Iteration: 4644, training loss: 0.3863501932878566\n",
      "Iteration: 4645, training loss: 0.38634500802114263\n",
      "Iteration: 4646, training loss: 0.38633982429378155\n",
      "Iteration: 4647, training loss: 0.38633464210496427\n",
      "Iteration: 4648, training loss: 0.3863294614538828\n",
      "Iteration: 4649, training loss: 0.3863242823397292\n",
      "Iteration: 4650, training loss: 0.38631910476169656\n",
      "Iteration: 4651, training loss: 0.3863139287189785\n",
      "Iteration: 4652, training loss: 0.386308754210769\n",
      "Iteration: 4653, training loss: 0.3863035812362628\n",
      "Iteration: 4654, training loss: 0.386298409794655\n",
      "Iteration: 4655, training loss: 0.3862932398851418\n",
      "Iteration: 4656, training loss: 0.38628807150691946\n",
      "Iteration: 4657, training loss: 0.38628290465918497\n",
      "Iteration: 4658, training loss: 0.3862777393411359\n",
      "Iteration: 4659, training loss: 0.3862725755519705\n",
      "Iteration: 4660, training loss: 0.38626741329088743\n",
      "Iteration: 4661, training loss: 0.3862622525570862\n",
      "Iteration: 4662, training loss: 0.38625709334976654\n",
      "Iteration: 4663, training loss: 0.3862519356681291\n",
      "Iteration: 4664, training loss: 0.38624677951137476\n",
      "Iteration: 4665, training loss: 0.3862416248787054\n",
      "Iteration: 4666, training loss: 0.386236471769323\n",
      "Iteration: 4667, training loss: 0.3862313201824305\n",
      "Iteration: 4668, training loss: 0.38622617011723126\n",
      "Iteration: 4669, training loss: 0.38622102157292915\n",
      "Iteration: 4670, training loss: 0.3862158745487288\n",
      "Iteration: 4671, training loss: 0.3862107290438353\n",
      "Iteration: 4672, training loss: 0.38620558505745417\n",
      "Iteration: 4673, training loss: 0.38620044258879177\n",
      "Iteration: 4674, training loss: 0.38619530163705484\n",
      "Iteration: 4675, training loss: 0.3861901622014508\n",
      "Iteration: 4676, training loss: 0.3861850242811875\n",
      "Iteration: 4677, training loss: 0.3861798878754737\n",
      "Iteration: 4678, training loss: 0.3861747529835182\n",
      "Iteration: 4679, training loss: 0.38616961960453083\n",
      "Iteration: 4680, training loss: 0.3861644877377219\n",
      "Iteration: 4681, training loss: 0.38615935738230195\n",
      "Iteration: 4682, training loss: 0.3861542285374826\n",
      "Iteration: 4683, training loss: 0.3861491012024756\n",
      "Iteration: 4684, training loss: 0.38614397537649364\n",
      "Iteration: 4685, training loss: 0.3861388510587495\n",
      "Iteration: 4686, training loss: 0.38613372824845704\n",
      "Iteration: 4687, training loss: 0.38612860694483037\n",
      "Iteration: 4688, training loss: 0.3861234871470843\n",
      "Iteration: 4689, training loss: 0.38611836885443407\n",
      "Iteration: 4690, training loss: 0.3861132520660957\n",
      "Iteration: 4691, training loss: 0.38610813678128547\n",
      "Iteration: 4692, training loss: 0.3861030229992205\n",
      "Iteration: 4693, training loss: 0.3860979107191184\n",
      "Iteration: 4694, training loss: 0.3860927999401972\n",
      "Iteration: 4695, training loss: 0.38608769066167564\n",
      "Iteration: 4696, training loss: 0.38608258288277303\n",
      "Iteration: 4697, training loss: 0.3860774766027091\n",
      "Iteration: 4698, training loss: 0.3860723718207043\n",
      "Iteration: 4699, training loss: 0.3860672685359795\n",
      "Iteration: 4700, training loss: 0.3860621667477563\n",
      "Iteration: 4701, training loss: 0.38605706645525667\n",
      "Iteration: 4702, training loss: 0.3860519676577032\n",
      "Iteration: 4703, training loss: 0.3860468703543191\n",
      "Iteration: 4704, training loss: 0.3860417745443281\n",
      "Iteration: 4705, training loss: 0.3860366802269545\n",
      "Iteration: 4706, training loss: 0.38603158740142307\n",
      "Iteration: 4707, training loss: 0.38602649606695916\n",
      "Iteration: 4708, training loss: 0.38602140622278897\n",
      "Iteration: 4709, training loss: 0.38601631786813867\n",
      "Iteration: 4710, training loss: 0.3860112310022355\n",
      "Iteration: 4711, training loss: 0.38600614562430713\n",
      "Iteration: 4712, training loss: 0.3860010617335815\n",
      "Iteration: 4713, training loss: 0.3859959793292875\n",
      "Iteration: 4714, training loss: 0.38599089841065437\n",
      "Iteration: 4715, training loss: 0.3859858189769118\n",
      "Iteration: 4716, training loss: 0.3859807410272903\n",
      "Iteration: 4717, training loss: 0.38597566456102067\n",
      "Iteration: 4718, training loss: 0.3859705895773345\n",
      "Iteration: 4719, training loss: 0.38596551607546364\n",
      "Iteration: 4720, training loss: 0.3859604440546408\n",
      "Iteration: 4721, training loss: 0.3859553735140991\n",
      "Iteration: 4722, training loss: 0.385950304453072\n",
      "Iteration: 4723, training loss: 0.3859452368707939\n",
      "Iteration: 4724, training loss: 0.3859401707664995\n",
      "Iteration: 4725, training loss: 0.3859351061394241\n",
      "Iteration: 4726, training loss: 0.38593004298880346\n",
      "Iteration: 4727, training loss: 0.38592498131387404\n",
      "Iteration: 4728, training loss: 0.38591992111387285\n",
      "Iteration: 4729, training loss: 0.3859148623880372\n",
      "Iteration: 4730, training loss: 0.3859098051356052\n",
      "Iteration: 4731, training loss: 0.3859047493558155\n",
      "Iteration: 4732, training loss: 0.385899695047907\n",
      "Iteration: 4733, training loss: 0.3858946422111195\n",
      "Iteration: 4734, training loss: 0.3858895908446932\n",
      "Iteration: 4735, training loss: 0.3858845409478688\n",
      "Iteration: 4736, training loss: 0.3858794925198875\n",
      "Iteration: 4737, training loss: 0.3858744455599912\n",
      "Iteration: 4738, training loss: 0.3858694000674222\n",
      "Iteration: 4739, training loss: 0.38586435604142344\n",
      "Iteration: 4740, training loss: 0.38585931348123836\n",
      "Iteration: 4741, training loss: 0.3858542723861109\n",
      "Iteration: 4742, training loss: 0.38584923275528554\n",
      "Iteration: 4743, training loss: 0.38584419458800734\n",
      "Iteration: 4744, training loss: 0.3858391578835219\n",
      "Iteration: 4745, training loss: 0.3858341226410754\n",
      "Iteration: 4746, training loss: 0.3858290888599144\n",
      "Iteration: 4747, training loss: 0.3858240565392861\n",
      "Iteration: 4748, training loss: 0.3858190256784383\n",
      "Iteration: 4749, training loss: 0.3858139962766192\n",
      "Iteration: 4750, training loss: 0.3858089683330776\n",
      "Iteration: 4751, training loss: 0.3858039418470628\n",
      "Iteration: 4752, training loss: 0.3857989168178247\n",
      "Iteration: 4753, training loss: 0.3857938932446137\n",
      "Iteration: 4754, training loss: 0.3857888711266807\n",
      "Iteration: 4755, training loss: 0.3857838504632772\n",
      "Iteration: 4756, training loss: 0.38577883125365514\n",
      "Iteration: 4757, training loss: 0.3857738134970671\n",
      "Iteration: 4758, training loss: 0.3857687971927661\n",
      "Iteration: 4759, training loss: 0.3857637823400056\n",
      "Iteration: 4760, training loss: 0.3857587689380399\n",
      "Iteration: 4761, training loss: 0.3857537569861235\n",
      "Iteration: 4762, training loss: 0.3857487464835117\n",
      "Iteration: 4763, training loss: 0.3857437374294601\n",
      "Iteration: 4764, training loss: 0.38573872982322494\n",
      "Iteration: 4765, training loss: 0.38573372366406306\n",
      "Iteration: 4766, training loss: 0.38572871895123156\n",
      "Iteration: 4767, training loss: 0.3857237156839884\n",
      "Iteration: 4768, training loss: 0.3857187138615917\n",
      "Iteration: 4769, training loss: 0.3857137134833006\n",
      "Iteration: 4770, training loss: 0.3857087145483743\n",
      "Iteration: 4771, training loss: 0.3857037170560727\n",
      "Iteration: 4772, training loss: 0.38569872100565633\n",
      "Iteration: 4773, training loss: 0.385693726396386\n",
      "Iteration: 4774, training loss: 0.3856887332275233\n",
      "Iteration: 4775, training loss: 0.3856837414983302\n",
      "Iteration: 4776, training loss: 0.38567875120806916\n",
      "Iteration: 4777, training loss: 0.3856737623560032\n",
      "Iteration: 4778, training loss: 0.3856687749413961\n",
      "Iteration: 4779, training loss: 0.3856637889635116\n",
      "Iteration: 4780, training loss: 0.38565880442161465\n",
      "Iteration: 4781, training loss: 0.38565382131497006\n",
      "Iteration: 4782, training loss: 0.38564883964284363\n",
      "Iteration: 4783, training loss: 0.3856438594045016\n",
      "Iteration: 4784, training loss: 0.38563888059921037\n",
      "Iteration: 4785, training loss: 0.38563390322623736\n",
      "Iteration: 4786, training loss: 0.3856289272848503\n",
      "Iteration: 4787, training loss: 0.38562395277431727\n",
      "Iteration: 4788, training loss: 0.3856189796939071\n",
      "Iteration: 4789, training loss: 0.385614008042889\n",
      "Iteration: 4790, training loss: 0.38560903782053285\n",
      "Iteration: 4791, training loss: 0.38560406902610883\n",
      "Iteration: 4792, training loss: 0.38559910165888783\n",
      "Iteration: 4793, training loss: 0.3855941357181411\n",
      "Iteration: 4794, training loss: 0.38558917120314057\n",
      "Iteration: 4795, training loss: 0.38558420811315847\n",
      "Iteration: 4796, training loss: 0.38557924644746766\n",
      "Iteration: 4797, training loss: 0.3855742862053416\n",
      "Iteration: 4798, training loss: 0.3855693273860542\n",
      "Iteration: 4799, training loss: 0.3855643699888798\n",
      "Iteration: 4800, training loss: 0.3855594140130932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4801, training loss: 0.38555445945797\n",
      "Iteration: 4802, training loss: 0.38554950632278595\n",
      "Iteration: 4803, training loss: 0.3855445546068177\n",
      "Iteration: 4804, training loss: 0.385539604309342\n",
      "Iteration: 4805, training loss: 0.3855346554296364\n",
      "Iteration: 4806, training loss: 0.38552970796697894\n",
      "Iteration: 4807, training loss: 0.3855247619206478\n",
      "Iteration: 4808, training loss: 0.38551981728992235\n",
      "Iteration: 4809, training loss: 0.3855148740740818\n",
      "Iteration: 4810, training loss: 0.38550993227240626\n",
      "Iteration: 4811, training loss: 0.38550499188417625\n",
      "Iteration: 4812, training loss: 0.3855000529086726\n",
      "Iteration: 4813, training loss: 0.38549511534517694\n",
      "Iteration: 4814, training loss: 0.3854901791929713\n",
      "Iteration: 4815, training loss: 0.38548524445133825\n",
      "Iteration: 4816, training loss: 0.3854803111195607\n",
      "Iteration: 4817, training loss: 0.3854753791969222\n",
      "Iteration: 4818, training loss: 0.38547044868270686\n",
      "Iteration: 4819, training loss: 0.38546551957619907\n",
      "Iteration: 4820, training loss: 0.385460591876684\n",
      "Iteration: 4821, training loss: 0.3854556655834471\n",
      "Iteration: 4822, training loss: 0.38545074069577445\n",
      "Iteration: 4823, training loss: 0.38544581721295257\n",
      "Iteration: 4824, training loss: 0.3854408951342685\n",
      "Iteration: 4825, training loss: 0.38543597445900973\n",
      "Iteration: 4826, training loss: 0.3854310551864644\n",
      "Iteration: 4827, training loss: 0.385426137315921\n",
      "Iteration: 4828, training loss: 0.3854212208466686\n",
      "Iteration: 4829, training loss: 0.3854163057779966\n",
      "Iteration: 4830, training loss: 0.38541139210919506\n",
      "Iteration: 4831, training loss: 0.3854064798395548\n",
      "Iteration: 4832, training loss: 0.3854015689683664\n",
      "Iteration: 4833, training loss: 0.3853966594949217\n",
      "Iteration: 4834, training loss: 0.3853917514185125\n",
      "Iteration: 4835, training loss: 0.3853868447384315\n",
      "Iteration: 4836, training loss: 0.38538193945397164\n",
      "Iteration: 4837, training loss: 0.3853770355644264\n",
      "Iteration: 4838, training loss: 0.38537213306908974\n",
      "Iteration: 4839, training loss: 0.38536723196725625\n",
      "Iteration: 4840, training loss: 0.3853623322582209\n",
      "Iteration: 4841, training loss: 0.3853574339412791\n",
      "Iteration: 4842, training loss: 0.3853525370157269\n",
      "Iteration: 4843, training loss: 0.3853476414808606\n",
      "Iteration: 4844, training loss: 0.3853427473359773\n",
      "Iteration: 4845, training loss: 0.38533785458037445\n",
      "Iteration: 4846, training loss: 0.38533296321335003\n",
      "Iteration: 4847, training loss: 0.38532807323420226\n",
      "Iteration: 4848, training loss: 0.3853231846422302\n",
      "Iteration: 4849, training loss: 0.38531829743673324\n",
      "Iteration: 4850, training loss: 0.3853134116170113\n",
      "Iteration: 4851, training loss: 0.3853085271823647\n",
      "Iteration: 4852, training loss: 0.3853036441320944\n",
      "Iteration: 4853, training loss: 0.3852987624655017\n",
      "Iteration: 4854, training loss: 0.38529388218188837\n",
      "Iteration: 4855, training loss: 0.38528900328055693\n",
      "Iteration: 4856, training loss: 0.38528412576080995\n",
      "Iteration: 4857, training loss: 0.3852792496219511\n",
      "Iteration: 4858, training loss: 0.38527437486328386\n",
      "Iteration: 4859, training loss: 0.38526950148411254\n",
      "Iteration: 4860, training loss: 0.385264629483742\n",
      "Iteration: 4861, training loss: 0.38525975886147756\n",
      "Iteration: 4862, training loss: 0.3852548896166247\n",
      "Iteration: 4863, training loss: 0.3852500217484898\n",
      "Iteration: 4864, training loss: 0.38524515525637965\n",
      "Iteration: 4865, training loss: 0.3852402901396012\n",
      "Iteration: 4866, training loss: 0.38523542639746233\n",
      "Iteration: 4867, training loss: 0.38523056402927097\n",
      "Iteration: 4868, training loss: 0.38522570303433595\n",
      "Iteration: 4869, training loss: 0.38522084341196633\n",
      "Iteration: 4870, training loss: 0.38521598516147154\n",
      "Iteration: 4871, training loss: 0.38521112828216186\n",
      "Iteration: 4872, training loss: 0.38520627277334774\n",
      "Iteration: 4873, training loss: 0.3852014186343402\n",
      "Iteration: 4874, training loss: 0.38519656586445067\n",
      "Iteration: 4875, training loss: 0.3851917144629914\n",
      "Iteration: 4876, training loss: 0.3851868644292746\n",
      "Iteration: 4877, training loss: 0.38518201576261324\n",
      "Iteration: 4878, training loss: 0.38517716846232086\n",
      "Iteration: 4879, training loss: 0.3851723225277113\n",
      "Iteration: 4880, training loss: 0.38516747795809886\n",
      "Iteration: 4881, training loss: 0.38516263475279855\n",
      "Iteration: 4882, training loss: 0.3851577929111254\n",
      "Iteration: 4883, training loss: 0.38515295243239556\n",
      "Iteration: 4884, training loss: 0.38514811331592513\n",
      "Iteration: 4885, training loss: 0.3851432755610309\n",
      "Iteration: 4886, training loss: 0.38513843916703\n",
      "Iteration: 4887, training loss: 0.3851336041332403\n",
      "Iteration: 4888, training loss: 0.38512877045897975\n",
      "Iteration: 4889, training loss: 0.38512393814356716\n",
      "Iteration: 4890, training loss: 0.38511910718632153\n",
      "Iteration: 4891, training loss: 0.38511427758656264\n",
      "Iteration: 4892, training loss: 0.3851094493436103\n",
      "Iteration: 4893, training loss: 0.38510462245678523\n",
      "Iteration: 4894, training loss: 0.3850997969254083\n",
      "Iteration: 4895, training loss: 0.38509497274880117\n",
      "Iteration: 4896, training loss: 0.38509014992628554\n",
      "Iteration: 4897, training loss: 0.38508532845718396\n",
      "Iteration: 4898, training loss: 0.3850805083408193\n",
      "Iteration: 4899, training loss: 0.38507568957651483\n",
      "Iteration: 4900, training loss: 0.3850708721635945\n",
      "Iteration: 4901, training loss: 0.38506605610138245\n",
      "Iteration: 4902, training loss: 0.3850612413892036\n",
      "Iteration: 4903, training loss: 0.38505642802638296\n",
      "Iteration: 4904, training loss: 0.38505161601224647\n",
      "Iteration: 4905, training loss: 0.38504680534612007\n",
      "Iteration: 4906, training loss: 0.3850419960273304\n",
      "Iteration: 4907, training loss: 0.38503718805520476\n",
      "Iteration: 4908, training loss: 0.38503238142907037\n",
      "Iteration: 4909, training loss: 0.3850275761482555\n",
      "Iteration: 4910, training loss: 0.3850227722120885\n",
      "Iteration: 4911, training loss: 0.3850179696198984\n",
      "Iteration: 4912, training loss: 0.38501316837101457\n",
      "Iteration: 4913, training loss: 0.3850083684647669\n",
      "Iteration: 4914, training loss: 0.38500356990048556\n",
      "Iteration: 4915, training loss: 0.38499877267750154\n",
      "Iteration: 4916, training loss: 0.384993976795146\n",
      "Iteration: 4917, training loss: 0.38498918225275075\n",
      "Iteration: 4918, training loss: 0.38498438904964793\n",
      "Iteration: 4919, training loss: 0.38497959718517\n",
      "Iteration: 4920, training loss: 0.38497480665865036\n",
      "Iteration: 4921, training loss: 0.3849700174694224\n",
      "Iteration: 4922, training loss: 0.3849652296168201\n",
      "Iteration: 4923, training loss: 0.3849604431001781\n",
      "Iteration: 4924, training loss: 0.3849556579188312\n",
      "Iteration: 4925, training loss: 0.3849508740721148\n",
      "Iteration: 4926, training loss: 0.3849460915593649\n",
      "Iteration: 4927, training loss: 0.38494131037991763\n",
      "Iteration: 4928, training loss: 0.38493653053310983\n",
      "Iteration: 4929, training loss: 0.3849317520182788\n",
      "Iteration: 4930, training loss: 0.38492697483476207\n",
      "Iteration: 4931, training loss: 0.38492219898189783\n",
      "Iteration: 4932, training loss: 0.3849174244590247\n",
      "Iteration: 4933, training loss: 0.3849126512654818\n",
      "Iteration: 4934, training loss: 0.3849078794006086\n",
      "Iteration: 4935, training loss: 0.38490310886374496\n",
      "Iteration: 4936, training loss: 0.38489833965423137\n",
      "Iteration: 4937, training loss: 0.38489357177140865\n",
      "Iteration: 4938, training loss: 0.3848888052146181\n",
      "Iteration: 4939, training loss: 0.3848840399832017\n",
      "Iteration: 4940, training loss: 0.38487927607650135\n",
      "Iteration: 4941, training loss: 0.38487451349386\n",
      "Iteration: 4942, training loss: 0.3848697522346207\n",
      "Iteration: 4943, training loss: 0.3848649922981269\n",
      "Iteration: 4944, training loss: 0.38486023368372274\n",
      "Iteration: 4945, training loss: 0.38485547639075274\n",
      "Iteration: 4946, training loss: 0.3848507204185618\n",
      "Iteration: 4947, training loss: 0.3848459657664952\n",
      "Iteration: 4948, training loss: 0.3848412124338989\n",
      "Iteration: 4949, training loss: 0.38483646042011915\n",
      "Iteration: 4950, training loss: 0.38483170972450265\n",
      "Iteration: 4951, training loss: 0.38482696034639663\n",
      "Iteration: 4952, training loss: 0.3848222122851487\n",
      "Iteration: 4953, training loss: 0.38481746554010676\n",
      "Iteration: 4954, training loss: 0.3848127201106196\n",
      "Iteration: 4955, training loss: 0.38480797599603606\n",
      "Iteration: 4956, training loss: 0.3848032331957054\n",
      "Iteration: 4957, training loss: 0.3847984917089779\n",
      "Iteration: 4958, training loss: 0.3847937515352034\n",
      "Iteration: 4959, training loss: 0.38478901267373294\n",
      "Iteration: 4960, training loss: 0.3847842751239176\n",
      "Iteration: 4961, training loss: 0.3847795388851091\n",
      "Iteration: 4962, training loss: 0.38477480395665936\n",
      "Iteration: 4963, training loss: 0.38477007033792104\n",
      "Iteration: 4964, training loss: 0.38476533802824714\n",
      "Iteration: 4965, training loss: 0.384760607026991\n",
      "Iteration: 4966, training loss: 0.38475587733350636\n",
      "Iteration: 4967, training loss: 0.38475114894714785\n",
      "Iteration: 4968, training loss: 0.3847464218672698\n",
      "Iteration: 4969, training loss: 0.3847416960932277\n",
      "Iteration: 4970, training loss: 0.38473697162437703\n",
      "Iteration: 4971, training loss: 0.38473224846007403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4972, training loss: 0.3847275265996749\n",
      "Iteration: 4973, training loss: 0.3847228060425368\n",
      "Iteration: 4974, training loss: 0.38471808678801706\n",
      "Iteration: 4975, training loss: 0.38471336883547347\n",
      "Iteration: 4976, training loss: 0.3847086521842643\n",
      "Iteration: 4977, training loss: 0.3847039368337483\n",
      "Iteration: 4978, training loss: 0.3846992227832846\n",
      "Iteration: 4979, training loss: 0.38469451003223276\n",
      "Iteration: 4980, training loss: 0.38468979857995284\n",
      "Iteration: 4981, training loss: 0.38468508842580507\n",
      "Iteration: 4982, training loss: 0.3846803795691506\n",
      "Iteration: 4983, training loss: 0.3846756720093506\n",
      "Iteration: 4984, training loss: 0.38467096574576687\n",
      "Iteration: 4985, training loss: 0.3846662607777616\n",
      "Iteration: 4986, training loss: 0.3846615571046975\n",
      "Iteration: 4987, training loss: 0.38465685472593747\n",
      "Iteration: 4988, training loss: 0.3846521536408451\n",
      "Iteration: 4989, training loss: 0.38464745384878446\n",
      "Iteration: 4990, training loss: 0.38464275534911957\n",
      "Iteration: 4991, training loss: 0.38463805814121554\n",
      "Iteration: 4992, training loss: 0.3846333622244375\n",
      "Iteration: 4993, training loss: 0.38462866759815106\n",
      "Iteration: 4994, training loss: 0.38462397426172235\n",
      "Iteration: 4995, training loss: 0.38461928221451785\n",
      "Iteration: 4996, training loss: 0.3846145914559047\n",
      "Iteration: 4997, training loss: 0.38460990198525\n",
      "Iteration: 4998, training loss: 0.38460521380192186\n",
      "Iteration: 4999, training loss: 0.38460052690528845\n",
      "Iteration: 5000, training loss: 0.38459584129471835\n",
      "Iteration: 5001, training loss: 0.38459115696958074\n",
      "Iteration: 5002, training loss: 0.3845864739292451\n",
      "Iteration: 5003, training loss: 0.3845817921730814\n",
      "Iteration: 5004, training loss: 0.3845771117004602\n",
      "Iteration: 5005, training loss: 0.3845724325107521\n",
      "Iteration: 5006, training loss: 0.3845677546033286\n",
      "Iteration: 5007, training loss: 0.3845630779775612\n",
      "Iteration: 5008, training loss: 0.3845584026328219\n",
      "Iteration: 5009, training loss: 0.38455372856848347\n",
      "Iteration: 5010, training loss: 0.38454905578391874\n",
      "Iteration: 5011, training loss: 0.38454438427850124\n",
      "Iteration: 5012, training loss: 0.38453971405160464\n",
      "Iteration: 5013, training loss: 0.38453504510260317\n",
      "Iteration: 5014, training loss: 0.3845303774308716\n",
      "Iteration: 5015, training loss: 0.38452571103578487\n",
      "Iteration: 5016, training loss: 0.3845210459167187\n",
      "Iteration: 5017, training loss: 0.3845163820730487\n",
      "Iteration: 5018, training loss: 0.38451171950415164\n",
      "Iteration: 5019, training loss: 0.38450705820940395\n",
      "Iteration: 5020, training loss: 0.38450239818818305\n",
      "Iteration: 5021, training loss: 0.3844977394398665\n",
      "Iteration: 5022, training loss: 0.3844930819638323\n",
      "Iteration: 5023, training loss: 0.38448842575945896\n",
      "Iteration: 5024, training loss: 0.38448377082612534\n",
      "Iteration: 5025, training loss: 0.38447911716321087\n",
      "Iteration: 5026, training loss: 0.38447446477009506\n",
      "Iteration: 5027, training loss: 0.3844698136461583\n",
      "Iteration: 5028, training loss: 0.38446516379078116\n",
      "Iteration: 5029, training loss: 0.38446051520334434\n",
      "Iteration: 5030, training loss: 0.3844558678832295\n",
      "Iteration: 5031, training loss: 0.38445122182981845\n",
      "Iteration: 5032, training loss: 0.38444657704249335\n",
      "Iteration: 5033, training loss: 0.38444193352063694\n",
      "Iteration: 5034, training loss: 0.38443729126363235\n",
      "Iteration: 5035, training loss: 0.384432650270863\n",
      "Iteration: 5036, training loss: 0.3844280105417128\n",
      "Iteration: 5037, training loss: 0.38442337207556604\n",
      "Iteration: 5038, training loss: 0.38441873487180767\n",
      "Iteration: 5039, training loss: 0.3844140989298227\n",
      "Iteration: 5040, training loss: 0.38440946424899675\n",
      "Iteration: 5041, training loss: 0.38440483082871585\n",
      "Iteration: 5042, training loss: 0.38440019866836633\n",
      "Iteration: 5043, training loss: 0.38439556776733513\n",
      "Iteration: 5044, training loss: 0.38439093812500935\n",
      "Iteration: 5045, training loss: 0.3843863097407768\n",
      "Iteration: 5046, training loss: 0.3843816826140255\n",
      "Iteration: 5047, training loss: 0.3843770567441439\n",
      "Iteration: 5048, training loss: 0.3843724321305209\n",
      "Iteration: 5049, training loss: 0.38436780877254584\n",
      "Iteration: 5050, training loss: 0.38436318666960845\n",
      "Iteration: 5051, training loss: 0.38435856582109884\n",
      "Iteration: 5052, training loss: 0.38435394622640756\n",
      "Iteration: 5053, training loss: 0.38434932788492554\n",
      "Iteration: 5054, training loss: 0.3843447107960442\n",
      "Iteration: 5055, training loss: 0.3843400949591553\n",
      "Iteration: 5056, training loss: 0.38433548037365095\n",
      "Iteration: 5057, training loss: 0.38433086703892394\n",
      "Iteration: 5058, training loss: 0.384326254954367\n",
      "Iteration: 5059, training loss: 0.38432164411937375\n",
      "Iteration: 5060, training loss: 0.38431703453333804\n",
      "Iteration: 5061, training loss: 0.384312426195654\n",
      "Iteration: 5062, training loss: 0.38430781910571626\n",
      "Iteration: 5063, training loss: 0.3843032132629199\n",
      "Iteration: 5064, training loss: 0.38429860866666044\n",
      "Iteration: 5065, training loss: 0.3842940053163337\n",
      "Iteration: 5066, training loss: 0.3842894032113358\n",
      "Iteration: 5067, training loss: 0.38428480235106366\n",
      "Iteration: 5068, training loss: 0.3842802027349143\n",
      "Iteration: 5069, training loss: 0.3842756043622851\n",
      "Iteration: 5070, training loss: 0.38427100723257396\n",
      "Iteration: 5071, training loss: 0.38426641134517936\n",
      "Iteration: 5072, training loss: 0.3842618166994998\n",
      "Iteration: 5073, training loss: 0.38425722329493456\n",
      "Iteration: 5074, training loss: 0.384252631130883\n",
      "Iteration: 5075, training loss: 0.384248040206745\n",
      "Iteration: 5076, training loss: 0.3842434505219211\n",
      "Iteration: 5077, training loss: 0.38423886207581187\n",
      "Iteration: 5078, training loss: 0.38423427486781847\n",
      "Iteration: 5079, training loss: 0.3842296888973424\n",
      "Iteration: 5080, training loss: 0.3842251041637856\n",
      "Iteration: 5081, training loss: 0.3842205206665504\n",
      "Iteration: 5082, training loss: 0.3842159384050396\n",
      "Iteration: 5083, training loss: 0.38421135737865614\n",
      "Iteration: 5084, training loss: 0.38420677758680366\n",
      "Iteration: 5085, training loss: 0.38420219902888625\n",
      "Iteration: 5086, training loss: 0.38419762170430805\n",
      "Iteration: 5087, training loss: 0.3841930456124739\n",
      "Iteration: 5088, training loss: 0.38418847075278884\n",
      "Iteration: 5089, training loss: 0.3841838971246584\n",
      "Iteration: 5090, training loss: 0.3841793247274886\n",
      "Iteration: 5091, training loss: 0.3841747535606857\n",
      "Iteration: 5092, training loss: 0.38417018362365646\n",
      "Iteration: 5093, training loss: 0.384165614915808\n",
      "Iteration: 5094, training loss: 0.3841610474365478\n",
      "Iteration: 5095, training loss: 0.3841564811852839\n",
      "Iteration: 5096, training loss: 0.3841519161614243\n",
      "Iteration: 5097, training loss: 0.3841473523643781\n",
      "Iteration: 5098, training loss: 0.3841427897935542\n",
      "Iteration: 5099, training loss: 0.3841382284483622\n",
      "Iteration: 5100, training loss: 0.38413366832821183\n",
      "Iteration: 5101, training loss: 0.3841291094325136\n",
      "Iteration: 5102, training loss: 0.384124551760678\n",
      "Iteration: 5103, training loss: 0.38411999531211627\n",
      "Iteration: 5104, training loss: 0.38411544008623977\n",
      "Iteration: 5105, training loss: 0.3841108860824604\n",
      "Iteration: 5106, training loss: 0.38410633330019045\n",
      "Iteration: 5107, training loss: 0.3841017817388426\n",
      "Iteration: 5108, training loss: 0.38409723139782986\n",
      "Iteration: 5109, training loss: 0.3840926822765656\n",
      "Iteration: 5110, training loss: 0.38408813437446376\n",
      "Iteration: 5111, training loss: 0.3840835876909385\n",
      "Iteration: 5112, training loss: 0.3840790422254045\n",
      "Iteration: 5113, training loss: 0.3840744979772767\n",
      "Iteration: 5114, training loss: 0.38406995494597057\n",
      "Iteration: 5115, training loss: 0.3840654131309018\n",
      "Iteration: 5116, training loss: 0.3840608725314867\n",
      "Iteration: 5117, training loss: 0.38405633314714177\n",
      "Iteration: 5118, training loss: 0.38405179497728387\n",
      "Iteration: 5119, training loss: 0.3840472580213305\n",
      "Iteration: 5120, training loss: 0.38404272227869934\n",
      "Iteration: 5121, training loss: 0.38403818774880844\n",
      "Iteration: 5122, training loss: 0.38403365443107634\n",
      "Iteration: 5123, training loss: 0.38402912232492203\n",
      "Iteration: 5124, training loss: 0.3840245914297648\n",
      "Iteration: 5125, training loss: 0.3840200617450242\n",
      "Iteration: 5126, training loss: 0.3840155332701204\n",
      "Iteration: 5127, training loss: 0.38401100600447363\n",
      "Iteration: 5128, training loss: 0.38400647994750503\n",
      "Iteration: 5129, training loss: 0.38400195509863577\n",
      "Iteration: 5130, training loss: 0.38399743145728726\n",
      "Iteration: 5131, training loss: 0.38399290902288163\n",
      "Iteration: 5132, training loss: 0.38398838779484123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5133, training loss: 0.3839838677725888\n",
      "Iteration: 5134, training loss: 0.38397934895554753\n",
      "Iteration: 5135, training loss: 0.3839748313431409\n",
      "Iteration: 5136, training loss: 0.38397031493479283\n",
      "Iteration: 5137, training loss: 0.38396579972992756\n",
      "Iteration: 5138, training loss: 0.38396128572797\n",
      "Iteration: 5139, training loss: 0.38395677292834496\n",
      "Iteration: 5140, training loss: 0.38395226133047794\n",
      "Iteration: 5141, training loss: 0.3839477509337949\n",
      "Iteration: 5142, training loss: 0.38394324173772176\n",
      "Iteration: 5143, training loss: 0.3839387337416855\n",
      "Iteration: 5144, training loss: 0.38393422694511276\n",
      "Iteration: 5145, training loss: 0.383929721347431\n",
      "Iteration: 5146, training loss: 0.38392521694806797\n",
      "Iteration: 5147, training loss: 0.3839207137464518\n",
      "Iteration: 5148, training loss: 0.383916211742011\n",
      "Iteration: 5149, training loss: 0.38391171093417437\n",
      "Iteration: 5150, training loss: 0.3839072113223712\n",
      "Iteration: 5151, training loss: 0.38390271290603106\n",
      "Iteration: 5152, training loss: 0.38389821568458404\n",
      "Iteration: 5153, training loss: 0.3838937196574604\n",
      "Iteration: 5154, training loss: 0.38388922482409116\n",
      "Iteration: 5155, training loss: 0.38388473118390715\n",
      "Iteration: 5156, training loss: 0.38388023873634014\n",
      "Iteration: 5157, training loss: 0.3838757474808218\n",
      "Iteration: 5158, training loss: 0.3838712574167846\n",
      "Iteration: 5159, training loss: 0.3838667685436611\n",
      "Iteration: 5160, training loss: 0.3838622808608844\n",
      "Iteration: 5161, training loss: 0.3838577943678878\n",
      "Iteration: 5162, training loss: 0.38385330906410503\n",
      "Iteration: 5163, training loss: 0.38384882494897044\n",
      "Iteration: 5164, training loss: 0.3838443420219184\n",
      "Iteration: 5165, training loss: 0.38383986028238387\n",
      "Iteration: 5166, training loss: 0.3838353797298021\n",
      "Iteration: 5167, training loss: 0.38383090036360873\n",
      "Iteration: 5168, training loss: 0.3838264221832399\n",
      "Iteration: 5169, training loss: 0.38382194518813184\n",
      "Iteration: 5170, training loss: 0.38381746937772143\n",
      "Iteration: 5171, training loss: 0.3838129947514457\n",
      "Iteration: 5172, training loss: 0.38380852130874227\n",
      "Iteration: 5173, training loss: 0.383804049049049\n",
      "Iteration: 5174, training loss: 0.3837995779718042\n",
      "Iteration: 5175, training loss: 0.3837951080764464\n",
      "Iteration: 5176, training loss: 0.38379063936241464\n",
      "Iteration: 5177, training loss: 0.38378617182914826\n",
      "Iteration: 5178, training loss: 0.3837817054760872\n",
      "Iteration: 5179, training loss: 0.3837772403026713\n",
      "Iteration: 5180, training loss: 0.38377277630834117\n",
      "Iteration: 5181, training loss: 0.38376831349253765\n",
      "Iteration: 5182, training loss: 0.3837638518547019\n",
      "Iteration: 5183, training loss: 0.38375939139427573\n",
      "Iteration: 5184, training loss: 0.38375493211070083\n",
      "Iteration: 5185, training loss: 0.3837504740034197\n",
      "Iteration: 5186, training loss: 0.383746017071875\n",
      "Iteration: 5187, training loss: 0.38374156131550974\n",
      "Iteration: 5188, training loss: 0.3837371067337674\n",
      "Iteration: 5189, training loss: 0.38373265332609174\n",
      "Iteration: 5190, training loss: 0.38372820109192707\n",
      "Iteration: 5191, training loss: 0.38372375003071774\n",
      "Iteration: 5192, training loss: 0.3837193001419087\n",
      "Iteration: 5193, training loss: 0.38371485142494527\n",
      "Iteration: 5194, training loss: 0.38371040387927297\n",
      "Iteration: 5195, training loss: 0.38370595750433795\n",
      "Iteration: 5196, training loss: 0.38370151229958643\n",
      "Iteration: 5197, training loss: 0.3836970682644653\n",
      "Iteration: 5198, training loss: 0.3836926253984215\n",
      "Iteration: 5199, training loss: 0.38368818370090263\n",
      "Iteration: 5200, training loss: 0.38368374317135634\n",
      "Iteration: 5201, training loss: 0.3836793038092309\n",
      "Iteration: 5202, training loss: 0.3836748656139749\n",
      "Iteration: 5203, training loss: 0.3836704285850372\n",
      "Iteration: 5204, training loss: 0.383665992721867\n",
      "Iteration: 5205, training loss: 0.38366155802391405\n",
      "Iteration: 5206, training loss: 0.38365712449062833\n",
      "Iteration: 5207, training loss: 0.38365269212146014\n",
      "Iteration: 5208, training loss: 0.3836482609158603\n",
      "Iteration: 5209, training loss: 0.3836438308732797\n",
      "Iteration: 5210, training loss: 0.38363940199317004\n",
      "Iteration: 5211, training loss: 0.38363497427498294\n",
      "Iteration: 5212, training loss: 0.3836305477181706\n",
      "Iteration: 5213, training loss: 0.3836261223221857\n",
      "Iteration: 5214, training loss: 0.3836216980864809\n",
      "Iteration: 5215, training loss: 0.3836172750105095\n",
      "Iteration: 5216, training loss: 0.3836128530937252\n",
      "Iteration: 5217, training loss: 0.38360843233558184\n",
      "Iteration: 5218, training loss: 0.3836040127355339\n",
      "Iteration: 5219, training loss: 0.38359959429303586\n",
      "Iteration: 5220, training loss: 0.3835951770075429\n",
      "Iteration: 5221, training loss: 0.3835907608785104\n",
      "Iteration: 5222, training loss: 0.3835863459053941\n",
      "Iteration: 5223, training loss: 0.38358193208765007\n",
      "Iteration: 5224, training loss: 0.3835775194247347\n",
      "Iteration: 5225, training loss: 0.38357310791610505\n",
      "Iteration: 5226, training loss: 0.38356869756121814\n",
      "Iteration: 5227, training loss: 0.38356428835953144\n",
      "Iteration: 5228, training loss: 0.38355988031050303\n",
      "Iteration: 5229, training loss: 0.38355547341359103\n",
      "Iteration: 5230, training loss: 0.38355106766825403\n",
      "Iteration: 5231, training loss: 0.3835466630739511\n",
      "Iteration: 5232, training loss: 0.3835422596301414\n",
      "Iteration: 5233, training loss: 0.3835378573362847\n",
      "Iteration: 5234, training loss: 0.38353345619184104\n",
      "Iteration: 5235, training loss: 0.38352905619627076\n",
      "Iteration: 5236, training loss: 0.3835246573490344\n",
      "Iteration: 5237, training loss: 0.3835202596495934\n",
      "Iteration: 5238, training loss: 0.38351586309740887\n",
      "Iteration: 5239, training loss: 0.38351146769194283\n",
      "Iteration: 5240, training loss: 0.3835070734326573\n",
      "Iteration: 5241, training loss: 0.38350268031901474\n",
      "Iteration: 5242, training loss: 0.38349828835047806\n",
      "Iteration: 5243, training loss: 0.3834938975265104\n",
      "Iteration: 5244, training loss: 0.3834895078465754\n",
      "Iteration: 5245, training loss: 0.38348511931013685\n",
      "Iteration: 5246, training loss: 0.383480731916659\n",
      "Iteration: 5247, training loss: 0.38347634566560657\n",
      "Iteration: 5248, training loss: 0.3834719605564444\n",
      "Iteration: 5249, training loss: 0.3834675765886377\n",
      "Iteration: 5250, training loss: 0.3834631937616524\n",
      "Iteration: 5251, training loss: 0.38345881207495414\n",
      "Iteration: 5252, training loss: 0.38345443152800945\n",
      "Iteration: 5253, training loss: 0.38345005212028505\n",
      "Iteration: 5254, training loss: 0.3834456738512479\n",
      "Iteration: 5255, training loss: 0.3834412967203654\n",
      "Iteration: 5256, training loss: 0.3834369207271054\n",
      "Iteration: 5257, training loss: 0.3834325458709358\n",
      "Iteration: 5258, training loss: 0.3834281721513252\n",
      "Iteration: 5259, training loss: 0.3834237995677422\n",
      "Iteration: 5260, training loss: 0.38341942811965607\n",
      "Iteration: 5261, training loss: 0.3834150578065362\n",
      "Iteration: 5262, training loss: 0.38341068862785244\n",
      "Iteration: 5263, training loss: 0.38340632058307494\n",
      "Iteration: 5264, training loss: 0.38340195367167423\n",
      "Iteration: 5265, training loss: 0.3833975878931212\n",
      "Iteration: 5266, training loss: 0.3833932232468869\n",
      "Iteration: 5267, training loss: 0.383388859732443\n",
      "Iteration: 5268, training loss: 0.3833844973492613\n",
      "Iteration: 5269, training loss: 0.38338013609681415\n",
      "Iteration: 5270, training loss: 0.38337577597457395\n",
      "Iteration: 5271, training loss: 0.3833714169820138\n",
      "Iteration: 5272, training loss: 0.38336705911860686\n",
      "Iteration: 5273, training loss: 0.38336270238382675\n",
      "Iteration: 5274, training loss: 0.3833583467771474\n",
      "Iteration: 5275, training loss: 0.38335399229804323\n",
      "Iteration: 5276, training loss: 0.3833496389459887\n",
      "Iteration: 5277, training loss: 0.3833452867204588\n",
      "Iteration: 5278, training loss: 0.38334093562092886\n",
      "Iteration: 5279, training loss: 0.38333658564687456\n",
      "Iteration: 5280, training loss: 0.383332236797772\n",
      "Iteration: 5281, training loss: 0.3833278890730974\n",
      "Iteration: 5282, training loss: 0.3833235424723274\n",
      "Iteration: 5283, training loss: 0.38331919699493905\n",
      "Iteration: 5284, training loss: 0.3833148526404098\n",
      "Iteration: 5285, training loss: 0.38331050940821726\n",
      "Iteration: 5286, training loss: 0.38330616729783956\n",
      "Iteration: 5287, training loss: 0.3833018263087549\n",
      "Iteration: 5288, training loss: 0.3832974864404421\n",
      "Iteration: 5289, training loss: 0.38329314769238026\n",
      "Iteration: 5290, training loss: 0.3832888100640487\n",
      "Iteration: 5291, training loss: 0.3832844735549272\n",
      "Iteration: 5292, training loss: 0.38328013816449574\n",
      "Iteration: 5293, training loss: 0.38327580389223487\n",
      "Iteration: 5294, training loss: 0.38327147073762513\n",
      "Iteration: 5295, training loss: 0.3832671387001478\n",
      "Iteration: 5296, training loss: 0.3832628077792842\n",
      "Iteration: 5297, training loss: 0.38325847797451607\n",
      "Iteration: 5298, training loss: 0.3832541492853256\n",
      "Iteration: 5299, training loss: 0.38324982171119504\n",
      "Iteration: 5300, training loss: 0.3832454952516072\n",
      "Iteration: 5301, training loss: 0.38324116990604534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5302, training loss: 0.3832368456739928\n",
      "Iteration: 5303, training loss: 0.38323252255493334\n",
      "Iteration: 5304, training loss: 0.38322820054835094\n",
      "Iteration: 5305, training loss: 0.3832238796537301\n",
      "Iteration: 5306, training loss: 0.3832195598705557\n",
      "Iteration: 5307, training loss: 0.3832152411983127\n",
      "Iteration: 5308, training loss: 0.3832109236364866\n",
      "Iteration: 5309, training loss: 0.38320660718456323\n",
      "Iteration: 5310, training loss: 0.38320229184202864\n",
      "Iteration: 5311, training loss: 0.38319797760836916\n",
      "Iteration: 5312, training loss: 0.3831936644830717\n",
      "Iteration: 5313, training loss: 0.3831893524656233\n",
      "Iteration: 5314, training loss: 0.3831850415555113\n",
      "Iteration: 5315, training loss: 0.3831807317522236\n",
      "Iteration: 5316, training loss: 0.38317642305524835\n",
      "Iteration: 5317, training loss: 0.38317211546407376\n",
      "Iteration: 5318, training loss: 0.38316780897818875\n",
      "Iteration: 5319, training loss: 0.3831635035970823\n",
      "Iteration: 5320, training loss: 0.3831591993202439\n",
      "Iteration: 5321, training loss: 0.3831548961471632\n",
      "Iteration: 5322, training loss: 0.3831505940773304\n",
      "Iteration: 5323, training loss: 0.3831462931102359\n",
      "Iteration: 5324, training loss: 0.3831419932453703\n",
      "Iteration: 5325, training loss: 0.3831376944822247\n",
      "Iteration: 5326, training loss: 0.38313339682029063\n",
      "Iteration: 5327, training loss: 0.3831291002590597\n",
      "Iteration: 5328, training loss: 0.38312480479802397\n",
      "Iteration: 5329, training loss: 0.3831205104366757\n",
      "Iteration: 5330, training loss: 0.3831162171745078\n",
      "Iteration: 5331, training loss: 0.38311192501101315\n",
      "Iteration: 5332, training loss: 0.3831076339456852\n",
      "Iteration: 5333, training loss: 0.38310334397801765\n",
      "Iteration: 5334, training loss: 0.38309905510750436\n",
      "Iteration: 5335, training loss: 0.3830947673336398\n",
      "Iteration: 5336, training loss: 0.38309048065591855\n",
      "Iteration: 5337, training loss: 0.38308619507383573\n",
      "Iteration: 5338, training loss: 0.3830819105868865\n",
      "Iteration: 5339, training loss: 0.3830776271945666\n",
      "Iteration: 5340, training loss: 0.38307334489637196\n",
      "Iteration: 5341, training loss: 0.3830690636917989\n",
      "Iteration: 5342, training loss: 0.3830647835803439\n",
      "Iteration: 5343, training loss: 0.38306050456150403\n",
      "Iteration: 5344, training loss: 0.38305622663477656\n",
      "Iteration: 5345, training loss: 0.38305194979965906\n",
      "Iteration: 5346, training loss: 0.3830476740556493\n",
      "Iteration: 5347, training loss: 0.3830433994022457\n",
      "Iteration: 5348, training loss: 0.38303912583894667\n",
      "Iteration: 5349, training loss: 0.3830348533652512\n",
      "Iteration: 5350, training loss: 0.38303058198065854\n",
      "Iteration: 5351, training loss: 0.383026311684668\n",
      "Iteration: 5352, training loss: 0.38302204247677957\n",
      "Iteration: 5353, training loss: 0.3830177743564933\n",
      "Iteration: 5354, training loss: 0.3830135073233099\n",
      "Iteration: 5355, training loss: 0.38300924137673\n",
      "Iteration: 5356, training loss: 0.38300497651625476\n",
      "Iteration: 5357, training loss: 0.38300071274138564\n",
      "Iteration: 5358, training loss: 0.38299645005162436\n",
      "Iteration: 5359, training loss: 0.382992188446473\n",
      "Iteration: 5360, training loss: 0.38298792792543407\n",
      "Iteration: 5361, training loss: 0.3829836684880103\n",
      "Iteration: 5362, training loss: 0.3829794101337047\n",
      "Iteration: 5363, training loss: 0.3829751528620205\n",
      "Iteration: 5364, training loss: 0.3829708966724616\n",
      "Iteration: 5365, training loss: 0.38296664156453186\n",
      "Iteration: 5366, training loss: 0.38296238753773565\n",
      "Iteration: 5367, training loss: 0.3829581345915777\n",
      "Iteration: 5368, training loss: 0.38295388272556286\n",
      "Iteration: 5369, training loss: 0.3829496319391963\n",
      "Iteration: 5370, training loss: 0.3829453822319839\n",
      "Iteration: 5371, training loss: 0.38294113360343135\n",
      "Iteration: 5372, training loss: 0.382936886053045\n",
      "Iteration: 5373, training loss: 0.3829326395803312\n",
      "Iteration: 5374, training loss: 0.382928394184797\n",
      "Iteration: 5375, training loss: 0.3829241498659496\n",
      "Iteration: 5376, training loss: 0.3829199066232963\n",
      "Iteration: 5377, training loss: 0.38291566445634506\n",
      "Iteration: 5378, training loss: 0.3829114233646039\n",
      "Iteration: 5379, training loss: 0.38290718334758134\n",
      "Iteration: 5380, training loss: 0.38290294440478617\n",
      "Iteration: 5381, training loss: 0.3828987065357273\n",
      "Iteration: 5382, training loss: 0.38289446973991426\n",
      "Iteration: 5383, training loss: 0.3828902340168567\n",
      "Iteration: 5384, training loss: 0.3828859993660645\n",
      "Iteration: 5385, training loss: 0.3828817657870482\n",
      "Iteration: 5386, training loss: 0.3828775332793183\n",
      "Iteration: 5387, training loss: 0.38287330184238577\n",
      "Iteration: 5388, training loss: 0.3828690714757619\n",
      "Iteration: 5389, training loss: 0.3828648421789582\n",
      "Iteration: 5390, training loss: 0.38286061395148663\n",
      "Iteration: 5391, training loss: 0.3828563867928594\n",
      "Iteration: 5392, training loss: 0.38285216070258893\n",
      "Iteration: 5393, training loss: 0.382847935680188\n",
      "Iteration: 5394, training loss: 0.3828437117251699\n",
      "Iteration: 5395, training loss: 0.382839488837048\n",
      "Iteration: 5396, training loss: 0.38283526701533616\n",
      "Iteration: 5397, training loss: 0.38283104625954817\n",
      "Iteration: 5398, training loss: 0.38282682656919864\n",
      "Iteration: 5399, training loss: 0.38282260794380213\n",
      "Iteration: 5400, training loss: 0.3828183903828738\n",
      "Iteration: 5401, training loss: 0.38281417388592875\n",
      "Iteration: 5402, training loss: 0.3828099584524828\n",
      "Iteration: 5403, training loss: 0.38280574408205165\n",
      "Iteration: 5404, training loss: 0.38280153077415163\n",
      "Iteration: 5405, training loss: 0.38279731852829935\n",
      "Iteration: 5406, training loss: 0.3827931073440116\n",
      "Iteration: 5407, training loss: 0.3827888972208055\n",
      "Iteration: 5408, training loss: 0.3827846881581986\n",
      "Iteration: 5409, training loss: 0.38278048015570865\n",
      "Iteration: 5410, training loss: 0.3827762732128537\n",
      "Iteration: 5411, training loss: 0.382772067329152\n",
      "Iteration: 5412, training loss: 0.3827678625041225\n",
      "Iteration: 5413, training loss: 0.38276365873728396\n",
      "Iteration: 5414, training loss: 0.382759456028156\n",
      "Iteration: 5415, training loss: 0.3827552543762578\n",
      "Iteration: 5416, training loss: 0.3827510537811096\n",
      "Iteration: 5417, training loss: 0.3827468542422316\n",
      "Iteration: 5418, training loss: 0.3827426557591441\n",
      "Iteration: 5419, training loss: 0.3827384583313683\n",
      "Iteration: 5420, training loss: 0.3827342619584251\n",
      "Iteration: 5421, training loss: 0.3827300666398359\n",
      "Iteration: 5422, training loss: 0.38272587237512257\n",
      "Iteration: 5423, training loss: 0.3827216791638071\n",
      "Iteration: 5424, training loss: 0.3827174870054119\n",
      "Iteration: 5425, training loss: 0.38271329589945957\n",
      "Iteration: 5426, training loss: 0.3827091058454731\n",
      "Iteration: 5427, training loss: 0.3827049168429758\n",
      "Iteration: 5428, training loss: 0.38270072889149115\n",
      "Iteration: 5429, training loss: 0.38269654199054304\n",
      "Iteration: 5430, training loss: 0.38269235613965574\n",
      "Iteration: 5431, training loss: 0.3826881713383536\n",
      "Iteration: 5432, training loss: 0.3826839875861615\n",
      "Iteration: 5433, training loss: 0.38267980488260445\n",
      "Iteration: 5434, training loss: 0.38267562322720794\n",
      "Iteration: 5435, training loss: 0.3826714426194975\n",
      "Iteration: 5436, training loss: 0.38266726305899923\n",
      "Iteration: 5437, training loss: 0.38266308454523934\n",
      "Iteration: 5438, training loss: 0.3826589070777444\n",
      "Iteration: 5439, training loss: 0.38265473065604144\n",
      "Iteration: 5440, training loss: 0.3826505552796575\n",
      "Iteration: 5441, training loss: 0.3826463809481202\n",
      "Iteration: 5442, training loss: 0.3826422076609572\n",
      "Iteration: 5443, training loss: 0.38263803541769664\n",
      "Iteration: 5444, training loss: 0.38263386421786694\n",
      "Iteration: 5445, training loss: 0.3826296940609966\n",
      "Iteration: 5446, training loss: 0.3826255249466149\n",
      "Iteration: 5447, training loss: 0.3826213568742509\n",
      "Iteration: 5448, training loss: 0.3826171898434343\n",
      "Iteration: 5449, training loss: 0.3826130238536949\n",
      "Iteration: 5450, training loss: 0.38260885890456287\n",
      "Iteration: 5451, training loss: 0.38260469499556865\n",
      "Iteration: 5452, training loss: 0.3826005321262432\n",
      "Iteration: 5453, training loss: 0.38259637029611737\n",
      "Iteration: 5454, training loss: 0.3825922095047227\n",
      "Iteration: 5455, training loss: 0.3825880497515907\n",
      "Iteration: 5456, training loss: 0.38258389103625345\n",
      "Iteration: 5457, training loss: 0.3825797333582431\n",
      "Iteration: 5458, training loss: 0.3825755767170923\n",
      "Iteration: 5459, training loss: 0.38257142111233366\n",
      "Iteration: 5460, training loss: 0.38256726654350076\n",
      "Iteration: 5461, training loss: 0.3825631130101266\n",
      "Iteration: 5462, training loss: 0.38255896051174504\n",
      "Iteration: 5463, training loss: 0.38255480904789035\n",
      "Iteration: 5464, training loss: 0.38255065861809645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5465, training loss: 0.38254650922189826\n",
      "Iteration: 5466, training loss: 0.3825423608588306\n",
      "Iteration: 5467, training loss: 0.3825382135284286\n",
      "Iteration: 5468, training loss: 0.38253406723022776\n",
      "Iteration: 5469, training loss: 0.38252992196376395\n",
      "Iteration: 5470, training loss: 0.3825257777285733\n",
      "Iteration: 5471, training loss: 0.382521634524192\n",
      "Iteration: 5472, training loss: 0.3825174923501569\n",
      "Iteration: 5473, training loss: 0.38251335120600494\n",
      "Iteration: 5474, training loss: 0.3825092110912731\n",
      "Iteration: 5475, training loss: 0.38250507200549927\n",
      "Iteration: 5476, training loss: 0.38250093394822116\n",
      "Iteration: 5477, training loss: 0.3824967969189769\n",
      "Iteration: 5478, training loss: 0.3824926609173048\n",
      "Iteration: 5479, training loss: 0.3824885259427438\n",
      "Iteration: 5480, training loss: 0.3824843919948327\n",
      "Iteration: 5481, training loss: 0.38248025907311095\n",
      "Iteration: 5482, training loss: 0.38247612717711793\n",
      "Iteration: 5483, training loss: 0.38247199630639367\n",
      "Iteration: 5484, training loss: 0.3824678664604782\n",
      "Iteration: 5485, training loss: 0.3824637376389122\n",
      "Iteration: 5486, training loss: 0.3824596098412362\n",
      "Iteration: 5487, training loss: 0.38245548306699134\n",
      "Iteration: 5488, training loss: 0.38245135731571883\n",
      "Iteration: 5489, training loss: 0.3824472325869604\n",
      "Iteration: 5490, training loss: 0.38244310888025795\n",
      "Iteration: 5491, training loss: 0.3824389861951535\n",
      "Iteration: 5492, training loss: 0.3824348645311897\n",
      "Iteration: 5493, training loss: 0.38243074388790915\n",
      "Iteration: 5494, training loss: 0.382426624264855\n",
      "Iteration: 5495, training loss: 0.38242250566157066\n",
      "Iteration: 5496, training loss: 0.38241838807759965\n",
      "Iteration: 5497, training loss: 0.3824142715124859\n",
      "Iteration: 5498, training loss: 0.3824101559657736\n",
      "Iteration: 5499, training loss: 0.38240604143700724\n",
      "Iteration: 5500, training loss: 0.3824019279257316\n",
      "Iteration: 5501, training loss: 0.3823978154314917\n",
      "Iteration: 5502, training loss: 0.38239370395383304\n",
      "Iteration: 5503, training loss: 0.382389593492301\n",
      "Iteration: 5504, training loss: 0.3823854840464416\n",
      "Iteration: 5505, training loss: 0.382381375615801\n",
      "Iteration: 5506, training loss: 0.38237726819992585\n",
      "Iteration: 5507, training loss: 0.3823731617983627\n",
      "Iteration: 5508, training loss: 0.38236905641065877\n",
      "Iteration: 5509, training loss: 0.38236495203636134\n",
      "Iteration: 5510, training loss: 0.382360848675018\n",
      "Iteration: 5511, training loss: 0.38235674632617667\n",
      "Iteration: 5512, training loss: 0.38235264498938554\n",
      "Iteration: 5513, training loss: 0.38234854466419305\n",
      "Iteration: 5514, training loss: 0.3823444453501481\n",
      "Iteration: 5515, training loss: 0.3823403470467995\n",
      "Iteration: 5516, training loss: 0.38233624975369673\n",
      "Iteration: 5517, training loss: 0.3823321534703894\n",
      "Iteration: 5518, training loss: 0.3823280581964273\n",
      "Iteration: 5519, training loss: 0.38232396393136064\n",
      "Iteration: 5520, training loss: 0.38231987067474\n",
      "Iteration: 5521, training loss: 0.3823157784261159\n",
      "Iteration: 5522, training loss: 0.3823116871850394\n",
      "Iteration: 5523, training loss: 0.3823075969510618\n",
      "Iteration: 5524, training loss: 0.3823035077237349\n",
      "Iteration: 5525, training loss: 0.38229941950261015\n",
      "Iteration: 5526, training loss: 0.38229533228724005\n",
      "Iteration: 5527, training loss: 0.38229124607717685\n",
      "Iteration: 5528, training loss: 0.3822871608719733\n",
      "Iteration: 5529, training loss: 0.3822830766711822\n",
      "Iteration: 5530, training loss: 0.38227899347435707\n",
      "Iteration: 5531, training loss: 0.38227491128105123\n",
      "Iteration: 5532, training loss: 0.3822708300908187\n",
      "Iteration: 5533, training loss: 0.3822667499032135\n",
      "Iteration: 5534, training loss: 0.38226267071778985\n",
      "Iteration: 5535, training loss: 0.3822585925341026\n",
      "Iteration: 5536, training loss: 0.38225451535170657\n",
      "Iteration: 5537, training loss: 0.382250439170157\n",
      "Iteration: 5538, training loss: 0.38224636398900946\n",
      "Iteration: 5539, training loss: 0.3822422898078196\n",
      "Iteration: 5540, training loss: 0.3822382166261434\n",
      "Iteration: 5541, training loss: 0.38223414444353737\n",
      "Iteration: 5542, training loss: 0.382230073259558\n",
      "Iteration: 5543, training loss: 0.3822260030737622\n",
      "Iteration: 5544, training loss: 0.3822219338857071\n",
      "Iteration: 5545, training loss: 0.38221786569495003\n",
      "Iteration: 5546, training loss: 0.38221379850104886\n",
      "Iteration: 5547, training loss: 0.3822097323035614\n",
      "Iteration: 5548, training loss: 0.38220566710204606\n",
      "Iteration: 5549, training loss: 0.38220160289606125\n",
      "Iteration: 5550, training loss: 0.38219753968516595\n",
      "Iteration: 5551, training loss: 0.3821934774689191\n",
      "Iteration: 5552, training loss: 0.3821894162468801\n",
      "Iteration: 5553, training loss: 0.3821853560186085\n",
      "Iteration: 5554, training loss: 0.3821812967836642\n",
      "Iteration: 5555, training loss: 0.3821772385416075\n",
      "Iteration: 5556, training loss: 0.3821731812919988\n",
      "Iteration: 5557, training loss: 0.38216912503439887\n",
      "Iteration: 5558, training loss: 0.38216506976836856\n",
      "Iteration: 5559, training loss: 0.3821610154934694\n",
      "Iteration: 5560, training loss: 0.38215696220926265\n",
      "Iteration: 5561, training loss: 0.3821529099153103\n",
      "Iteration: 5562, training loss: 0.3821488586111744\n",
      "Iteration: 5563, training loss: 0.38214480829641745\n",
      "Iteration: 5564, training loss: 0.38214075897060185\n",
      "Iteration: 5565, training loss: 0.38213671063329063\n",
      "Iteration: 5566, training loss: 0.382132663284047\n",
      "Iteration: 5567, training loss: 0.3821286169224344\n",
      "Iteration: 5568, training loss: 0.38212457154801655\n",
      "Iteration: 5569, training loss: 0.3821205271603574\n",
      "Iteration: 5570, training loss: 0.38211648375902124\n",
      "Iteration: 5571, training loss: 0.38211244134357264\n",
      "Iteration: 5572, training loss: 0.3821083999135764\n",
      "Iteration: 5573, training loss: 0.3821043594685976\n",
      "Iteration: 5574, training loss: 0.38210032000820165\n",
      "Iteration: 5575, training loss: 0.3820962815319541\n",
      "Iteration: 5576, training loss: 0.38209224403942077\n",
      "Iteration: 5577, training loss: 0.3820882075301679\n",
      "Iteration: 5578, training loss: 0.382084172003762\n",
      "Iteration: 5579, training loss: 0.38208013745976965\n",
      "Iteration: 5580, training loss: 0.3820761038977579\n",
      "Iteration: 5581, training loss: 0.3820720713172939\n",
      "Iteration: 5582, training loss: 0.38206803971794523\n",
      "Iteration: 5583, training loss: 0.3820640090992796\n",
      "Iteration: 5584, training loss: 0.3820599794608652\n",
      "Iteration: 5585, training loss: 0.38205595080227017\n",
      "Iteration: 5586, training loss: 0.3820519231230632\n",
      "Iteration: 5587, training loss: 0.38204789642281295\n",
      "Iteration: 5588, training loss: 0.3820438707010888\n",
      "Iteration: 5589, training loss: 0.38203984595746\n",
      "Iteration: 5590, training loss: 0.3820358221914962\n",
      "Iteration: 5591, training loss: 0.38203179940276727\n",
      "Iteration: 5592, training loss: 0.38202777759084355\n",
      "Iteration: 5593, training loss: 0.38202375675529526\n",
      "Iteration: 5594, training loss: 0.3820197368956933\n",
      "Iteration: 5595, training loss: 0.3820157180116085\n",
      "Iteration: 5596, training loss: 0.38201170010261226\n",
      "Iteration: 5597, training loss: 0.382007683168276\n",
      "Iteration: 5598, training loss: 0.3820036672081715\n",
      "Iteration: 5599, training loss: 0.38199965222187093\n",
      "Iteration: 5600, training loss: 0.3819956382089464\n",
      "Iteration: 5601, training loss: 0.38199162516897056\n",
      "Iteration: 5602, training loss: 0.3819876131015163\n",
      "Iteration: 5603, training loss: 0.3819836020061566\n",
      "Iteration: 5604, training loss: 0.38197959188246494\n",
      "Iteration: 5605, training loss: 0.381975582730015\n",
      "Iteration: 5606, training loss: 0.38197157454838043\n",
      "Iteration: 5607, training loss: 0.3819675673371355\n",
      "Iteration: 5608, training loss: 0.3819635610958549\n",
      "Iteration: 5609, training loss: 0.3819595558241129\n",
      "Iteration: 5610, training loss: 0.3819555515214846\n",
      "Iteration: 5611, training loss: 0.38195154818754534\n",
      "Iteration: 5612, training loss: 0.3819475458218704\n",
      "Iteration: 5613, training loss: 0.3819435444240356\n",
      "Iteration: 5614, training loss: 0.38193954399361696\n",
      "Iteration: 5615, training loss: 0.38193554453019074\n",
      "Iteration: 5616, training loss: 0.38193154603333335\n",
      "Iteration: 5617, training loss: 0.3819275485026217\n",
      "Iteration: 5618, training loss: 0.3819235519376327\n",
      "Iteration: 5619, training loss: 0.38191955633794383\n",
      "Iteration: 5620, training loss: 0.38191556170313246\n",
      "Iteration: 5621, training loss: 0.38191156803277665\n",
      "Iteration: 5622, training loss: 0.38190757532645425\n",
      "Iteration: 5623, training loss: 0.38190358358374366\n",
      "Iteration: 5624, training loss: 0.3818995928042235\n",
      "Iteration: 5625, training loss: 0.38189560298747277\n",
      "Iteration: 5626, training loss: 0.3818916141330703\n",
      "Iteration: 5627, training loss: 0.38188762624059586\n",
      "Iteration: 5628, training loss: 0.38188363930962876\n",
      "Iteration: 5629, training loss: 0.38187965333974905\n",
      "Iteration: 5630, training loss: 0.3818756683305368\n",
      "Iteration: 5631, training loss: 0.38187168428157264\n",
      "Iteration: 5632, training loss: 0.38186770119243707\n",
      "Iteration: 5633, training loss: 0.38186371906271105\n",
      "Iteration: 5634, training loss: 0.3818597378919758\n",
      "Iteration: 5635, training loss: 0.3818557576798127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5636, training loss: 0.38185177842580365\n",
      "Iteration: 5637, training loss: 0.3818478001295304\n",
      "Iteration: 5638, training loss: 0.3818438227905753\n",
      "Iteration: 5639, training loss: 0.3818398464085208\n",
      "Iteration: 5640, training loss: 0.3818358709829497\n",
      "Iteration: 5641, training loss: 0.3818318965134449\n",
      "Iteration: 5642, training loss: 0.38182792299958956\n",
      "Iteration: 5643, training loss: 0.38182395044096745\n",
      "Iteration: 5644, training loss: 0.3818199788371623\n",
      "Iteration: 5645, training loss: 0.38181600818775785\n",
      "Iteration: 5646, training loss: 0.38181203849233863\n",
      "Iteration: 5647, training loss: 0.3818080697504891\n",
      "Iteration: 5648, training loss: 0.3818041019617941\n",
      "Iteration: 5649, training loss: 0.38180013512583855\n",
      "Iteration: 5650, training loss: 0.381796169242208\n",
      "Iteration: 5651, training loss: 0.38179220431048766\n",
      "Iteration: 5652, training loss: 0.38178824033026354\n",
      "Iteration: 5653, training loss: 0.3817842773011217\n",
      "Iteration: 5654, training loss: 0.3817803152226484\n",
      "Iteration: 5655, training loss: 0.3817763540944302\n",
      "Iteration: 5656, training loss: 0.38177239391605405\n",
      "Iteration: 5657, training loss: 0.3817684346871069\n",
      "Iteration: 5658, training loss: 0.38176447640717603\n",
      "Iteration: 5659, training loss: 0.3817605190758493\n",
      "Iteration: 5660, training loss: 0.38175656269271424\n",
      "Iteration: 5661, training loss: 0.38175260725735916\n",
      "Iteration: 5662, training loss: 0.3817486527693722\n",
      "Iteration: 5663, training loss: 0.38174469922834203\n",
      "Iteration: 5664, training loss: 0.38174074663385754\n",
      "Iteration: 5665, training loss: 0.3817367949855079\n",
      "Iteration: 5666, training loss: 0.38173284428288234\n",
      "Iteration: 5667, training loss: 0.3817288945255706\n",
      "Iteration: 5668, training loss: 0.38172494571316223\n",
      "Iteration: 5669, training loss: 0.38172099784524766\n",
      "Iteration: 5670, training loss: 0.38171705092141706\n",
      "Iteration: 5671, training loss: 0.38171310494126115\n",
      "Iteration: 5672, training loss: 0.3817091599043708\n",
      "Iteration: 5673, training loss: 0.38170521581033695\n",
      "Iteration: 5674, training loss: 0.3817012726587512\n",
      "Iteration: 5675, training loss: 0.38169733044920495\n",
      "Iteration: 5676, training loss: 0.3816933891812901\n",
      "Iteration: 5677, training loss: 0.3816894488545989\n",
      "Iteration: 5678, training loss: 0.38168550946872354\n",
      "Iteration: 5679, training loss: 0.3816815710232568\n",
      "Iteration: 5680, training loss: 0.38167763351779144\n",
      "Iteration: 5681, training loss: 0.38167369695192044\n",
      "Iteration: 5682, training loss: 0.3816697613252373\n",
      "Iteration: 5683, training loss: 0.38166582663733567\n",
      "Iteration: 5684, training loss: 0.38166189288780933\n",
      "Iteration: 5685, training loss: 0.3816579600762523\n",
      "Iteration: 5686, training loss: 0.38165402820225913\n",
      "Iteration: 5687, training loss: 0.3816500972654242\n",
      "Iteration: 5688, training loss: 0.38164616726534256\n",
      "Iteration: 5689, training loss: 0.3816422382016092\n",
      "Iteration: 5690, training loss: 0.3816383100738194\n",
      "Iteration: 5691, training loss: 0.38163438288156887\n",
      "Iteration: 5692, training loss: 0.38163045662445333\n",
      "Iteration: 5693, training loss: 0.38162653130206886\n",
      "Iteration: 5694, training loss: 0.3816226069140119\n",
      "Iteration: 5695, training loss: 0.381618683459879\n",
      "Iteration: 5696, training loss: 0.38161476093926683\n",
      "Iteration: 5697, training loss: 0.38161083935177265\n",
      "Iteration: 5698, training loss: 0.38160691869699365\n",
      "Iteration: 5699, training loss: 0.38160299897452743\n",
      "Iteration: 5700, training loss: 0.3815990801839717\n",
      "Iteration: 5701, training loss: 0.38159516232492474\n",
      "Iteration: 5702, training loss: 0.3815912453969846\n",
      "Iteration: 5703, training loss: 0.38158732939974993\n",
      "Iteration: 5704, training loss: 0.38158341433281956\n",
      "Iteration: 5705, training loss: 0.3815795001957924\n",
      "Iteration: 5706, training loss: 0.38157558698826777\n",
      "Iteration: 5707, training loss: 0.38157167470984515\n",
      "Iteration: 5708, training loss: 0.3815677633601244\n",
      "Iteration: 5709, training loss: 0.3815638529387054\n",
      "Iteration: 5710, training loss: 0.3815599434451884\n",
      "Iteration: 5711, training loss: 0.38155603487917406\n",
      "Iteration: 5712, training loss: 0.38155212724026294\n",
      "Iteration: 5713, training loss: 0.3815482205280561\n",
      "Iteration: 5714, training loss: 0.38154431474215467\n",
      "Iteration: 5715, training loss: 0.38154040988216026\n",
      "Iteration: 5716, training loss: 0.3815365059476744\n",
      "Iteration: 5717, training loss: 0.3815326029382992\n",
      "Iteration: 5718, training loss: 0.38152870085363677\n",
      "Iteration: 5719, training loss: 0.38152479969328956\n",
      "Iteration: 5720, training loss: 0.38152089945686013\n",
      "Iteration: 5721, training loss: 0.38151700014395157\n",
      "Iteration: 5722, training loss: 0.3815131017541669\n",
      "Iteration: 5723, training loss: 0.3815092042871096\n",
      "Iteration: 5724, training loss: 0.3815053077423833\n",
      "Iteration: 5725, training loss: 0.38150141211959177\n",
      "Iteration: 5726, training loss: 0.38149751741833926\n",
      "Iteration: 5727, training loss: 0.38149362363823014\n",
      "Iteration: 5728, training loss: 0.3814897307788689\n",
      "Iteration: 5729, training loss: 0.38148583883986037\n",
      "Iteration: 5730, training loss: 0.3814819478208097\n",
      "Iteration: 5731, training loss: 0.38147805772132226\n",
      "Iteration: 5732, training loss: 0.3814741685410036\n",
      "Iteration: 5733, training loss: 0.38147028027945934\n",
      "Iteration: 5734, training loss: 0.38146639293629575\n",
      "Iteration: 5735, training loss: 0.381462506511119\n",
      "Iteration: 5736, training loss: 0.38145862100353556\n",
      "Iteration: 5737, training loss: 0.38145473641315225\n",
      "Iteration: 5738, training loss: 0.3814508527395761\n",
      "Iteration: 5739, training loss: 0.3814469699824142\n",
      "Iteration: 5740, training loss: 0.3814430881412741\n",
      "Iteration: 5741, training loss: 0.3814392072157635\n",
      "Iteration: 5742, training loss: 0.38143532720549034\n",
      "Iteration: 5743, training loss: 0.38143144811006297\n",
      "Iteration: 5744, training loss: 0.38142756992908955\n",
      "Iteration: 5745, training loss: 0.3814236926621788\n",
      "Iteration: 5746, training loss: 0.3814198163089397\n",
      "Iteration: 5747, training loss: 0.38141594086898134\n",
      "Iteration: 5748, training loss: 0.38141206634191305\n",
      "Iteration: 5749, training loss: 0.3814081927273445\n",
      "Iteration: 5750, training loss: 0.3814043200248855\n",
      "Iteration: 5751, training loss: 0.3814004482341461\n",
      "Iteration: 5752, training loss: 0.38139657735473664\n",
      "Iteration: 5753, training loss: 0.38139270738626774\n",
      "Iteration: 5754, training loss: 0.3813888383283501\n",
      "Iteration: 5755, training loss: 0.38138497018059464\n",
      "Iteration: 5756, training loss: 0.3813811029426128\n",
      "Iteration: 5757, training loss: 0.38137723661401596\n",
      "Iteration: 5758, training loss: 0.3813733711944159\n",
      "Iteration: 5759, training loss: 0.3813695066834245\n",
      "Iteration: 5760, training loss: 0.38136564308065407\n",
      "Iteration: 5761, training loss: 0.38136178038571694\n",
      "Iteration: 5762, training loss: 0.3813579185982258\n",
      "Iteration: 5763, training loss: 0.3813540577177936\n",
      "Iteration: 5764, training loss: 0.3813501977440334\n",
      "Iteration: 5765, training loss: 0.3813463386765586\n",
      "Iteration: 5766, training loss: 0.3813424805149829\n",
      "Iteration: 5767, training loss: 0.3813386232589198\n",
      "Iteration: 5768, training loss: 0.3813347669079837\n",
      "Iteration: 5769, training loss: 0.38133091146178877\n",
      "Iteration: 5770, training loss: 0.3813270569199495\n",
      "Iteration: 5771, training loss: 0.38132320328208075\n",
      "Iteration: 5772, training loss: 0.3813193505477975\n",
      "Iteration: 5773, training loss: 0.3813154987167149\n",
      "Iteration: 5774, training loss: 0.3813116477884484\n",
      "Iteration: 5775, training loss: 0.38130779776261375\n",
      "Iteration: 5776, training loss: 0.3813039486388269\n",
      "Iteration: 5777, training loss: 0.381300100416704\n",
      "Iteration: 5778, training loss: 0.3812962530958613\n",
      "Iteration: 5779, training loss: 0.3812924066759156\n",
      "Iteration: 5780, training loss: 0.38128856115648363\n",
      "Iteration: 5781, training loss: 0.38128471653718254\n",
      "Iteration: 5782, training loss: 0.38128087281762957\n",
      "Iteration: 5783, training loss: 0.38127702999744234\n",
      "Iteration: 5784, training loss: 0.38127318807623856\n",
      "Iteration: 5785, training loss: 0.3812693470536362\n",
      "Iteration: 5786, training loss: 0.3812655069292536\n",
      "Iteration: 5787, training loss: 0.38126166770270914\n",
      "Iteration: 5788, training loss: 0.38125782937362146\n",
      "Iteration: 5789, training loss: 0.38125399194160964\n",
      "Iteration: 5790, training loss: 0.3812501554062927\n",
      "Iteration: 5791, training loss: 0.3812463197672901\n",
      "Iteration: 5792, training loss: 0.38124248502422126\n",
      "Iteration: 5793, training loss: 0.38123865117670624\n",
      "Iteration: 5794, training loss: 0.3812348182243651\n",
      "Iteration: 5795, training loss: 0.38123098616681794\n",
      "Iteration: 5796, training loss: 0.3812271550036854\n",
      "Iteration: 5797, training loss: 0.38122332473458825\n",
      "Iteration: 5798, training loss: 0.3812194953591475\n",
      "Iteration: 5799, training loss: 0.38121566687698427\n",
      "Iteration: 5800, training loss: 0.38121183928771996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5801, training loss: 0.38120801259097636\n",
      "Iteration: 5802, training loss: 0.38120418678637524\n",
      "Iteration: 5803, training loss: 0.38120036187353873\n",
      "Iteration: 5804, training loss: 0.3811965378520893\n",
      "Iteration: 5805, training loss: 0.3811927147216493\n",
      "Iteration: 5806, training loss: 0.38118889248184173\n",
      "Iteration: 5807, training loss: 0.38118507113228944\n",
      "Iteration: 5808, training loss: 0.38118125067261593\n",
      "Iteration: 5809, training loss: 0.3811774311024443\n",
      "Iteration: 5810, training loss: 0.3811736124213986\n",
      "Iteration: 5811, training loss: 0.3811697946291026\n",
      "Iteration: 5812, training loss: 0.3811659777251804\n",
      "Iteration: 5813, training loss: 0.3811621617092565\n",
      "Iteration: 5814, training loss: 0.3811583465809553\n",
      "Iteration: 5815, training loss: 0.38115453233990193\n",
      "Iteration: 5816, training loss: 0.3811507189857213\n",
      "Iteration: 5817, training loss: 0.3811469065180385\n",
      "Iteration: 5818, training loss: 0.3811430949364793\n",
      "Iteration: 5819, training loss: 0.38113928424066934\n",
      "Iteration: 5820, training loss: 0.3811354744302345\n",
      "Iteration: 5821, training loss: 0.381131665504801\n",
      "Iteration: 5822, training loss: 0.38112785746399513\n",
      "Iteration: 5823, training loss: 0.38112405030744384\n",
      "Iteration: 5824, training loss: 0.3811202440347736\n",
      "Iteration: 5825, training loss: 0.38111643864561173\n",
      "Iteration: 5826, training loss: 0.3811126341395853\n",
      "Iteration: 5827, training loss: 0.381108830516322\n",
      "Iteration: 5828, training loss: 0.38110502777544963\n",
      "Iteration: 5829, training loss: 0.38110122591659584\n",
      "Iteration: 5830, training loss: 0.3810974249393891\n",
      "Iteration: 5831, training loss: 0.3810936248434577\n",
      "Iteration: 5832, training loss: 0.3810898256284303\n",
      "Iteration: 5833, training loss: 0.38108602729393576\n",
      "Iteration: 5834, training loss: 0.3810822298396031\n",
      "Iteration: 5835, training loss: 0.38107843326506163\n",
      "Iteration: 5836, training loss: 0.381074637569941\n",
      "Iteration: 5837, training loss: 0.3810708427538706\n",
      "Iteration: 5838, training loss: 0.38106704881648074\n",
      "Iteration: 5839, training loss: 0.38106325575740146\n",
      "Iteration: 5840, training loss: 0.38105946357626314\n",
      "Iteration: 5841, training loss: 0.3810556722726964\n",
      "Iteration: 5842, training loss: 0.38105188184633215\n",
      "Iteration: 5843, training loss: 0.38104809229680137\n",
      "Iteration: 5844, training loss: 0.3810443036237354\n",
      "Iteration: 5845, training loss: 0.3810405158267657\n",
      "Iteration: 5846, training loss: 0.38103672890552404\n",
      "Iteration: 5847, training loss: 0.3810329428596423\n",
      "Iteration: 5848, training loss: 0.3810291576887527\n",
      "Iteration: 5849, training loss: 0.3810253733924875\n",
      "Iteration: 5850, training loss: 0.3810215899704794\n",
      "Iteration: 5851, training loss: 0.3810178074223613\n",
      "Iteration: 5852, training loss: 0.38101402574776605\n",
      "Iteration: 5853, training loss: 0.38101024494632707\n",
      "Iteration: 5854, training loss: 0.3810064650176777\n",
      "Iteration: 5855, training loss: 0.3810026859614517\n",
      "Iteration: 5856, training loss: 0.380998907777283\n",
      "Iteration: 5857, training loss: 0.38099513046480565\n",
      "Iteration: 5858, training loss: 0.38099135402365414\n",
      "Iteration: 5859, training loss: 0.3809875784534629\n",
      "Iteration: 5860, training loss: 0.38098380375386676\n",
      "Iteration: 5861, training loss: 0.3809800299245007\n",
      "Iteration: 5862, training loss: 0.3809762569649999\n",
      "Iteration: 5863, training loss: 0.380972484875\n",
      "Iteration: 5864, training loss: 0.3809687136541363\n",
      "Iteration: 5865, training loss: 0.3809649433020451\n",
      "Iteration: 5866, training loss: 0.38096117381836214\n",
      "Iteration: 5867, training loss: 0.38095740520272386\n",
      "Iteration: 5868, training loss: 0.38095363745476674\n",
      "Iteration: 5869, training loss: 0.38094987057412755\n",
      "Iteration: 5870, training loss: 0.3809461045604432\n",
      "Iteration: 5871, training loss: 0.3809423394133509\n",
      "Iteration: 5872, training loss: 0.38093857513248797\n",
      "Iteration: 5873, training loss: 0.38093481171749205\n",
      "Iteration: 5874, training loss: 0.38093104916800097\n",
      "Iteration: 5875, training loss: 0.3809272874836527\n",
      "Iteration: 5876, training loss: 0.3809235266640856\n",
      "Iteration: 5877, training loss: 0.380919766708938\n",
      "Iteration: 5878, training loss: 0.3809160076178485\n",
      "Iteration: 5879, training loss: 0.3809122493904563\n",
      "Iteration: 5880, training loss: 0.38090849202640026\n",
      "Iteration: 5881, training loss: 0.3809047355253196\n",
      "Iteration: 5882, training loss: 0.3809009798868542\n",
      "Iteration: 5883, training loss: 0.38089722511064344\n",
      "Iteration: 5884, training loss: 0.38089347119632755\n",
      "Iteration: 5885, training loss: 0.38088971814354644\n",
      "Iteration: 5886, training loss: 0.3808859659519408\n",
      "Iteration: 5887, training loss: 0.38088221462115107\n",
      "Iteration: 5888, training loss: 0.38087846415081794\n",
      "Iteration: 5889, training loss: 0.3808747145405827\n",
      "Iteration: 5890, training loss: 0.3808709657900863\n",
      "Iteration: 5891, training loss: 0.3808672178989704\n",
      "Iteration: 5892, training loss: 0.38086347086687655\n",
      "Iteration: 5893, training loss: 0.3808597246934468\n",
      "Iteration: 5894, training loss: 0.380855979378323\n",
      "Iteration: 5895, training loss: 0.3808522349211476\n",
      "Iteration: 5896, training loss: 0.38084849132156307\n",
      "Iteration: 5897, training loss: 0.38084474857921213\n",
      "Iteration: 5898, training loss: 0.38084100669373777\n",
      "Iteration: 5899, training loss: 0.38083726566478304\n",
      "Iteration: 5900, training loss: 0.38083352549199134\n",
      "Iteration: 5901, training loss: 0.38082978617500624\n",
      "Iteration: 5902, training loss: 0.38082604771347145\n",
      "Iteration: 5903, training loss: 0.38082231010703116\n",
      "Iteration: 5904, training loss: 0.3808185733553293\n",
      "Iteration: 5905, training loss: 0.3808148374580106\n",
      "Iteration: 5906, training loss: 0.3808111024147193\n",
      "Iteration: 5907, training loss: 0.3808073682251006\n",
      "Iteration: 5908, training loss: 0.3808036348887993\n",
      "Iteration: 5909, training loss: 0.38079990240546086\n",
      "Iteration: 5910, training loss: 0.3807961707747305\n",
      "Iteration: 5911, training loss: 0.3807924399962539\n",
      "Iteration: 5912, training loss: 0.3807887100696772\n",
      "Iteration: 5913, training loss: 0.38078498099464625\n",
      "Iteration: 5914, training loss: 0.3807812527708075\n",
      "Iteration: 5915, training loss: 0.38077752539780735\n",
      "Iteration: 5916, training loss: 0.38077379887529256\n",
      "Iteration: 5917, training loss: 0.38077007320291006\n",
      "Iteration: 5918, training loss: 0.38076634838030693\n",
      "Iteration: 5919, training loss: 0.38076262440713066\n",
      "Iteration: 5920, training loss: 0.3807589012830286\n",
      "Iteration: 5921, training loss: 0.38075517900764866\n",
      "Iteration: 5922, training loss: 0.38075145758063866\n",
      "Iteration: 5923, training loss: 0.38074773700164705\n",
      "Iteration: 5924, training loss: 0.3807440172703219\n",
      "Iteration: 5925, training loss: 0.380740298386312\n",
      "Iteration: 5926, training loss: 0.38073658034926605\n",
      "Iteration: 5927, training loss: 0.3807328631588331\n",
      "Iteration: 5928, training loss: 0.3807291468146623\n",
      "Iteration: 5929, training loss: 0.38072543131640313\n",
      "Iteration: 5930, training loss: 0.3807217166637051\n",
      "Iteration: 5931, training loss: 0.3807180028562182\n",
      "Iteration: 5932, training loss: 0.3807142898935924\n",
      "Iteration: 5933, training loss: 0.380710577775478\n",
      "Iteration: 5934, training loss: 0.3807068665015253\n",
      "Iteration: 5935, training loss: 0.3807031560713851\n",
      "Iteration: 5936, training loss: 0.38069944648470827\n",
      "Iteration: 5937, training loss: 0.3806957377411456\n",
      "Iteration: 5938, training loss: 0.3806920298403488\n",
      "Iteration: 5939, training loss: 0.3806883227819689\n",
      "Iteration: 5940, training loss: 0.380684616565658\n",
      "Iteration: 5941, training loss: 0.3806809111910676\n",
      "Iteration: 5942, training loss: 0.3806772066578502\n",
      "Iteration: 5943, training loss: 0.38067350296565783\n",
      "Iteration: 5944, training loss: 0.380669800114143\n",
      "Iteration: 5945, training loss: 0.3806660981029583\n",
      "Iteration: 5946, training loss: 0.38066239693175696\n",
      "Iteration: 5947, training loss: 0.380658696600192\n",
      "Iteration: 5948, training loss: 0.3806549971079166\n",
      "Iteration: 5949, training loss: 0.3806512984545843\n",
      "Iteration: 5950, training loss: 0.3806476006398489\n",
      "Iteration: 5951, training loss: 0.38064390366336426\n",
      "Iteration: 5952, training loss: 0.3806402075247846\n",
      "Iteration: 5953, training loss: 0.38063651222376427\n",
      "Iteration: 5954, training loss: 0.38063281775995766\n",
      "Iteration: 5955, training loss: 0.38062912413301975\n",
      "Iteration: 5956, training loss: 0.3806254313426052\n",
      "Iteration: 5957, training loss: 0.38062173938836924\n",
      "Iteration: 5958, training loss: 0.38061804826996737\n",
      "Iteration: 5959, training loss: 0.38061435798705506\n",
      "Iteration: 5960, training loss: 0.380610668539288\n",
      "Iteration: 5961, training loss: 0.38060697992632214\n",
      "Iteration: 5962, training loss: 0.3806032921478138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5963, training loss: 0.3805996052034192\n",
      "Iteration: 5964, training loss: 0.38059591909279483\n",
      "Iteration: 5965, training loss: 0.3805922338155977\n",
      "Iteration: 5966, training loss: 0.38058854937148456\n",
      "Iteration: 5967, training loss: 0.38058486576011263\n",
      "Iteration: 5968, training loss: 0.3805811829811393\n",
      "Iteration: 5969, training loss: 0.38057750103422217\n",
      "Iteration: 5970, training loss: 0.38057381991901884\n",
      "Iteration: 5971, training loss: 0.38057013963518754\n",
      "Iteration: 5972, training loss: 0.38056646018238616\n",
      "Iteration: 5973, training loss: 0.3805627815602733\n",
      "Iteration: 5974, training loss: 0.3805591037685073\n",
      "Iteration: 5975, training loss: 0.3805554268067472\n",
      "Iteration: 5976, training loss: 0.3805517506746518\n",
      "Iteration: 5977, training loss: 0.3805480753718803\n",
      "Iteration: 5978, training loss: 0.38054440089809194\n",
      "Iteration: 5979, training loss: 0.3805407272529465\n",
      "Iteration: 5980, training loss: 0.38053705443610375\n",
      "Iteration: 5981, training loss: 0.3805333824472235\n",
      "Iteration: 5982, training loss: 0.3805297112859659\n",
      "Iteration: 5983, training loss: 0.3805260409519915\n",
      "Iteration: 5984, training loss: 0.3805223714449606\n",
      "Iteration: 5985, training loss: 0.38051870276453426\n",
      "Iteration: 5986, training loss: 0.38051503491037314\n",
      "Iteration: 5987, training loss: 0.3805113678821387\n",
      "Iteration: 5988, training loss: 0.38050770167949205\n",
      "Iteration: 5989, training loss: 0.38050403630209473\n",
      "Iteration: 5990, training loss: 0.3805003717496087\n",
      "Iteration: 5991, training loss: 0.38049670802169583\n",
      "Iteration: 5992, training loss: 0.3804930451180181\n",
      "Iteration: 5993, training loss: 0.3804893830382381\n",
      "Iteration: 5994, training loss: 0.3804857217820183\n",
      "Iteration: 5995, training loss: 0.38048206134902124\n",
      "Iteration: 5996, training loss: 0.38047840173891007\n",
      "Iteration: 5997, training loss: 0.38047474295134776\n",
      "Iteration: 5998, training loss: 0.38047108498599785\n",
      "Iteration: 5999, training loss: 0.38046742784252374\n",
      "Iteration: 6000, training loss: 0.38046377152058913\n",
      "Iteration: 6001, training loss: 0.38046011601985796\n",
      "Iteration: 6002, training loss: 0.3804564613399943\n",
      "Iteration: 6003, training loss: 0.38045280748066257\n",
      "Iteration: 6004, training loss: 0.38044915444152716\n",
      "Iteration: 6005, training loss: 0.38044550222225276\n",
      "Iteration: 6006, training loss: 0.3804418508225044\n",
      "Iteration: 6007, training loss: 0.3804382002419471\n",
      "Iteration: 6008, training loss: 0.38043455048024605\n",
      "Iteration: 6009, training loss: 0.3804309015370668\n",
      "Iteration: 6010, training loss: 0.3804272534120751\n",
      "Iteration: 6011, training loss: 0.3804236061049367\n",
      "Iteration: 6012, training loss: 0.3804199596153178\n",
      "Iteration: 6013, training loss: 0.3804163139428845\n",
      "Iteration: 6014, training loss: 0.3804126690873034\n",
      "Iteration: 6015, training loss: 0.38040902504824103\n",
      "Iteration: 6016, training loss: 0.3804053818253643\n",
      "Iteration: 6017, training loss: 0.3804017394183402\n",
      "Iteration: 6018, training loss: 0.3803980978268361\n",
      "Iteration: 6019, training loss: 0.3803944570505192\n",
      "Iteration: 6020, training loss: 0.38039081708905725\n",
      "Iteration: 6021, training loss: 0.380387177942118\n",
      "Iteration: 6022, training loss: 0.38038353960936955\n",
      "Iteration: 6023, training loss: 0.38037990209048\n",
      "Iteration: 6024, training loss: 0.38037626538511776\n",
      "Iteration: 6025, training loss: 0.3803726294929514\n",
      "Iteration: 6026, training loss: 0.3803689944136498\n",
      "Iteration: 6027, training loss: 0.38036536014688177\n",
      "Iteration: 6028, training loss: 0.38036172669231655\n",
      "Iteration: 6029, training loss: 0.3803580940496235\n",
      "Iteration: 6030, training loss: 0.38035446221847224\n",
      "Iteration: 6031, training loss: 0.38035083119853225\n",
      "Iteration: 6032, training loss: 0.3803472009894737\n",
      "Iteration: 6033, training loss: 0.38034357159096666\n",
      "Iteration: 6034, training loss: 0.38033994300268137\n",
      "Iteration: 6035, training loss: 0.3803363152242883\n",
      "Iteration: 6036, training loss: 0.38033268825545835\n",
      "Iteration: 6037, training loss: 0.3803290620958622\n",
      "Iteration: 6038, training loss: 0.380325436745171\n",
      "Iteration: 6039, training loss: 0.38032181220305594\n",
      "Iteration: 6040, training loss: 0.38031818846918874\n",
      "Iteration: 6041, training loss: 0.38031456554324067\n",
      "Iteration: 6042, training loss: 0.3803109434248838\n",
      "Iteration: 6043, training loss: 0.38030732211379015\n",
      "Iteration: 6044, training loss: 0.3803037016096319\n",
      "Iteration: 6045, training loss: 0.38030008191208137\n",
      "Iteration: 6046, training loss: 0.3802964630208113\n",
      "Iteration: 6047, training loss: 0.3802928449354944\n",
      "Iteration: 6048, training loss: 0.38028922765580364\n",
      "Iteration: 6049, training loss: 0.3802856111814123\n",
      "Iteration: 6050, training loss: 0.3802819955119936\n",
      "Iteration: 6051, training loss: 0.38027838064722114\n",
      "Iteration: 6052, training loss: 0.3802747665867686\n",
      "Iteration: 6053, training loss: 0.3802711533303101\n",
      "Iteration: 6054, training loss: 0.38026754087751946\n",
      "Iteration: 6055, training loss: 0.38026392922807123\n",
      "Iteration: 6056, training loss: 0.38026031838163965\n",
      "Iteration: 6057, training loss: 0.38025670833789954\n",
      "Iteration: 6058, training loss: 0.38025309909652594\n",
      "Iteration: 6059, training loss: 0.38024949065719355\n",
      "Iteration: 6060, training loss: 0.3802458830195779\n",
      "Iteration: 6061, training loss: 0.38024227618335427\n",
      "Iteration: 6062, training loss: 0.3802386701481984\n",
      "Iteration: 6063, training loss: 0.3802350649137859\n",
      "Iteration: 6064, training loss: 0.38023146047979295\n",
      "Iteration: 6065, training loss: 0.38022785684589555\n",
      "Iteration: 6066, training loss: 0.3802242540117703\n",
      "Iteration: 6067, training loss: 0.3802206519770935\n",
      "Iteration: 6068, training loss: 0.3802170507415421\n",
      "Iteration: 6069, training loss: 0.38021345030479287\n",
      "Iteration: 6070, training loss: 0.3802098506665231\n",
      "Iteration: 6071, training loss: 0.3802062518264099\n",
      "Iteration: 6072, training loss: 0.3802026537841308\n",
      "Iteration: 6073, training loss: 0.3801990565393636\n",
      "Iteration: 6074, training loss: 0.380195460091786\n",
      "Iteration: 6075, training loss: 0.38019186444107617\n",
      "Iteration: 6076, training loss: 0.38018826958691226\n",
      "Iteration: 6077, training loss: 0.38018467552897267\n",
      "Iteration: 6078, training loss: 0.3801810822669361\n",
      "Iteration: 6079, training loss: 0.38017748980048127\n",
      "Iteration: 6080, training loss: 0.38017389812928715\n",
      "Iteration: 6081, training loss: 0.3801703072530329\n",
      "Iteration: 6082, training loss: 0.38016671717139794\n",
      "Iteration: 6083, training loss: 0.3801631278840617\n",
      "Iteration: 6084, training loss: 0.38015953939070385\n",
      "Iteration: 6085, training loss: 0.38015595169100447\n",
      "Iteration: 6086, training loss: 0.38015236478464337\n",
      "Iteration: 6087, training loss: 0.38014877867130115\n",
      "Iteration: 6088, training loss: 0.38014519335065794\n",
      "Iteration: 6089, training loss: 0.38014160882239456\n",
      "Iteration: 6090, training loss: 0.38013802508619177\n",
      "Iteration: 6091, training loss: 0.3801344421417306\n",
      "Iteration: 6092, training loss: 0.38013085998869217\n",
      "Iteration: 6093, training loss: 0.38012727862675794\n",
      "Iteration: 6094, training loss: 0.3801236980556094\n",
      "Iteration: 6095, training loss: 0.3801201182749283\n",
      "Iteration: 6096, training loss: 0.38011653928439665\n",
      "Iteration: 6097, training loss: 0.3801129610836964\n",
      "Iteration: 6098, training loss: 0.38010938367250985\n",
      "Iteration: 6099, training loss: 0.3801058070505195\n",
      "Iteration: 6100, training loss: 0.38010223121740805\n",
      "Iteration: 6101, training loss: 0.38009865617285826\n",
      "Iteration: 6102, training loss: 0.3800950819165532\n",
      "Iteration: 6103, training loss: 0.380091508448176\n",
      "Iteration: 6104, training loss: 0.3800879357674102\n",
      "Iteration: 6105, training loss: 0.3800843638739391\n",
      "Iteration: 6106, training loss: 0.38008079276744666\n",
      "Iteration: 6107, training loss: 0.3800772224476167\n",
      "Iteration: 6108, training loss: 0.38007365291413336\n",
      "Iteration: 6109, training loss: 0.3800700841666809\n",
      "Iteration: 6110, training loss: 0.3800665162049438\n",
      "Iteration: 6111, training loss: 0.38006294902860677\n",
      "Iteration: 6112, training loss: 0.38005938263735456\n",
      "Iteration: 6113, training loss: 0.3800558170308721\n",
      "Iteration: 6114, training loss: 0.38005225220884475\n",
      "Iteration: 6115, training loss: 0.3800486881709579\n",
      "Iteration: 6116, training loss: 0.38004512491689696\n",
      "Iteration: 6117, training loss: 0.38004156244634774\n",
      "Iteration: 6118, training loss: 0.380038000758996\n",
      "Iteration: 6119, training loss: 0.38003443985452817\n",
      "Iteration: 6120, training loss: 0.3800308797326302\n",
      "Iteration: 6121, training loss: 0.38002732039298875\n",
      "Iteration: 6122, training loss: 0.38002376183529035\n",
      "Iteration: 6123, training loss: 0.38002020405922177\n",
      "Iteration: 6124, training loss: 0.3800166470644702\n",
      "Iteration: 6125, training loss: 0.3800130908507226\n",
      "Iteration: 6126, training loss: 0.3800095354176663\n",
      "Iteration: 6127, training loss: 0.3800059807649891\n",
      "Iteration: 6128, training loss: 0.3800024268923784\n",
      "Iteration: 6129, training loss: 0.3799988737995223\n",
      "Iteration: 6130, training loss: 0.3799953214861087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6131, training loss: 0.37999176995182604\n",
      "Iteration: 6132, training loss: 0.37998821919636266\n",
      "Iteration: 6133, training loss: 0.37998466921940705\n",
      "Iteration: 6134, training loss: 0.3799811200206481\n",
      "Iteration: 6135, training loss: 0.37997757159977474\n",
      "Iteration: 6136, training loss: 0.3799740239564762\n",
      "Iteration: 6137, training loss: 0.37997047709044157\n",
      "Iteration: 6138, training loss: 0.3799669310013606\n",
      "Iteration: 6139, training loss: 0.3799633856889228\n",
      "Iteration: 6140, training loss: 0.37995984115281806\n",
      "Iteration: 6141, training loss: 0.3799562973927364\n",
      "Iteration: 6142, training loss: 0.37995275440836795\n",
      "Iteration: 6143, training loss: 0.37994921219940325\n",
      "Iteration: 6144, training loss: 0.37994567076553276\n",
      "Iteration: 6145, training loss: 0.3799421301064471\n",
      "Iteration: 6146, training loss: 0.37993859022183735\n",
      "Iteration: 6147, training loss: 0.37993505111139464\n",
      "Iteration: 6148, training loss: 0.3799315127748099\n",
      "Iteration: 6149, training loss: 0.37992797521177496\n",
      "Iteration: 6150, training loss: 0.3799244384219813\n",
      "Iteration: 6151, training loss: 0.3799209024051206\n",
      "Iteration: 6152, training loss: 0.37991736716088487\n",
      "Iteration: 6153, training loss: 0.37991383268896634\n",
      "Iteration: 6154, training loss: 0.3799102989890573\n",
      "Iteration: 6155, training loss: 0.3799067660608502\n",
      "Iteration: 6156, training loss: 0.3799032339040377\n",
      "Iteration: 6157, training loss: 0.3798997025183126\n",
      "Iteration: 6158, training loss: 0.379896171903368\n",
      "Iteration: 6159, training loss: 0.3798926420588971\n",
      "Iteration: 6160, training loss: 0.3798891129845932\n",
      "Iteration: 6161, training loss: 0.3798855846801501\n",
      "Iteration: 6162, training loss: 0.37988205714526113\n",
      "Iteration: 6163, training loss: 0.3798785303796203\n",
      "Iteration: 6164, training loss: 0.3798750043829218\n",
      "Iteration: 6165, training loss: 0.37987147915485986\n",
      "Iteration: 6166, training loss: 0.3798679546951288\n",
      "Iteration: 6167, training loss: 0.3798644310034232\n",
      "Iteration: 6168, training loss: 0.3798609080794379\n",
      "Iteration: 6169, training loss: 0.3798573859228679\n",
      "Iteration: 6170, training loss: 0.3798538645334081\n",
      "Iteration: 6171, training loss: 0.37985034391075395\n",
      "Iteration: 6172, training loss: 0.3798468240546008\n",
      "Iteration: 6173, training loss: 0.3798433049646443\n",
      "Iteration: 6174, training loss: 0.37983978664058043\n",
      "Iteration: 6175, training loss: 0.37983626908210494\n",
      "Iteration: 6176, training loss: 0.37983275228891417\n",
      "Iteration: 6177, training loss: 0.37982923626070425\n",
      "Iteration: 6178, training loss: 0.3798257209971717\n",
      "Iteration: 6179, training loss: 0.3798222064980134\n",
      "Iteration: 6180, training loss: 0.3798186927629259\n",
      "Iteration: 6181, training loss: 0.3798151797916065\n",
      "Iteration: 6182, training loss: 0.3798116675837521\n",
      "Iteration: 6183, training loss: 0.3798081561390603\n",
      "Iteration: 6184, training loss: 0.37980464545722853\n",
      "Iteration: 6185, training loss: 0.37980113553795447\n",
      "Iteration: 6186, training loss: 0.379797626380936\n",
      "Iteration: 6187, training loss: 0.3797941179858711\n",
      "Iteration: 6188, training loss: 0.37979061035245815\n",
      "Iteration: 6189, training loss: 0.37978710348039535\n",
      "Iteration: 6190, training loss: 0.37978359736938144\n",
      "Iteration: 6191, training loss: 0.379780092019115\n",
      "Iteration: 6192, training loss: 0.379776587429295\n",
      "Iteration: 6193, training loss: 0.37977308359962053\n",
      "Iteration: 6194, training loss: 0.3797695805297908\n",
      "Iteration: 6195, training loss: 0.37976607821950525\n",
      "Iteration: 6196, training loss: 0.3797625766684634\n",
      "Iteration: 6197, training loss: 0.379759075876365\n",
      "Iteration: 6198, training loss: 0.37975557584291003\n",
      "Iteration: 6199, training loss: 0.3797520765677986\n",
      "Iteration: 6200, training loss: 0.37974857805073087\n",
      "Iteration: 6201, training loss: 0.3797450802914073\n",
      "Iteration: 6202, training loss: 0.3797415832895287\n",
      "Iteration: 6203, training loss: 0.3797380870447955\n",
      "Iteration: 6204, training loss: 0.3797345915569089\n",
      "Iteration: 6205, training loss: 0.37973109682556994\n",
      "Iteration: 6206, training loss: 0.3797276028504798\n",
      "Iteration: 6207, training loss: 0.37972410963134007\n",
      "Iteration: 6208, training loss: 0.3797206171678523\n",
      "Iteration: 6209, training loss: 0.37971712545971825\n",
      "Iteration: 6210, training loss: 0.37971363450664\n",
      "Iteration: 6211, training loss: 0.3797101443083195\n",
      "Iteration: 6212, training loss: 0.3797066548644592\n",
      "Iteration: 6213, training loss: 0.37970316617476146\n",
      "Iteration: 6214, training loss: 0.379699678238929\n",
      "Iteration: 6215, training loss: 0.37969619105666447\n",
      "Iteration: 6216, training loss: 0.379692704627671\n",
      "Iteration: 6217, training loss: 0.37968921895165153\n",
      "Iteration: 6218, training loss: 0.37968573402830963\n",
      "Iteration: 6219, training loss: 0.37968224985734855\n",
      "Iteration: 6220, training loss: 0.3796787664384719\n",
      "Iteration: 6221, training loss: 0.37967528377138376\n",
      "Iteration: 6222, training loss: 0.3796718018557879\n",
      "Iteration: 6223, training loss: 0.3796683206913884\n",
      "Iteration: 6224, training loss: 0.3796648402778897\n",
      "Iteration: 6225, training loss: 0.3796613606149962\n",
      "Iteration: 6226, training loss: 0.3796578817024126\n",
      "Iteration: 6227, training loss: 0.3796544035398437\n",
      "Iteration: 6228, training loss: 0.37965092612699447\n",
      "Iteration: 6229, training loss: 0.37964744946356993\n",
      "Iteration: 6230, training loss: 0.37964397354927554\n",
      "Iteration: 6231, training loss: 0.37964049838381664\n",
      "Iteration: 6232, training loss: 0.379637023966899\n",
      "Iteration: 6233, training loss: 0.3796335502982283\n",
      "Iteration: 6234, training loss: 0.3796300773775107\n",
      "Iteration: 6235, training loss: 0.3796266052044521\n",
      "Iteration: 6236, training loss: 0.379623133778759\n",
      "Iteration: 6237, training loss: 0.3796196631001378\n",
      "Iteration: 6238, training loss: 0.37961619316829504\n",
      "Iteration: 6239, training loss: 0.37961272398293766\n",
      "Iteration: 6240, training loss: 0.37960925554377256\n",
      "Iteration: 6241, training loss: 0.37960578785050686\n",
      "Iteration: 6242, training loss: 0.37960232090284785\n",
      "Iteration: 6243, training loss: 0.379598854700503\n",
      "Iteration: 6244, training loss: 0.3795953892431799\n",
      "Iteration: 6245, training loss: 0.37959192453058643\n",
      "Iteration: 6246, training loss: 0.3795884605624305\n",
      "Iteration: 6247, training loss: 0.3795849973384202\n",
      "Iteration: 6248, training loss: 0.3795815348582638\n",
      "Iteration: 6249, training loss: 0.37957807312166986\n",
      "Iteration: 6250, training loss: 0.3795746121283468\n",
      "Iteration: 6251, training loss: 0.37957115187800355\n",
      "Iteration: 6252, training loss: 0.37956769237034904\n",
      "Iteration: 6253, training loss: 0.3795642336050923\n",
      "Iteration: 6254, training loss: 0.3795607755819425\n",
      "Iteration: 6255, training loss: 0.3795573183006093\n",
      "Iteration: 6256, training loss: 0.37955386176080214\n",
      "Iteration: 6257, training loss: 0.3795504059622309\n",
      "Iteration: 6258, training loss: 0.37954695090460533\n",
      "Iteration: 6259, training loss: 0.37954349658763564\n",
      "Iteration: 6260, training loss: 0.379540043011032\n",
      "Iteration: 6261, training loss: 0.3795365901745048\n",
      "Iteration: 6262, training loss: 0.37953313807776473\n",
      "Iteration: 6263, training loss: 0.3795296867205224\n",
      "Iteration: 6264, training loss: 0.3795262361024887\n",
      "Iteration: 6265, training loss: 0.3795227862233749\n",
      "Iteration: 6266, training loss: 0.379519337082892\n",
      "Iteration: 6267, training loss: 0.3795158886807513\n",
      "Iteration: 6268, training loss: 0.3795124410166646\n",
      "Iteration: 6269, training loss: 0.37950899409034344\n",
      "Iteration: 6270, training loss: 0.37950554790149976\n",
      "Iteration: 6271, training loss: 0.3795021024498456\n",
      "Iteration: 6272, training loss: 0.37949865773509306\n",
      "Iteration: 6273, training loss: 0.3794952137569546\n",
      "Iteration: 6274, training loss: 0.3794917705151427\n",
      "Iteration: 6275, training loss: 0.3794883280093698\n",
      "Iteration: 6276, training loss: 0.37948488623934923\n",
      "Iteration: 6277, training loss: 0.3794814452047935\n",
      "Iteration: 6278, training loss: 0.37947800490541594\n",
      "Iteration: 6279, training loss: 0.37947456534093\n",
      "Iteration: 6280, training loss: 0.37947112651104903\n",
      "Iteration: 6281, training loss: 0.3794676884154866\n",
      "Iteration: 6282, training loss: 0.3794642510539568\n",
      "Iteration: 6283, training loss: 0.37946081442617324\n",
      "Iteration: 6284, training loss: 0.37945737853185024\n",
      "Iteration: 6285, training loss: 0.379453943370702\n",
      "Iteration: 6286, training loss: 0.379450508942443\n",
      "Iteration: 6287, training loss: 0.37944707524678783\n",
      "Iteration: 6288, training loss: 0.3794436422834512\n",
      "Iteration: 6289, training loss: 0.3794402100521482\n",
      "Iteration: 6290, training loss: 0.3794367785525936\n",
      "Iteration: 6291, training loss: 0.3794333477845029\n",
      "Iteration: 6292, training loss: 0.3794299177475914\n",
      "Iteration: 6293, training loss: 0.3794264884415748\n",
      "Iteration: 6294, training loss: 0.3794230598661685\n",
      "Iteration: 6295, training loss: 0.37941963202108864\n",
      "Iteration: 6296, training loss: 0.37941620490605116\n",
      "Iteration: 6297, training loss: 0.3794127785207723\n",
      "Iteration: 6298, training loss: 0.37940935286496835\n",
      "Iteration: 6299, training loss: 0.37940592793835576\n",
      "Iteration: 6300, training loss: 0.3794025037406514\n",
      "Iteration: 6301, training loss: 0.379399080271572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6302, training loss: 0.37939565753083443\n",
      "Iteration: 6303, training loss: 0.37939223551815593\n",
      "Iteration: 6304, training loss: 0.37938881423325377\n",
      "Iteration: 6305, training loss: 0.37938539367584545\n",
      "Iteration: 6306, training loss: 0.37938197384564853\n",
      "Iteration: 6307, training loss: 0.3793785547423809\n",
      "Iteration: 6308, training loss: 0.37937513636576037\n",
      "Iteration: 6309, training loss: 0.379371718715505\n",
      "Iteration: 6310, training loss: 0.37936830179133313\n",
      "Iteration: 6311, training loss: 0.37936488559296316\n",
      "Iteration: 6312, training loss: 0.3793614701201136\n",
      "Iteration: 6313, training loss: 0.37935805537250317\n",
      "Iteration: 6314, training loss: 0.37935464134985075\n",
      "Iteration: 6315, training loss: 0.3793512280518754\n",
      "Iteration: 6316, training loss: 0.37934781547829616\n",
      "Iteration: 6317, training loss: 0.37934440362883265\n",
      "Iteration: 6318, training loss: 0.37934099250320413\n",
      "Iteration: 6319, training loss: 0.3793375821011303\n",
      "Iteration: 6320, training loss: 0.379334172422331\n",
      "Iteration: 6321, training loss: 0.3793307634665262\n",
      "Iteration: 6322, training loss: 0.3793273552334362\n",
      "Iteration: 6323, training loss: 0.3793239477227809\n",
      "Iteration: 6324, training loss: 0.3793205409342811\n",
      "Iteration: 6325, training loss: 0.37931713486765706\n",
      "Iteration: 6326, training loss: 0.3793137295226298\n",
      "Iteration: 6327, training loss: 0.37931032489892014\n",
      "Iteration: 6328, training loss: 0.37930692099624896\n",
      "Iteration: 6329, training loss: 0.3793035178143378\n",
      "Iteration: 6330, training loss: 0.37930011535290786\n",
      "Iteration: 6331, training loss: 0.3792967136116805\n",
      "Iteration: 6332, training loss: 0.3792933125903777\n",
      "Iteration: 6333, training loss: 0.37928991228872094\n",
      "Iteration: 6334, training loss: 0.3792865127064326\n",
      "Iteration: 6335, training loss: 0.3792831138432346\n",
      "Iteration: 6336, training loss: 0.3792797156988492\n",
      "Iteration: 6337, training loss: 0.37927631827299896\n",
      "Iteration: 6338, training loss: 0.37927292156540643\n",
      "Iteration: 6339, training loss: 0.37926952557579435\n",
      "Iteration: 6340, training loss: 0.3792661303038857\n",
      "Iteration: 6341, training loss: 0.3792627357494035\n",
      "Iteration: 6342, training loss: 0.3792593419120709\n",
      "Iteration: 6343, training loss: 0.3792559487916114\n",
      "Iteration: 6344, training loss: 0.3792525563877484\n",
      "Iteration: 6345, training loss: 0.37924916470020575\n",
      "Iteration: 6346, training loss: 0.3792457737287071\n",
      "Iteration: 6347, training loss: 0.37924238347297656\n",
      "Iteration: 6348, training loss: 0.37923899393273824\n",
      "Iteration: 6349, training loss: 0.3792356051077164\n",
      "Iteration: 6350, training loss: 0.37923221699763554\n",
      "Iteration: 6351, training loss: 0.3792288296022202\n",
      "Iteration: 6352, training loss: 0.37922544292119525\n",
      "Iteration: 6353, training loss: 0.3792220569542854\n",
      "Iteration: 6354, training loss: 0.37921867170121576\n",
      "Iteration: 6355, training loss: 0.3792152871617117\n",
      "Iteration: 6356, training loss: 0.37921190333549837\n",
      "Iteration: 6357, training loss: 0.37920852022230156\n",
      "Iteration: 6358, training loss: 0.3792051378218466\n",
      "Iteration: 6359, training loss: 0.37920175613385954\n",
      "Iteration: 6360, training loss: 0.37919837515806615\n",
      "Iteration: 6361, training loss: 0.37919499489419284\n",
      "Iteration: 6362, training loss: 0.37919161534196555\n",
      "Iteration: 6363, training loss: 0.37918823650111105\n",
      "Iteration: 6364, training loss: 0.3791848583713556\n",
      "Iteration: 6365, training loss: 0.3791814809524261\n",
      "Iteration: 6366, training loss: 0.3791781042440494\n",
      "Iteration: 6367, training loss: 0.3791747282459526\n",
      "Iteration: 6368, training loss: 0.3791713529578627\n",
      "Iteration: 6369, training loss: 0.3791679783795072\n",
      "Iteration: 6370, training loss: 0.37916460451061346\n",
      "Iteration: 6371, training loss: 0.3791612313509092\n",
      "Iteration: 6372, training loss: 0.3791578589001221\n",
      "Iteration: 6373, training loss: 0.3791544871579803\n",
      "Iteration: 6374, training loss: 0.3791511161242117\n",
      "Iteration: 6375, training loss: 0.37914774579854454\n",
      "Iteration: 6376, training loss: 0.37914437618070723\n",
      "Iteration: 6377, training loss: 0.3791410072704284\n",
      "Iteration: 6378, training loss: 0.3791376390674366\n",
      "Iteration: 6379, training loss: 0.37913427157146073\n",
      "Iteration: 6380, training loss: 0.3791309047822298\n",
      "Iteration: 6381, training loss: 0.37912753869947285\n",
      "Iteration: 6382, training loss: 0.3791241733229193\n",
      "Iteration: 6383, training loss: 0.37912080865229847\n",
      "Iteration: 6384, training loss: 0.37911744468733993\n",
      "Iteration: 6385, training loss: 0.37911408142777353\n",
      "Iteration: 6386, training loss: 0.3791107188733291\n",
      "Iteration: 6387, training loss: 0.37910735702373655\n",
      "Iteration: 6388, training loss: 0.37910399587872634\n",
      "Iteration: 6389, training loss: 0.37910063543802847\n",
      "Iteration: 6390, training loss: 0.3790972757013736\n",
      "Iteration: 6391, training loss: 0.37909391666849224\n",
      "Iteration: 6392, training loss: 0.3790905583391153\n",
      "Iteration: 6393, training loss: 0.37908720071297364\n",
      "Iteration: 6394, training loss: 0.3790838437897984\n",
      "Iteration: 6395, training loss: 0.3790804875693206\n",
      "Iteration: 6396, training loss: 0.37907713205127186\n",
      "Iteration: 6397, training loss: 0.37907377723538355\n",
      "Iteration: 6398, training loss: 0.37907042312138733\n",
      "Iteration: 6399, training loss: 0.37906706970901505\n",
      "Iteration: 6400, training loss: 0.3790637169979986\n",
      "Iteration: 6401, training loss: 0.3790603649880703\n",
      "Iteration: 6402, training loss: 0.37905701367896205\n",
      "Iteration: 6403, training loss: 0.3790536630704065\n",
      "Iteration: 6404, training loss: 0.37905031316213617\n",
      "Iteration: 6405, training loss: 0.3790469639538837\n",
      "Iteration: 6406, training loss: 0.37904361544538195\n",
      "Iteration: 6407, training loss: 0.37904026763636395\n",
      "Iteration: 6408, training loss: 0.3790369205265628\n",
      "Iteration: 6409, training loss: 0.3790335741157117\n",
      "Iteration: 6410, training loss: 0.3790302284035442\n",
      "Iteration: 6411, training loss: 0.3790268833897939\n",
      "Iteration: 6412, training loss: 0.3790235390741944\n",
      "Iteration: 6413, training loss: 0.37902019545647964\n",
      "Iteration: 6414, training loss: 0.3790168525363837\n",
      "Iteration: 6415, training loss: 0.3790135103136405\n",
      "Iteration: 6416, training loss: 0.3790101687879846\n",
      "Iteration: 6417, training loss: 0.37900682795915036\n",
      "Iteration: 6418, training loss: 0.3790034878268724\n",
      "Iteration: 6419, training loss: 0.3790001483908854\n",
      "Iteration: 6420, training loss: 0.3789968096509243\n",
      "Iteration: 6421, training loss: 0.37899347160672414\n",
      "Iteration: 6422, training loss: 0.37899013425802014\n",
      "Iteration: 6423, training loss: 0.37898679760454745\n",
      "Iteration: 6424, training loss: 0.37898346164604185\n",
      "Iteration: 6425, training loss: 0.3789801263822387\n",
      "Iteration: 6426, training loss: 0.3789767918128738\n",
      "Iteration: 6427, training loss: 0.3789734579376832\n",
      "Iteration: 6428, training loss: 0.37897012475640274\n",
      "Iteration: 6429, training loss: 0.3789667922687687\n",
      "Iteration: 6430, training loss: 0.3789634604745176\n",
      "Iteration: 6431, training loss: 0.3789601293733857\n",
      "Iteration: 6432, training loss: 0.3789567989651097\n",
      "Iteration: 6433, training loss: 0.37895346924942636\n",
      "Iteration: 6434, training loss: 0.37895014022607265\n",
      "Iteration: 6435, training loss: 0.3789468118947855\n",
      "Iteration: 6436, training loss: 0.37894348425530233\n",
      "Iteration: 6437, training loss: 0.37894015730736036\n",
      "Iteration: 6438, training loss: 0.3789368310506971\n",
      "Iteration: 6439, training loss: 0.37893350548505017\n",
      "Iteration: 6440, training loss: 0.3789301806101573\n",
      "Iteration: 6441, training loss: 0.37892685642575663\n",
      "Iteration: 6442, training loss: 0.37892353293158604\n",
      "Iteration: 6443, training loss: 0.3789202101273838\n",
      "Iteration: 6444, training loss: 0.3789168880128883\n",
      "Iteration: 6445, training loss: 0.37891356658783787\n",
      "Iteration: 6446, training loss: 0.3789102458519714\n",
      "Iteration: 6447, training loss: 0.3789069258050275\n",
      "Iteration: 6448, training loss: 0.37890360644674514\n",
      "Iteration: 6449, training loss: 0.3789002877768634\n",
      "Iteration: 6450, training loss: 0.3788969697951215\n",
      "Iteration: 6451, training loss: 0.37889365250125884\n",
      "Iteration: 6452, training loss: 0.37889033589501486\n",
      "Iteration: 6453, training loss: 0.37888701997612917\n",
      "Iteration: 6454, training loss: 0.37888370474434163\n",
      "Iteration: 6455, training loss: 0.3788803901993921\n",
      "Iteration: 6456, training loss: 0.37887707634102075\n",
      "Iteration: 6457, training loss: 0.37887376316896765\n",
      "Iteration: 6458, training loss: 0.37887045068297326\n",
      "Iteration: 6459, training loss: 0.37886713888277795\n",
      "Iteration: 6460, training loss: 0.37886382776812255\n",
      "Iteration: 6461, training loss: 0.3788605173387477\n",
      "Iteration: 6462, training loss: 0.3788572075943943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6463, training loss: 0.3788538985348034\n",
      "Iteration: 6464, training loss: 0.37885059015971634\n",
      "Iteration: 6465, training loss: 0.37884728246887434\n",
      "Iteration: 6466, training loss: 0.3788439754620189\n",
      "Iteration: 6467, training loss: 0.37884066913889164\n",
      "Iteration: 6468, training loss: 0.37883736349923447\n",
      "Iteration: 6469, training loss: 0.3788340585427891\n",
      "Iteration: 6470, training loss: 0.37883075426929763\n",
      "Iteration: 6471, training loss: 0.37882745067850226\n",
      "Iteration: 6472, training loss: 0.37882414777014534\n",
      "Iteration: 6473, training loss: 0.3788208455439693\n",
      "Iteration: 6474, training loss: 0.3788175439997168\n",
      "Iteration: 6475, training loss: 0.3788142431371306\n",
      "Iteration: 6476, training loss: 0.3788109429559534\n",
      "Iteration: 6477, training loss: 0.37880764345592843\n",
      "Iteration: 6478, training loss: 0.3788043446367988\n",
      "Iteration: 6479, training loss: 0.3788010464983079\n",
      "Iteration: 6480, training loss: 0.37879774904019897\n",
      "Iteration: 6481, training loss: 0.37879445226221575\n",
      "Iteration: 6482, training loss: 0.37879115616410197\n",
      "Iteration: 6483, training loss: 0.37878786074560156\n",
      "Iteration: 6484, training loss: 0.3787845660064584\n",
      "Iteration: 6485, training loss: 0.37878127194641664\n",
      "Iteration: 6486, training loss: 0.37877797856522055\n",
      "Iteration: 6487, training loss: 0.37877468586261465\n",
      "Iteration: 6488, training loss: 0.3787713938383434\n",
      "Iteration: 6489, training loss: 0.37876810249215165\n",
      "Iteration: 6490, training loss: 0.3787648118237841\n",
      "Iteration: 6491, training loss: 0.37876152183298567\n",
      "Iteration: 6492, training loss: 0.37875823251950175\n",
      "Iteration: 6493, training loss: 0.37875494388307734\n",
      "Iteration: 6494, training loss: 0.37875165592345794\n",
      "Iteration: 6495, training loss: 0.378748368640389\n",
      "Iteration: 6496, training loss: 0.37874508203361634\n",
      "Iteration: 6497, training loss: 0.37874179610288566\n",
      "Iteration: 6498, training loss: 0.378738510847943\n",
      "Iteration: 6499, training loss: 0.3787352262685343\n",
      "Iteration: 6500, training loss: 0.3787319423644059\n",
      "Iteration: 6501, training loss: 0.37872865913530407\n",
      "Iteration: 6502, training loss: 0.37872537658097544\n",
      "Iteration: 6503, training loss: 0.37872209470116647\n",
      "Iteration: 6504, training loss: 0.37871881349562414\n",
      "Iteration: 6505, training loss: 0.3787155329640951\n",
      "Iteration: 6506, training loss: 0.3787122531063266\n",
      "Iteration: 6507, training loss: 0.3787089739220659\n",
      "Iteration: 6508, training loss: 0.37870569541106014\n",
      "Iteration: 6509, training loss: 0.37870241757305684\n",
      "Iteration: 6510, training loss: 0.37869914040780356\n",
      "Iteration: 6511, training loss: 0.37869586391504806\n",
      "Iteration: 6512, training loss: 0.37869258809453826\n",
      "Iteration: 6513, training loss: 0.37868931294602204\n",
      "Iteration: 6514, training loss: 0.3786860384692477\n",
      "Iteration: 6515, training loss: 0.3786827646639634\n",
      "Iteration: 6516, training loss: 0.37867949152991764\n",
      "Iteration: 6517, training loss: 0.3786762190668589\n",
      "Iteration: 6518, training loss: 0.378672947274536\n",
      "Iteration: 6519, training loss: 0.3786696761526976\n",
      "Iteration: 6520, training loss: 0.3786664057010927\n",
      "Iteration: 6521, training loss: 0.3786631359194705\n",
      "Iteration: 6522, training loss: 0.37865986680758007\n",
      "Iteration: 6523, training loss: 0.37865659836517085\n",
      "Iteration: 6524, training loss: 0.3786533305919924\n",
      "Iteration: 6525, training loss: 0.37865006348779434\n",
      "Iteration: 6526, training loss: 0.3786467970523264\n",
      "Iteration: 6527, training loss: 0.37864353128533856\n",
      "Iteration: 6528, training loss: 0.3786402661865807\n",
      "Iteration: 6529, training loss: 0.37863700175580317\n",
      "Iteration: 6530, training loss: 0.37863373799275624\n",
      "Iteration: 6531, training loss: 0.37863047489719037\n",
      "Iteration: 6532, training loss: 0.37862721246885606\n",
      "Iteration: 6533, training loss: 0.37862395070750415\n",
      "Iteration: 6534, training loss: 0.37862068961288536\n",
      "Iteration: 6535, training loss: 0.37861742918475083\n",
      "Iteration: 6536, training loss: 0.37861416942285164\n",
      "Iteration: 6537, training loss: 0.37861091032693894\n",
      "Iteration: 6538, training loss: 0.3786076518967642\n",
      "Iteration: 6539, training loss: 0.3786043941320789\n",
      "Iteration: 6540, training loss: 0.37860113703263487\n",
      "Iteration: 6541, training loss: 0.3785978805981837\n",
      "Iteration: 6542, training loss: 0.3785946248284775\n",
      "Iteration: 6543, training loss: 0.3785913697232681\n",
      "Iteration: 6544, training loss: 0.3785881152823079\n",
      "Iteration: 6545, training loss: 0.37858486150534915\n",
      "Iteration: 6546, training loss: 0.37858160839214433\n",
      "Iteration: 6547, training loss: 0.37857835594244593\n",
      "Iteration: 6548, training loss: 0.37857510415600687\n",
      "Iteration: 6549, training loss: 0.3785718530325799\n",
      "Iteration: 6550, training loss: 0.37856860257191793\n",
      "Iteration: 6551, training loss: 0.3785653527737743\n",
      "Iteration: 6552, training loss: 0.37856210363790216\n",
      "Iteration: 6553, training loss: 0.3785588551640549\n",
      "Iteration: 6554, training loss: 0.37855560735198607\n",
      "Iteration: 6555, training loss: 0.37855236020144933\n",
      "Iteration: 6556, training loss: 0.37854911371219846\n",
      "Iteration: 6557, training loss: 0.37854586788398736\n",
      "Iteration: 6558, training loss: 0.37854262271657013\n",
      "Iteration: 6559, training loss: 0.378539378209701\n",
      "Iteration: 6560, training loss: 0.37853613436313416\n",
      "Iteration: 6561, training loss: 0.37853289117662425\n",
      "Iteration: 6562, training loss: 0.3785296486499257\n",
      "Iteration: 6563, training loss: 0.37852640678279326\n",
      "Iteration: 6564, training loss: 0.37852316557498183\n",
      "Iteration: 6565, training loss: 0.3785199250262465\n",
      "Iteration: 6566, training loss: 0.3785166851363421\n",
      "Iteration: 6567, training loss: 0.3785134459050241\n",
      "Iteration: 6568, training loss: 0.37851020733204777\n",
      "Iteration: 6569, training loss: 0.37850696941716877\n",
      "Iteration: 6570, training loss: 0.37850373216014255\n",
      "Iteration: 6571, training loss: 0.378500495560725\n",
      "Iteration: 6572, training loss: 0.378497259618672\n",
      "Iteration: 6573, training loss: 0.3784940243337395\n",
      "Iteration: 6574, training loss: 0.3784907897056839\n",
      "Iteration: 6575, training loss: 0.37848755573426135\n",
      "Iteration: 6576, training loss: 0.37848432241922814\n",
      "Iteration: 6577, training loss: 0.3784810897603411\n",
      "Iteration: 6578, training loss: 0.37847785775735665\n",
      "Iteration: 6579, training loss: 0.3784746264100318\n",
      "Iteration: 6580, training loss: 0.37847139571812344\n",
      "Iteration: 6581, training loss: 0.37846816568138864\n",
      "Iteration: 6582, training loss: 0.37846493629958466\n",
      "Iteration: 6583, training loss: 0.37846170757246883\n",
      "Iteration: 6584, training loss: 0.3784584794997986\n",
      "Iteration: 6585, training loss: 0.3784552520813315\n",
      "Iteration: 6586, training loss: 0.37845202531682554\n",
      "Iteration: 6587, training loss: 0.37844879920603813\n",
      "Iteration: 6588, training loss: 0.37844557374872756\n",
      "Iteration: 6589, training loss: 0.37844234894465206\n",
      "Iteration: 6590, training loss: 0.37843912479356956\n",
      "Iteration: 6591, training loss: 0.37843590129523874\n",
      "Iteration: 6592, training loss: 0.3784326784494178\n",
      "Iteration: 6593, training loss: 0.3784294562558657\n",
      "Iteration: 6594, training loss: 0.378426234714341\n",
      "Iteration: 6595, training loss: 0.3784230138246027\n",
      "Iteration: 6596, training loss: 0.3784197935864097\n",
      "Iteration: 6597, training loss: 0.3784165739995214\n",
      "Iteration: 6598, training loss: 0.3784133550636968\n",
      "Iteration: 6599, training loss: 0.3784101367786955\n",
      "Iteration: 6600, training loss: 0.378406919144277\n",
      "Iteration: 6601, training loss: 0.3784037021602009\n",
      "Iteration: 6602, training loss: 0.37840048582622704\n",
      "Iteration: 6603, training loss: 0.3783972701421155\n",
      "Iteration: 6604, training loss: 0.3783940551076261\n",
      "Iteration: 6605, training loss: 0.3783908407225191\n",
      "Iteration: 6606, training loss: 0.3783876269865549\n",
      "Iteration: 6607, training loss: 0.3783844138994939\n",
      "Iteration: 6608, training loss: 0.37838120146109666\n",
      "Iteration: 6609, training loss: 0.37837798967112374\n",
      "Iteration: 6610, training loss: 0.37837477852933626\n",
      "Iteration: 6611, training loss: 0.378371568035495\n",
      "Iteration: 6612, training loss: 0.378368358189361\n",
      "Iteration: 6613, training loss: 0.37836514899069557\n",
      "Iteration: 6614, training loss: 0.37836194043925997\n",
      "Iteration: 6615, training loss: 0.37835873253481567\n",
      "Iteration: 6616, training loss: 0.37835552527712435\n",
      "Iteration: 6617, training loss: 0.37835231866594754\n",
      "Iteration: 6618, training loss: 0.3783491127010473\n",
      "Iteration: 6619, training loss: 0.37834590738218543\n",
      "Iteration: 6620, training loss: 0.37834270270912423\n",
      "Iteration: 6621, training loss: 0.37833949868162575\n",
      "Iteration: 6622, training loss: 0.37833629529945234\n",
      "Iteration: 6623, training loss: 0.37833309256236664\n",
      "Iteration: 6624, training loss: 0.3783298904701311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6625, training loss: 0.3783266890225085\n",
      "Iteration: 6626, training loss: 0.37832348821926176\n",
      "Iteration: 6627, training loss: 0.3783202880601538\n",
      "Iteration: 6628, training loss: 0.3783170885449478\n",
      "Iteration: 6629, training loss: 0.37831388967340684\n",
      "Iteration: 6630, training loss: 0.37831069144529444\n",
      "Iteration: 6631, training loss: 0.3783074938603741\n",
      "Iteration: 6632, training loss: 0.3783042969184094\n",
      "Iteration: 6633, training loss: 0.3783011006191639\n",
      "Iteration: 6634, training loss: 0.37829790496240173\n",
      "Iteration: 6635, training loss: 0.3782947099478869\n",
      "Iteration: 6636, training loss: 0.37829151557538326\n",
      "Iteration: 6637, training loss: 0.3782883218446554\n",
      "Iteration: 6638, training loss: 0.3782851287554674\n",
      "Iteration: 6639, training loss: 0.3782819363075838\n",
      "Iteration: 6640, training loss: 0.37827874450076937\n",
      "Iteration: 6641, training loss: 0.37827555333478874\n",
      "Iteration: 6642, training loss: 0.3782723628094069\n",
      "Iteration: 6643, training loss: 0.3782691729243887\n",
      "Iteration: 6644, training loss: 0.3782659836794993\n",
      "Iteration: 6645, training loss: 0.37826279507450405\n",
      "Iteration: 6646, training loss: 0.3782596071091683\n",
      "Iteration: 6647, training loss: 0.37825641978325736\n",
      "Iteration: 6648, training loss: 0.37825323309653713\n",
      "Iteration: 6649, training loss: 0.37825004704877313\n",
      "Iteration: 6650, training loss: 0.3782468616397313\n",
      "Iteration: 6651, training loss: 0.37824367686917776\n",
      "Iteration: 6652, training loss: 0.37824049273687843\n",
      "Iteration: 6653, training loss: 0.37823730924259963\n",
      "Iteration: 6654, training loss: 0.37823412638610787\n",
      "Iteration: 6655, training loss: 0.3782309441671694\n",
      "Iteration: 6656, training loss: 0.378227762585551\n",
      "Iteration: 6657, training loss: 0.37822458164101935\n",
      "Iteration: 6658, training loss: 0.3782214013333413\n",
      "Iteration: 6659, training loss: 0.37821822166228397\n",
      "Iteration: 6660, training loss: 0.37821504262761435\n",
      "Iteration: 6661, training loss: 0.3782118642290996\n",
      "Iteration: 6662, training loss: 0.37820868646650724\n",
      "Iteration: 6663, training loss: 0.37820550933960456\n",
      "Iteration: 6664, training loss: 0.3782023328481594\n",
      "Iteration: 6665, training loss: 0.37819915699193934\n",
      "Iteration: 6666, training loss: 0.3781959817707124\n",
      "Iteration: 6667, training loss: 0.37819280718424625\n",
      "Iteration: 6668, training loss: 0.3781896332323092\n",
      "Iteration: 6669, training loss: 0.37818645991466954\n",
      "Iteration: 6670, training loss: 0.37818328723109534\n",
      "Iteration: 6671, training loss: 0.3781801151813553\n",
      "Iteration: 6672, training loss: 0.37817694376521793\n",
      "Iteration: 6673, training loss: 0.3781737729824519\n",
      "Iteration: 6674, training loss: 0.37817060283282616\n",
      "Iteration: 6675, training loss: 0.37816743331610964\n",
      "Iteration: 6676, training loss: 0.3781642644320713\n",
      "Iteration: 6677, training loss: 0.37816109618048044\n",
      "Iteration: 6678, training loss: 0.3781579285611063\n",
      "Iteration: 6679, training loss: 0.37815476157371847\n",
      "Iteration: 6680, training loss: 0.37815159521808633\n",
      "Iteration: 6681, training loss: 0.3781484294939798\n",
      "Iteration: 6682, training loss: 0.37814526440116847\n",
      "Iteration: 6683, training loss: 0.37814209993942244\n",
      "Iteration: 6684, training loss: 0.3781389361085116\n",
      "Iteration: 6685, training loss: 0.3781357729082062\n",
      "Iteration: 6686, training loss: 0.3781326103382766\n",
      "Iteration: 6687, training loss: 0.3781294483984932\n",
      "Iteration: 6688, training loss: 0.37812628708862644\n",
      "Iteration: 6689, training loss: 0.37812312640844703\n",
      "Iteration: 6690, training loss: 0.3781199663577259\n",
      "Iteration: 6691, training loss: 0.37811680693623373\n",
      "Iteration: 6692, training loss: 0.3781136481437417\n",
      "Iteration: 6693, training loss: 0.3781104899800207\n",
      "Iteration: 6694, training loss: 0.3781073324448424\n",
      "Iteration: 6695, training loss: 0.3781041755379779\n",
      "Iteration: 6696, training loss: 0.3781010192591988\n",
      "Iteration: 6697, training loss: 0.37809786360827663\n",
      "Iteration: 6698, training loss: 0.37809470858498334\n",
      "Iteration: 6699, training loss: 0.3780915541890906\n",
      "Iteration: 6700, training loss: 0.37808840042037056\n",
      "Iteration: 6701, training loss: 0.3780852472785952\n",
      "Iteration: 6702, training loss: 0.3780820947635368\n",
      "Iteration: 6703, training loss: 0.37807894287496774\n",
      "Iteration: 6704, training loss: 0.37807579161266036\n",
      "Iteration: 6705, training loss: 0.3780726409763875\n",
      "Iteration: 6706, training loss: 0.3780694909659217\n",
      "Iteration: 6707, training loss: 0.3780663415810357\n",
      "Iteration: 6708, training loss: 0.3780631928215026\n",
      "Iteration: 6709, training loss: 0.3780600446870954\n",
      "Iteration: 6710, training loss: 0.3780568971775874\n",
      "Iteration: 6711, training loss: 0.37805375029275184\n",
      "Iteration: 6712, training loss: 0.37805060403236207\n",
      "Iteration: 6713, training loss: 0.3780474583961917\n",
      "Iteration: 6714, training loss: 0.3780443133840144\n",
      "Iteration: 6715, training loss: 0.3780411689956039\n",
      "Iteration: 6716, training loss: 0.37803802523073426\n",
      "Iteration: 6717, training loss: 0.37803488208917935\n",
      "Iteration: 6718, training loss: 0.37803173957071345\n",
      "Iteration: 6719, training loss: 0.37802859767511066\n",
      "Iteration: 6720, training loss: 0.37802545640214547\n",
      "Iteration: 6721, training loss: 0.3780223157515923\n",
      "Iteration: 6722, training loss: 0.37801917572322585\n",
      "Iteration: 6723, training loss: 0.3780160363168208\n",
      "Iteration: 6724, training loss: 0.378012897532152\n",
      "Iteration: 6725, training loss: 0.3780097593689944\n",
      "Iteration: 6726, training loss: 0.3780066218271233\n",
      "Iteration: 6727, training loss: 0.3780034849063136\n",
      "Iteration: 6728, training loss: 0.3780003486063409\n",
      "Iteration: 6729, training loss: 0.3779972129269804\n",
      "Iteration: 6730, training loss: 0.37799407786800776\n",
      "Iteration: 6731, training loss: 0.37799094342919876\n",
      "Iteration: 6732, training loss: 0.3779878096103291\n",
      "Iteration: 6733, training loss: 0.3779846764111748\n",
      "Iteration: 6734, training loss: 0.3779815438315117\n",
      "Iteration: 6735, training loss: 0.37797841187111614\n",
      "Iteration: 6736, training loss: 0.37797528052976426\n",
      "Iteration: 6737, training loss: 0.37797214980723254\n",
      "Iteration: 6738, training loss: 0.3779690197032974\n",
      "Iteration: 6739, training loss: 0.3779658902177357\n",
      "Iteration: 6740, training loss: 0.37796276135032375\n",
      "Iteration: 6741, training loss: 0.37795963310083863\n",
      "Iteration: 6742, training loss: 0.3779565054690575\n",
      "Iteration: 6743, training loss: 0.37795337845475724\n",
      "Iteration: 6744, training loss: 0.37795025205771504\n",
      "Iteration: 6745, training loss: 0.37794712627770827\n",
      "Iteration: 6746, training loss: 0.37794400111451437\n",
      "Iteration: 6747, training loss: 0.377940876567911\n",
      "Iteration: 6748, training loss: 0.3779377526376757\n",
      "Iteration: 6749, training loss: 0.37793462932358624\n",
      "Iteration: 6750, training loss: 0.3779315066254207\n",
      "Iteration: 6751, training loss: 0.3779283845429569\n",
      "Iteration: 6752, training loss: 0.37792526307597313\n",
      "Iteration: 6753, training loss: 0.3779221422242475\n",
      "Iteration: 6754, training loss: 0.3779190219875585\n",
      "Iteration: 6755, training loss: 0.3779159023656846\n",
      "Iteration: 6756, training loss: 0.37791278335840434\n",
      "Iteration: 6757, training loss: 0.3779096649654965\n",
      "Iteration: 6758, training loss: 0.37790654718674\n",
      "Iteration: 6759, training loss: 0.3779034300219136\n",
      "Iteration: 6760, training loss: 0.37790031347079645\n",
      "Iteration: 6761, training loss: 0.3778971975331678\n",
      "Iteration: 6762, training loss: 0.3778940822088069\n",
      "Iteration: 6763, training loss: 0.37789096749749307\n",
      "Iteration: 6764, training loss: 0.377887853399006\n",
      "Iteration: 6765, training loss: 0.37788473991312516\n",
      "Iteration: 6766, training loss: 0.37788162703963046\n",
      "Iteration: 6767, training loss: 0.3778785147783017\n",
      "Iteration: 6768, training loss: 0.3778754031289189\n",
      "Iteration: 6769, training loss: 0.37787229209126216\n",
      "Iteration: 6770, training loss: 0.3778691816651117\n",
      "Iteration: 6771, training loss: 0.3778660718502478\n",
      "Iteration: 6772, training loss: 0.3778629626464509\n",
      "Iteration: 6773, training loss: 0.3778598540535017\n",
      "Iteration: 6774, training loss: 0.37785674607118075\n",
      "Iteration: 6775, training loss: 0.3778536386992689\n",
      "Iteration: 6776, training loss: 0.3778505319375471\n",
      "Iteration: 6777, training loss: 0.3778474257857962\n",
      "Iteration: 6778, training loss: 0.37784432024379755\n",
      "Iteration: 6779, training loss: 0.3778412153113322\n",
      "Iteration: 6780, training loss: 0.37783811098818165\n",
      "Iteration: 6781, training loss: 0.3778350072741273\n",
      "Iteration: 6782, training loss: 0.3778319041689508\n",
      "Iteration: 6783, training loss: 0.3778288016724338\n",
      "Iteration: 6784, training loss: 0.3778256997843582\n",
      "Iteration: 6785, training loss: 0.3778225985045059\n",
      "Iteration: 6786, training loss: 0.37781949783265883\n",
      "Iteration: 6787, training loss: 0.37781639776859927\n",
      "Iteration: 6788, training loss: 0.3778132983121096\n",
      "Iteration: 6789, training loss: 0.37781019946297195\n",
      "Iteration: 6790, training loss: 0.377807101220969\n",
      "Iteration: 6791, training loss: 0.37780400358588334\n",
      "Iteration: 6792, training loss: 0.37780090655749754\n",
      "Iteration: 6793, training loss: 0.37779781013559466\n",
      "Iteration: 6794, training loss: 0.3777947143199576\n",
      "Iteration: 6795, training loss: 0.3777916191103694\n",
      "Iteration: 6796, training loss: 0.3777885245066132\n",
      "Iteration: 6797, training loss: 0.37778543050847224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6798, training loss: 0.3777823371157301\n",
      "Iteration: 6799, training loss: 0.3777792443281702\n",
      "Iteration: 6800, training loss: 0.3777761521455761\n",
      "Iteration: 6801, training loss: 0.3777730605677317\n",
      "Iteration: 6802, training loss: 0.3777699695944207\n",
      "Iteration: 6803, training loss: 0.3777668792254272\n",
      "Iteration: 6804, training loss: 0.3777637894605352\n",
      "Iteration: 6805, training loss: 0.37776070029952885\n",
      "Iteration: 6806, training loss: 0.3777576117421925\n",
      "Iteration: 6807, training loss: 0.3777545237883107\n",
      "Iteration: 6808, training loss: 0.3777514364376678\n",
      "Iteration: 6809, training loss: 0.3777483496900485\n",
      "Iteration: 6810, training loss: 0.37774526354523746\n",
      "Iteration: 6811, training loss: 0.37774217800301974\n",
      "Iteration: 6812, training loss: 0.3777390930631802\n",
      "Iteration: 6813, training loss: 0.3777360087255039\n",
      "Iteration: 6814, training loss: 0.37773292498977606\n",
      "Iteration: 6815, training loss: 0.37772984185578207\n",
      "Iteration: 6816, training loss: 0.37772675932330724\n",
      "Iteration: 6817, training loss: 0.3777236773921371\n",
      "Iteration: 6818, training loss: 0.3777205960620573\n",
      "Iteration: 6819, training loss: 0.3777175153328538\n",
      "Iteration: 6820, training loss: 0.37771443520431225\n",
      "Iteration: 6821, training loss: 0.3777113556762187\n",
      "Iteration: 6822, training loss: 0.37770827674835916\n",
      "Iteration: 6823, training loss: 0.37770519842051986\n",
      "Iteration: 6824, training loss: 0.37770212069248715\n",
      "Iteration: 6825, training loss: 0.37769904356404754\n",
      "Iteration: 6826, training loss: 0.3776959670349874\n",
      "Iteration: 6827, training loss: 0.3776928911050934\n",
      "Iteration: 6828, training loss: 0.3776898157741524\n",
      "Iteration: 6829, training loss: 0.3776867410419511\n",
      "Iteration: 6830, training loss: 0.3776836669082767\n",
      "Iteration: 6831, training loss: 0.37768059337291604\n",
      "Iteration: 6832, training loss: 0.3776775204356564\n",
      "Iteration: 6833, training loss: 0.37767444809628525\n",
      "Iteration: 6834, training loss: 0.3776713763545898\n",
      "Iteration: 6835, training loss: 0.3776683052103576\n",
      "Iteration: 6836, training loss: 0.3776652346633763\n",
      "Iteration: 6837, training loss: 0.37766216471343383\n",
      "Iteration: 6838, training loss: 0.3776590953603177\n",
      "Iteration: 6839, training loss: 0.37765602660381614\n",
      "Iteration: 6840, training loss: 0.37765295844371716\n",
      "Iteration: 6841, training loss: 0.3776498908798089\n",
      "Iteration: 6842, training loss: 0.37764682391187965\n",
      "Iteration: 6843, training loss: 0.37764375753971785\n",
      "Iteration: 6844, training loss: 0.377640691763112\n",
      "Iteration: 6845, training loss: 0.3776376265818508\n",
      "Iteration: 6846, training loss: 0.3776345619957228\n",
      "Iteration: 6847, training loss: 0.377631498004517\n",
      "Iteration: 6848, training loss: 0.37762843460802226\n",
      "Iteration: 6849, training loss: 0.37762537180602773\n",
      "Iteration: 6850, training loss: 0.3776223095983225\n",
      "Iteration: 6851, training loss: 0.37761924798469576\n",
      "Iteration: 6852, training loss: 0.3776161869649371\n",
      "Iteration: 6853, training loss: 0.37761312653883594\n",
      "Iteration: 6854, training loss: 0.3776100667061818\n",
      "Iteration: 6855, training loss: 0.3776070074667645\n",
      "Iteration: 6856, training loss: 0.3776039488203737\n",
      "Iteration: 6857, training loss: 0.3776008907667994\n",
      "Iteration: 6858, training loss: 0.37759783330583174\n",
      "Iteration: 6859, training loss: 0.3775947764372609\n",
      "Iteration: 6860, training loss: 0.37759172016087683\n",
      "Iteration: 6861, training loss: 0.3775886644764702\n",
      "Iteration: 6862, training loss: 0.3775856093838314\n",
      "Iteration: 6863, training loss: 0.37758255488275083\n",
      "Iteration: 6864, training loss: 0.37757950097301934\n",
      "Iteration: 6865, training loss: 0.37757644765442777\n",
      "Iteration: 6866, training loss: 0.37757339492676695\n",
      "Iteration: 6867, training loss: 0.3775703427898278\n",
      "Iteration: 6868, training loss: 0.3775672912434016\n",
      "Iteration: 6869, training loss: 0.37756424028727953\n",
      "Iteration: 6870, training loss: 0.37756118992125276\n",
      "Iteration: 6871, training loss: 0.3775581401451129\n",
      "Iteration: 6872, training loss: 0.3775550909586514\n",
      "Iteration: 6873, training loss: 0.37755204236166007\n",
      "Iteration: 6874, training loss: 0.3775489943539305\n",
      "Iteration: 6875, training loss: 0.3775459469352546\n",
      "Iteration: 6876, training loss: 0.3775429001054244\n",
      "Iteration: 6877, training loss: 0.3775398538642319\n",
      "Iteration: 6878, training loss: 0.3775368082114694\n",
      "Iteration: 6879, training loss: 0.3775337631469291\n",
      "Iteration: 6880, training loss: 0.37753071867040344\n",
      "Iteration: 6881, training loss: 0.3775276747816849\n",
      "Iteration: 6882, training loss: 0.37752463148056614\n",
      "Iteration: 6883, training loss: 0.3775215887668399\n",
      "Iteration: 6884, training loss: 0.3775185466402989\n",
      "Iteration: 6885, training loss: 0.37751550510073617\n",
      "Iteration: 6886, training loss: 0.3775124641479447\n",
      "Iteration: 6887, training loss: 0.37750942378171765\n",
      "Iteration: 6888, training loss: 0.3775063840018484\n",
      "Iteration: 6889, training loss: 0.37750334480813\n",
      "Iteration: 6890, training loss: 0.3775003062003563\n",
      "Iteration: 6891, training loss: 0.37749726817832063\n",
      "Iteration: 6892, training loss: 0.37749423074181676\n",
      "Iteration: 6893, training loss: 0.37749119389063834\n",
      "Iteration: 6894, training loss: 0.37748815762457943\n",
      "Iteration: 6895, training loss: 0.37748512194343403\n",
      "Iteration: 6896, training loss: 0.3774820868469961\n",
      "Iteration: 6897, training loss: 0.3774790523350599\n",
      "Iteration: 6898, training loss: 0.37747601840741984\n",
      "Iteration: 6899, training loss: 0.3774729850638703\n",
      "Iteration: 6900, training loss: 0.3774699523042055\n",
      "Iteration: 6901, training loss: 0.37746692012822064\n",
      "Iteration: 6902, training loss: 0.37746388853571\n",
      "Iteration: 6903, training loss: 0.37746085752646863\n",
      "Iteration: 6904, training loss: 0.37745782710029135\n",
      "Iteration: 6905, training loss: 0.37745479725697334\n",
      "Iteration: 6906, training loss: 0.37745176799630964\n",
      "Iteration: 6907, training loss: 0.37744873931809564\n",
      "Iteration: 6908, training loss: 0.37744571122212656\n",
      "Iteration: 6909, training loss: 0.377442683708198\n",
      "Iteration: 6910, training loss: 0.3774396567761054\n",
      "Iteration: 6911, training loss: 0.3774366304256447\n",
      "Iteration: 6912, training loss: 0.37743360465661124\n",
      "Iteration: 6913, training loss: 0.3774305794688014\n",
      "Iteration: 6914, training loss: 0.3774275548620108\n",
      "Iteration: 6915, training loss: 0.37742453083603567\n",
      "Iteration: 6916, training loss: 0.3774215073906723\n",
      "Iteration: 6917, training loss: 0.3774184845257168\n",
      "Iteration: 6918, training loss: 0.37741546224096584\n",
      "Iteration: 6919, training loss: 0.37741244053621575\n",
      "Iteration: 6920, training loss: 0.37740941941126316\n",
      "Iteration: 6921, training loss: 0.37740639886590477\n",
      "Iteration: 6922, training loss: 0.37740337889993764\n",
      "Iteration: 6923, training loss: 0.3774003595131584\n",
      "Iteration: 6924, training loss: 0.3773973407053642\n",
      "Iteration: 6925, training loss: 0.37739432247635224\n",
      "Iteration: 6926, training loss: 0.3773913048259196\n",
      "Iteration: 6927, training loss: 0.37738828775386385\n",
      "Iteration: 6928, training loss: 0.3773852712599823\n",
      "Iteration: 6929, training loss: 0.3773822553440725\n",
      "Iteration: 6930, training loss: 0.3773792400059321\n",
      "Iteration: 6931, training loss: 0.37737622524535896\n",
      "Iteration: 6932, training loss: 0.37737321106215077\n",
      "Iteration: 6933, training loss: 0.37737019745610567\n",
      "Iteration: 6934, training loss: 0.37736718442702166\n",
      "Iteration: 6935, training loss: 0.3773641719746969\n",
      "Iteration: 6936, training loss: 0.37736116009892967\n",
      "Iteration: 6937, training loss: 0.3773581487995183\n",
      "Iteration: 6938, training loss: 0.3773551380762613\n",
      "Iteration: 6939, training loss: 0.37735212792895734\n",
      "Iteration: 6940, training loss: 0.3773491183574051\n",
      "Iteration: 6941, training loss: 0.3773461093614032\n",
      "Iteration: 6942, training loss: 0.3773431009407507\n",
      "Iteration: 6943, training loss: 0.37734009309524646\n",
      "Iteration: 6944, training loss: 0.37733708582468967\n",
      "Iteration: 6945, training loss: 0.37733407912887956\n",
      "Iteration: 6946, training loss: 0.3773310730076154\n",
      "Iteration: 6947, training loss: 0.3773280674606965\n",
      "Iteration: 6948, training loss: 0.3773250624879224\n",
      "Iteration: 6949, training loss: 0.3773220580890929\n",
      "Iteration: 6950, training loss: 0.37731905426400736\n",
      "Iteration: 6951, training loss: 0.3773160510124659\n",
      "Iteration: 6952, training loss: 0.3773130483342683\n",
      "Iteration: 6953, training loss: 0.3773100462292146\n",
      "Iteration: 6954, training loss: 0.3773070446971049\n",
      "Iteration: 6955, training loss: 0.37730404373773946\n",
      "Iteration: 6956, training loss: 0.3773010433509185\n",
      "Iteration: 6957, training loss: 0.37729804353644264\n",
      "Iteration: 6958, training loss: 0.3772950442941121\n",
      "Iteration: 6959, training loss: 0.37729204562372787\n",
      "Iteration: 6960, training loss: 0.37728904752509035\n",
      "Iteration: 6961, training loss: 0.37728604999800053\n",
      "Iteration: 6962, training loss: 0.37728305304225934\n",
      "Iteration: 6963, training loss: 0.3772800566576678\n",
      "Iteration: 6964, training loss: 0.37727706084402696\n",
      "Iteration: 6965, training loss: 0.37727406560113813\n",
      "Iteration: 6966, training loss: 0.3772710709288027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6967, training loss: 0.377268076826822\n",
      "Iteration: 6968, training loss: 0.37726508329499764\n",
      "Iteration: 6969, training loss: 0.37726209033313113\n",
      "Iteration: 6970, training loss: 0.37725909794102436\n",
      "Iteration: 6971, training loss: 0.37725610611847904\n",
      "Iteration: 6972, training loss: 0.3772531148652973\n",
      "Iteration: 6973, training loss: 0.3772501241812809\n",
      "Iteration: 6974, training loss: 0.3772471340662323\n",
      "Iteration: 6975, training loss: 0.3772441445199534\n",
      "Iteration: 6976, training loss: 0.3772411555422468\n",
      "Iteration: 6977, training loss: 0.37723816713291486\n",
      "Iteration: 6978, training loss: 0.37723517929176004\n",
      "Iteration: 6979, training loss: 0.37723219201858504\n",
      "Iteration: 6980, training loss: 0.3772292053131926\n",
      "Iteration: 6981, training loss: 0.3772262191753857\n",
      "Iteration: 6982, training loss: 0.377223233604967\n",
      "Iteration: 6983, training loss: 0.3772202486017397\n",
      "Iteration: 6984, training loss: 0.3772172641655071\n",
      "Iteration: 6985, training loss: 0.3772142802960721\n",
      "Iteration: 6986, training loss: 0.37721129699323824\n",
      "Iteration: 6987, training loss: 0.377208314256809\n",
      "Iteration: 6988, training loss: 0.3772053320865878\n",
      "Iteration: 6989, training loss: 0.37720235048237843\n",
      "Iteration: 6990, training loss: 0.3771993694439844\n",
      "Iteration: 6991, training loss: 0.37719638897120966\n",
      "Iteration: 6992, training loss: 0.37719340906385823\n",
      "Iteration: 6993, training loss: 0.37719042972173394\n",
      "Iteration: 6994, training loss: 0.37718745094464123\n",
      "Iteration: 6995, training loss: 0.3771844727323841\n",
      "Iteration: 6996, training loss: 0.3771814950847669\n",
      "Iteration: 6997, training loss: 0.3771785180015942\n",
      "Iteration: 6998, training loss: 0.3771755414826704\n",
      "Iteration: 6999, training loss: 0.3771725655278001\n",
      "Iteration: 7000, training loss: 0.37716959013678825\n",
      "Iteration: 7001, training loss: 0.3771666153094394\n",
      "Iteration: 7002, training loss: 0.37716364104555866\n",
      "Iteration: 7003, training loss: 0.377160667344951\n",
      "Iteration: 7004, training loss: 0.37715769420742157\n",
      "Iteration: 7005, training loss: 0.37715472163277547\n",
      "Iteration: 7006, training loss: 0.3771517496208182\n",
      "Iteration: 7007, training loss: 0.377148778171355\n",
      "Iteration: 7008, training loss: 0.3771458072841915\n",
      "Iteration: 7009, training loss: 0.37714283695913337\n",
      "Iteration: 7010, training loss: 0.37713986719598624\n",
      "Iteration: 7011, training loss: 0.37713689799455585\n",
      "Iteration: 7012, training loss: 0.37713392935464823\n",
      "Iteration: 7013, training loss: 0.3771309612760693\n",
      "Iteration: 7014, training loss: 0.37712799375862527\n",
      "Iteration: 7015, training loss: 0.37712502680212234\n",
      "Iteration: 7016, training loss: 0.37712206040636664\n",
      "Iteration: 7017, training loss: 0.37711909457116477\n",
      "Iteration: 7018, training loss: 0.37711612929632304\n",
      "Iteration: 7019, training loss: 0.3771131645816482\n",
      "Iteration: 7020, training loss: 0.3771102004269469\n",
      "Iteration: 7021, training loss: 0.3771072368320259\n",
      "Iteration: 7022, training loss: 0.3771042737966921\n",
      "Iteration: 7023, training loss: 0.3771013113207524\n",
      "Iteration: 7024, training loss: 0.3770983494040141\n",
      "Iteration: 7025, training loss: 0.3770953880462842\n",
      "Iteration: 7026, training loss: 0.37709242724737\n",
      "Iteration: 7027, training loss: 0.37708946700707885\n",
      "Iteration: 7028, training loss: 0.37708650732521826\n",
      "Iteration: 7029, training loss: 0.37708354820159584\n",
      "Iteration: 7030, training loss: 0.3770805896360192\n",
      "Iteration: 7031, training loss: 0.3770776316282961\n",
      "Iteration: 7032, training loss: 0.37707467417823437\n",
      "Iteration: 7033, training loss: 0.37707171728564204\n",
      "Iteration: 7034, training loss: 0.37706876095032715\n",
      "Iteration: 7035, training loss: 0.37706580517209776\n",
      "Iteration: 7036, training loss: 0.37706284995076217\n",
      "Iteration: 7037, training loss: 0.37705989528612877\n",
      "Iteration: 7038, training loss: 0.37705694117800587\n",
      "Iteration: 7039, training loss: 0.3770539876262022\n",
      "Iteration: 7040, training loss: 0.37705103463052614\n",
      "Iteration: 7041, training loss: 0.3770480821907865\n",
      "Iteration: 7042, training loss: 0.3770451303067923\n",
      "Iteration: 7043, training loss: 0.37704217897835224\n",
      "Iteration: 7044, training loss: 0.3770392282052754\n",
      "Iteration: 7045, training loss: 0.3770362779873709\n",
      "Iteration: 7046, training loss: 0.3770333283244478\n",
      "Iteration: 7047, training loss: 0.3770303792163156\n",
      "Iteration: 7048, training loss: 0.3770274306627837\n",
      "Iteration: 7049, training loss: 0.37702448266366145\n",
      "Iteration: 7050, training loss: 0.37702153521875853\n",
      "Iteration: 7051, training loss: 0.3770185883278846\n",
      "Iteration: 7052, training loss: 0.3770156419908494\n",
      "Iteration: 7053, training loss: 0.37701269620746286\n",
      "Iteration: 7054, training loss: 0.3770097509775349\n",
      "Iteration: 7055, training loss: 0.37700680630087563\n",
      "Iteration: 7056, training loss: 0.37700386217729526\n",
      "Iteration: 7057, training loss: 0.37700091860660395\n",
      "Iteration: 7058, training loss: 0.37699797558861214\n",
      "Iteration: 7059, training loss: 0.3769950331231301\n",
      "Iteration: 7060, training loss: 0.3769920912099686\n",
      "Iteration: 7061, training loss: 0.3769891498489382\n",
      "Iteration: 7062, training loss: 0.3769862090398496\n",
      "Iteration: 7063, training loss: 0.37698326878251354\n",
      "Iteration: 7064, training loss: 0.376980329076741\n",
      "Iteration: 7065, training loss: 0.37697738992234314\n",
      "Iteration: 7066, training loss: 0.3769744513191309\n",
      "Iteration: 7067, training loss: 0.3769715132669156\n",
      "Iteration: 7068, training loss: 0.3769685757655084\n",
      "Iteration: 7069, training loss: 0.3769656388147208\n",
      "Iteration: 7070, training loss: 0.37696270241436425\n",
      "Iteration: 7071, training loss: 0.3769597665642504\n",
      "Iteration: 7072, training loss: 0.3769568312641909\n",
      "Iteration: 7073, training loss: 0.3769538965139974\n",
      "Iteration: 7074, training loss: 0.3769509623134819\n",
      "Iteration: 7075, training loss: 0.3769480286624564\n",
      "Iteration: 7076, training loss: 0.3769450955607328\n",
      "Iteration: 7077, training loss: 0.3769421630081233\n",
      "Iteration: 7078, training loss: 0.3769392310044402\n",
      "Iteration: 7079, training loss: 0.3769362995494958\n",
      "Iteration: 7080, training loss: 0.37693336864310245\n",
      "Iteration: 7081, training loss: 0.37693043828507283\n",
      "Iteration: 7082, training loss: 0.37692750847521955\n",
      "Iteration: 7083, training loss: 0.37692457921335515\n",
      "Iteration: 7084, training loss: 0.3769216504992926\n",
      "Iteration: 7085, training loss: 0.37691872233284474\n",
      "Iteration: 7086, training loss: 0.37691579471382447\n",
      "Iteration: 7087, training loss: 0.376912867642045\n",
      "Iteration: 7088, training loss: 0.37690994111731946\n",
      "Iteration: 7089, training loss: 0.37690701513946123\n",
      "Iteration: 7090, training loss: 0.3769040897082835\n",
      "Iteration: 7091, training loss: 0.37690116482359975\n",
      "Iteration: 7092, training loss: 0.37689824048522363\n",
      "Iteration: 7093, training loss: 0.37689531669296883\n",
      "Iteration: 7094, training loss: 0.3768923934466488\n",
      "Iteration: 7095, training loss: 0.3768894707460777\n",
      "Iteration: 7096, training loss: 0.37688654859106935\n",
      "Iteration: 7097, training loss: 0.37688362698143774\n",
      "Iteration: 7098, training loss: 0.3768807059169969\n",
      "Iteration: 7099, training loss: 0.37687778539756106\n",
      "Iteration: 7100, training loss: 0.3768748654229447\n",
      "Iteration: 7101, training loss: 0.376871945992962\n",
      "Iteration: 7102, training loss: 0.3768690271074275\n",
      "Iteration: 7103, training loss: 0.3768661087661557\n",
      "Iteration: 7104, training loss: 0.3768631909689614\n",
      "Iteration: 7105, training loss: 0.37686027371565933\n",
      "Iteration: 7106, training loss: 0.37685735700606426\n",
      "Iteration: 7107, training loss: 0.3768544408399911\n",
      "Iteration: 7108, training loss: 0.37685152521725507\n",
      "Iteration: 7109, training loss: 0.37684861013767124\n",
      "Iteration: 7110, training loss: 0.3768456956010547\n",
      "Iteration: 7111, training loss: 0.3768427816072208\n",
      "Iteration: 7112, training loss: 0.37683986815598497\n",
      "Iteration: 7113, training loss: 0.3768369552471627\n",
      "Iteration: 7114, training loss: 0.3768340428805696\n",
      "Iteration: 7115, training loss: 0.37683113105602134\n",
      "Iteration: 7116, training loss: 0.37682821977333364\n",
      "Iteration: 7117, training loss: 0.37682530903232236\n",
      "Iteration: 7118, training loss: 0.37682239883280355\n",
      "Iteration: 7119, training loss: 0.3768194891745932\n",
      "Iteration: 7120, training loss: 0.3768165800575074\n",
      "Iteration: 7121, training loss: 0.37681367148136236\n",
      "Iteration: 7122, training loss: 0.3768107634459745\n",
      "Iteration: 7123, training loss: 0.3768078559511601\n",
      "Iteration: 7124, training loss: 0.3768049489967357\n",
      "Iteration: 7125, training loss: 0.37680204258251804\n",
      "Iteration: 7126, training loss: 0.37679913670832366\n",
      "Iteration: 7127, training loss: 0.37679623137396934\n",
      "Iteration: 7128, training loss: 0.37679332657927195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7129, training loss: 0.37679042232404847\n",
      "Iteration: 7130, training loss: 0.37678751860811593\n",
      "Iteration: 7131, training loss: 0.3767846154312915\n",
      "Iteration: 7132, training loss: 0.3767817127933923\n",
      "Iteration: 7133, training loss: 0.37677881069423574\n",
      "Iteration: 7134, training loss: 0.37677590913363923\n",
      "Iteration: 7135, training loss: 0.3767730081114202\n",
      "Iteration: 7136, training loss: 0.37677010762739643\n",
      "Iteration: 7137, training loss: 0.37676720768138533\n",
      "Iteration: 7138, training loss: 0.37676430827320484\n",
      "Iteration: 7139, training loss: 0.37676140940267283\n",
      "Iteration: 7140, training loss: 0.37675851106960717\n",
      "Iteration: 7141, training loss: 0.37675561327382595\n",
      "Iteration: 7142, training loss: 0.37675271601514726\n",
      "Iteration: 7143, training loss: 0.3767498192933893\n",
      "Iteration: 7144, training loss: 0.3767469231083705\n",
      "Iteration: 7145, training loss: 0.37674402745990926\n",
      "Iteration: 7146, training loss: 0.3767411323478239\n",
      "Iteration: 7147, training loss: 0.3767382377719331\n",
      "Iteration: 7148, training loss: 0.37673534373205564\n",
      "Iteration: 7149, training loss: 0.3767324502280101\n",
      "Iteration: 7150, training loss: 0.37672955725961543\n",
      "Iteration: 7151, training loss: 0.3767266648266906\n",
      "Iteration: 7152, training loss: 0.37672377292905457\n",
      "Iteration: 7153, training loss: 0.3767208815665265\n",
      "Iteration: 7154, training loss: 0.3767179907389256\n",
      "Iteration: 7155, training loss: 0.376715100446071\n",
      "Iteration: 7156, training loss: 0.37671221068778243\n",
      "Iteration: 7157, training loss: 0.37670932146387914\n",
      "Iteration: 7158, training loss: 0.37670643277418064\n",
      "Iteration: 7159, training loss: 0.3767035446185068\n",
      "Iteration: 7160, training loss: 0.37670065699667726\n",
      "Iteration: 7161, training loss: 0.37669776990851184\n",
      "Iteration: 7162, training loss: 0.37669488335383033\n",
      "Iteration: 7163, training loss: 0.37669199733245307\n",
      "Iteration: 7164, training loss: 0.37668911184419984\n",
      "Iteration: 7165, training loss: 0.37668622688889103\n",
      "Iteration: 7166, training loss: 0.37668334246634677\n",
      "Iteration: 7167, training loss: 0.37668045857638766\n",
      "Iteration: 7168, training loss: 0.37667757521883394\n",
      "Iteration: 7169, training loss: 0.37667469239350615\n",
      "Iteration: 7170, training loss: 0.37667181010022516\n",
      "Iteration: 7171, training loss: 0.37666892833881144\n",
      "Iteration: 7172, training loss: 0.37666604710908586\n",
      "Iteration: 7173, training loss: 0.37666316641086944\n",
      "Iteration: 7174, training loss: 0.376660286243983\n",
      "Iteration: 7175, training loss: 0.37665740660824776\n",
      "Iteration: 7176, training loss: 0.37665452750348477\n",
      "Iteration: 7177, training loss: 0.3766516489295153\n",
      "Iteration: 7178, training loss: 0.3766487708861608\n",
      "Iteration: 7179, training loss: 0.3766458933732426\n",
      "Iteration: 7180, training loss: 0.3766430163905823\n",
      "Iteration: 7181, training loss: 0.37664013993800144\n",
      "Iteration: 7182, training loss: 0.37663726401532166\n",
      "Iteration: 7183, training loss: 0.3766343886223648\n",
      "Iteration: 7184, training loss: 0.37663151375895276\n",
      "Iteration: 7185, training loss: 0.3766286394249075\n",
      "Iteration: 7186, training loss: 0.376625765620051\n",
      "Iteration: 7187, training loss: 0.3766228923442054\n",
      "Iteration: 7188, training loss: 0.37662001959719293\n",
      "Iteration: 7189, training loss: 0.376617147378836\n",
      "Iteration: 7190, training loss: 0.37661427568895683\n",
      "Iteration: 7191, training loss: 0.376611404527378\n",
      "Iteration: 7192, training loss: 0.3766085338939221\n",
      "Iteration: 7193, training loss: 0.37660566378841176\n",
      "Iteration: 7194, training loss: 0.3766027942106696\n",
      "Iteration: 7195, training loss: 0.37659992516051877\n",
      "Iteration: 7196, training loss: 0.37659705663778187\n",
      "Iteration: 7197, training loss: 0.37659418864228195\n",
      "Iteration: 7198, training loss: 0.3765913211738423\n",
      "Iteration: 7199, training loss: 0.37658845423228593\n",
      "Iteration: 7200, training loss: 0.3765855878174362\n",
      "Iteration: 7201, training loss: 0.37658272192911635\n",
      "Iteration: 7202, training loss: 0.3765798565671499\n",
      "Iteration: 7203, training loss: 0.37657699173136033\n",
      "Iteration: 7204, training loss: 0.3765741274215713\n",
      "Iteration: 7205, training loss: 0.3765712636376065\n",
      "Iteration: 7206, training loss: 0.3765684003792897\n",
      "Iteration: 7207, training loss: 0.3765655376464448\n",
      "Iteration: 7208, training loss: 0.3765626754388957\n",
      "Iteration: 7209, training loss: 0.3765598137564666\n",
      "Iteration: 7210, training loss: 0.3765569525989815\n",
      "Iteration: 7211, training loss: 0.37655409196626455\n",
      "Iteration: 7212, training loss: 0.37655123185814016\n",
      "Iteration: 7213, training loss: 0.37654837227443283\n",
      "Iteration: 7214, training loss: 0.37654551321496676\n",
      "Iteration: 7215, training loss: 0.37654265467956677\n",
      "Iteration: 7216, training loss: 0.37653979666805737\n",
      "Iteration: 7217, training loss: 0.3765369391802633\n",
      "Iteration: 7218, training loss: 0.3765340822160095\n",
      "Iteration: 7219, training loss: 0.3765312257751207\n",
      "Iteration: 7220, training loss: 0.37652836985742216\n",
      "Iteration: 7221, training loss: 0.3765255144627387\n",
      "Iteration: 7222, training loss: 0.3765226595908955\n",
      "Iteration: 7223, training loss: 0.3765198052417179\n",
      "Iteration: 7224, training loss: 0.37651695141503133\n",
      "Iteration: 7225, training loss: 0.376514098110661\n",
      "Iteration: 7226, training loss: 0.3765112453284326\n",
      "Iteration: 7227, training loss: 0.37650839306817163\n",
      "Iteration: 7228, training loss: 0.3765055413297037\n",
      "Iteration: 7229, training loss: 0.37650269011285475\n",
      "Iteration: 7230, training loss: 0.37649983941745047\n",
      "Iteration: 7231, training loss: 0.3764969892433169\n",
      "Iteration: 7232, training loss: 0.37649413959028005\n",
      "Iteration: 7233, training loss: 0.3764912904581659\n",
      "Iteration: 7234, training loss: 0.37648844184680086\n",
      "Iteration: 7235, training loss: 0.376485593756011\n",
      "Iteration: 7236, training loss: 0.3764827461856227\n",
      "Iteration: 7237, training loss: 0.3764798991354626\n",
      "Iteration: 7238, training loss: 0.37647705260535697\n",
      "Iteration: 7239, training loss: 0.3764742065951326\n",
      "Iteration: 7240, training loss: 0.37647136110461615\n",
      "Iteration: 7241, training loss: 0.37646851613363436\n",
      "Iteration: 7242, training loss: 0.3764656716820141\n",
      "Iteration: 7243, training loss: 0.37646282774958245\n",
      "Iteration: 7244, training loss: 0.37645998433616634\n",
      "Iteration: 7245, training loss: 0.3764571414415928\n",
      "Iteration: 7246, training loss: 0.3764542990656892\n",
      "Iteration: 7247, training loss: 0.3764514572082828\n",
      "Iteration: 7248, training loss: 0.376448615869201\n",
      "Iteration: 7249, training loss: 0.3764457750482711\n",
      "Iteration: 7250, training loss: 0.3764429347453207\n",
      "Iteration: 7251, training loss: 0.3764400949601776\n",
      "Iteration: 7252, training loss: 0.37643725569266934\n",
      "Iteration: 7253, training loss: 0.37643441694262375\n",
      "Iteration: 7254, training loss: 0.3764315787098687\n",
      "Iteration: 7255, training loss: 0.37642874099423235\n",
      "Iteration: 7256, training loss: 0.3764259037955423\n",
      "Iteration: 7257, training loss: 0.37642306711362716\n",
      "Iteration: 7258, training loss: 0.37642023094831495\n",
      "Iteration: 7259, training loss: 0.37641739529943385\n",
      "Iteration: 7260, training loss: 0.3764145601668124\n",
      "Iteration: 7261, training loss: 0.37641172555027913\n",
      "Iteration: 7262, training loss: 0.37640889144966244\n",
      "Iteration: 7263, training loss: 0.376406057864791\n",
      "Iteration: 7264, training loss: 0.3764032247954935\n",
      "Iteration: 7265, training loss: 0.37640039224159877\n",
      "Iteration: 7266, training loss: 0.37639756020293574\n",
      "Iteration: 7267, training loss: 0.3763947286793333\n",
      "Iteration: 7268, training loss: 0.3763918976706207\n",
      "Iteration: 7269, training loss: 0.3763890671766268\n",
      "Iteration: 7270, training loss: 0.37638623719718095\n",
      "Iteration: 7271, training loss: 0.3763834077321125\n",
      "Iteration: 7272, training loss: 0.3763805787812507\n",
      "Iteration: 7273, training loss: 0.376377750344425\n",
      "Iteration: 7274, training loss: 0.3763749224214652\n",
      "Iteration: 7275, training loss: 0.3763720950122007\n",
      "Iteration: 7276, training loss: 0.3763692681164613\n",
      "Iteration: 7277, training loss: 0.3763664417340768\n",
      "Iteration: 7278, training loss: 0.37636361586487704\n",
      "Iteration: 7279, training loss: 0.376360790508692\n",
      "Iteration: 7280, training loss: 0.3763579656653518\n",
      "Iteration: 7281, training loss: 0.3763551413346864\n",
      "Iteration: 7282, training loss: 0.37635231751652626\n",
      "Iteration: 7283, training loss: 0.3763494942107014\n",
      "Iteration: 7284, training loss: 0.3763466714170425\n",
      "Iteration: 7285, training loss: 0.37634384913537966\n",
      "Iteration: 7286, training loss: 0.3763410273655437\n",
      "Iteration: 7287, training loss: 0.3763382061073651\n",
      "Iteration: 7288, training loss: 0.37633538536067473\n",
      "Iteration: 7289, training loss: 0.3763325651253032\n",
      "Iteration: 7290, training loss: 0.3763297454010815\n",
      "Iteration: 7291, training loss: 0.37632692618784047\n",
      "Iteration: 7292, training loss: 0.37632410748541123\n",
      "Iteration: 7293, training loss: 0.3763212892936249\n",
      "Iteration: 7294, training loss: 0.37631847161231263\n",
      "Iteration: 7295, training loss: 0.37631565444130566\n",
      "Iteration: 7296, training loss: 0.3763128377804355\n",
      "Iteration: 7297, training loss: 0.3763100216295335\n",
      "Iteration: 7298, training loss: 0.37630720598843115\n",
      "Iteration: 7299, training loss: 0.37630439085696016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7300, training loss: 0.37630157623495214\n",
      "Iteration: 7301, training loss: 0.3762987621222389\n",
      "Iteration: 7302, training loss: 0.3762959485186522\n",
      "Iteration: 7303, training loss: 0.37629313542402415\n",
      "Iteration: 7304, training loss: 0.37629032283818664\n",
      "Iteration: 7305, training loss: 0.3762875107609718\n",
      "Iteration: 7306, training loss: 0.3762846991922117\n",
      "Iteration: 7307, training loss: 0.3762818881317388\n",
      "Iteration: 7308, training loss: 0.3762790775793853\n",
      "Iteration: 7309, training loss: 0.3762762675349836\n",
      "Iteration: 7310, training loss: 0.37627345799836637\n",
      "Iteration: 7311, training loss: 0.3762706489693661\n",
      "Iteration: 7312, training loss: 0.37626784044781536\n",
      "Iteration: 7313, training loss: 0.37626503243354703\n",
      "Iteration: 7314, training loss: 0.37626222492639383\n",
      "Iteration: 7315, training loss: 0.3762594179261888\n",
      "Iteration: 7316, training loss: 0.37625661143276473\n",
      "Iteration: 7317, training loss: 0.3762538054459549\n",
      "Iteration: 7318, training loss: 0.37625099996559236\n",
      "Iteration: 7319, training loss: 0.3762481949915104\n",
      "Iteration: 7320, training loss: 0.3762453905235422\n",
      "Iteration: 7321, training loss: 0.3762425865615212\n",
      "Iteration: 7322, training loss: 0.37623978310528106\n",
      "Iteration: 7323, training loss: 0.37623698015465507\n",
      "Iteration: 7324, training loss: 0.376234177709477\n",
      "Iteration: 7325, training loss: 0.3762313757695805\n",
      "Iteration: 7326, training loss: 0.37622857433479945\n",
      "Iteration: 7327, training loss: 0.3762257734049677\n",
      "Iteration: 7328, training loss: 0.3762229729799191\n",
      "Iteration: 7329, training loss: 0.37622017305948785\n",
      "Iteration: 7330, training loss: 0.37621737364350794\n",
      "Iteration: 7331, training loss: 0.3762145747318135\n",
      "Iteration: 7332, training loss: 0.3762117763242391\n",
      "Iteration: 7333, training loss: 0.3762089784206188\n",
      "Iteration: 7334, training loss: 0.37620618102078707\n",
      "Iteration: 7335, training loss: 0.3762033841245786\n",
      "Iteration: 7336, training loss: 0.3762005877318279\n",
      "Iteration: 7337, training loss: 0.37619779184236946\n",
      "Iteration: 7338, training loss: 0.3761949964560384\n",
      "Iteration: 7339, training loss: 0.3761922015726693\n",
      "Iteration: 7340, training loss: 0.37618940719209704\n",
      "Iteration: 7341, training loss: 0.37618661331415665\n",
      "Iteration: 7342, training loss: 0.3761838199386834\n",
      "Iteration: 7343, training loss: 0.3761810270655122\n",
      "Iteration: 7344, training loss: 0.3761782346944784\n",
      "Iteration: 7345, training loss: 0.37617544282541726\n",
      "Iteration: 7346, training loss: 0.37617265145816425\n",
      "Iteration: 7347, training loss: 0.3761698605925547\n",
      "Iteration: 7348, training loss: 0.37616707022842427\n",
      "Iteration: 7349, training loss: 0.37616428036560856\n",
      "Iteration: 7350, training loss: 0.3761614910039434\n",
      "Iteration: 7351, training loss: 0.3761587021432643\n",
      "Iteration: 7352, training loss: 0.3761559137834073\n",
      "Iteration: 7353, training loss: 0.3761531259242083\n",
      "Iteration: 7354, training loss: 0.37615033856550334\n",
      "Iteration: 7355, training loss: 0.3761475517071286\n",
      "Iteration: 7356, training loss: 0.37614476534892016\n",
      "Iteration: 7357, training loss: 0.3761419794907143\n",
      "Iteration: 7358, training loss: 0.37613919413234737\n",
      "Iteration: 7359, training loss: 0.3761364092736558\n",
      "Iteration: 7360, training loss: 0.37613362491447605\n",
      "Iteration: 7361, training loss: 0.3761308410546446\n",
      "Iteration: 7362, training loss: 0.3761280576939983\n",
      "Iteration: 7363, training loss: 0.37612527483237385\n",
      "Iteration: 7364, training loss: 0.37612249246960794\n",
      "Iteration: 7365, training loss: 0.3761197106055375\n",
      "Iteration: 7366, training loss: 0.3761169292399996\n",
      "Iteration: 7367, training loss: 0.3761141483728312\n",
      "Iteration: 7368, training loss: 0.37611136800386935\n",
      "Iteration: 7369, training loss: 0.37610858813295134\n",
      "Iteration: 7370, training loss: 0.3761058087599145\n",
      "Iteration: 7371, training loss: 0.37610302988459604\n",
      "Iteration: 7372, training loss: 0.37610025150683357\n",
      "Iteration: 7373, training loss: 0.37609747362646445\n",
      "Iteration: 7374, training loss: 0.3760946962433263\n",
      "Iteration: 7375, training loss: 0.37609191935725683\n",
      "Iteration: 7376, training loss: 0.37608914296809387\n",
      "Iteration: 7377, training loss: 0.37608636707567517\n",
      "Iteration: 7378, training loss: 0.3760835916798385\n",
      "Iteration: 7379, training loss: 0.37608081678042204\n",
      "Iteration: 7380, training loss: 0.37607804237726367\n",
      "Iteration: 7381, training loss: 0.37607526847020173\n",
      "Iteration: 7382, training loss: 0.3760724950590743\n",
      "Iteration: 7383, training loss: 0.37606972214371964\n",
      "Iteration: 7384, training loss: 0.37606694972397625\n",
      "Iteration: 7385, training loss: 0.3760641777996825\n",
      "Iteration: 7386, training loss: 0.37606140637067687\n",
      "Iteration: 7387, training loss: 0.376058635436798\n",
      "Iteration: 7388, training loss: 0.3760558649978846\n",
      "Iteration: 7389, training loss: 0.37605309505377543\n",
      "Iteration: 7390, training loss: 0.37605032560430923\n",
      "Iteration: 7391, training loss: 0.37604755664932504\n",
      "Iteration: 7392, training loss: 0.37604478818866177\n",
      "Iteration: 7393, training loss: 0.3760420202221584\n",
      "Iteration: 7394, training loss: 0.3760392527496543\n",
      "Iteration: 7395, training loss: 0.3760364857709884\n",
      "Iteration: 7396, training loss: 0.3760337192860002\n",
      "Iteration: 7397, training loss: 0.376030953294529\n",
      "Iteration: 7398, training loss: 0.37602818779641434\n",
      "Iteration: 7399, training loss: 0.37602542279149564\n",
      "Iteration: 7400, training loss: 0.37602265827961245\n",
      "Iteration: 7401, training loss: 0.37601989426060456\n",
      "Iteration: 7402, training loss: 0.3760171307343118\n",
      "Iteration: 7403, training loss: 0.37601436770057384\n",
      "Iteration: 7404, training loss: 0.37601160515923054\n",
      "Iteration: 7405, training loss: 0.37600884311012217\n",
      "Iteration: 7406, training loss: 0.3760060815530885\n",
      "Iteration: 7407, training loss: 0.37600332048796975\n",
      "Iteration: 7408, training loss: 0.3760005599146062\n",
      "Iteration: 7409, training loss: 0.37599779983283804\n",
      "Iteration: 7410, training loss: 0.3759950402425058\n",
      "Iteration: 7411, training loss: 0.37599228114344974\n",
      "Iteration: 7412, training loss: 0.37598952253551055\n",
      "Iteration: 7413, training loss: 0.3759867644185286\n",
      "Iteration: 7414, training loss: 0.3759840067923448\n",
      "Iteration: 7415, training loss: 0.3759812496567997\n",
      "Iteration: 7416, training loss: 0.37597849301173425\n",
      "Iteration: 7417, training loss: 0.3759757368569894\n",
      "Iteration: 7418, training loss: 0.3759729811924059\n",
      "Iteration: 7419, training loss: 0.37597022601782504\n",
      "Iteration: 7420, training loss: 0.3759674713330878\n",
      "Iteration: 7421, training loss: 0.37596471713803536\n",
      "Iteration: 7422, training loss: 0.3759619634325091\n",
      "Iteration: 7423, training loss: 0.37595921021635037\n",
      "Iteration: 7424, training loss: 0.3759564574894006\n",
      "Iteration: 7425, training loss: 0.3759537052515012\n",
      "Iteration: 7426, training loss: 0.37595095350249386\n",
      "Iteration: 7427, training loss: 0.3759482022422201\n",
      "Iteration: 7428, training loss: 0.3759454514705217\n",
      "Iteration: 7429, training loss: 0.3759427011872406\n",
      "Iteration: 7430, training loss: 0.37593995139221853\n",
      "Iteration: 7431, training loss: 0.3759372020852975\n",
      "Iteration: 7432, training loss: 0.37593445326631963\n",
      "Iteration: 7433, training loss: 0.37593170493512684\n",
      "Iteration: 7434, training loss: 0.37592895709156143\n",
      "Iteration: 7435, training loss: 0.3759262097354657\n",
      "Iteration: 7436, training loss: 0.3759234628666818\n",
      "Iteration: 7437, training loss: 0.3759207164850523\n",
      "Iteration: 7438, training loss: 0.37591797059041976\n",
      "Iteration: 7439, training loss: 0.3759152251826265\n",
      "Iteration: 7440, training loss: 0.37591248026151525\n",
      "Iteration: 7441, training loss: 0.37590973582692877\n",
      "Iteration: 7442, training loss: 0.37590699187870974\n",
      "Iteration: 7443, training loss: 0.37590424841670117\n",
      "Iteration: 7444, training loss: 0.37590150544074585\n",
      "Iteration: 7445, training loss: 0.3758987629506869\n",
      "Iteration: 7446, training loss: 0.3758960209463673\n",
      "Iteration: 7447, training loss: 0.3758932794276303\n",
      "Iteration: 7448, training loss: 0.37589053839431896\n",
      "Iteration: 7449, training loss: 0.3758877978462768\n",
      "Iteration: 7450, training loss: 0.375885057783347\n",
      "Iteration: 7451, training loss: 0.3758823182053732\n",
      "Iteration: 7452, training loss: 0.3758795791121988\n",
      "Iteration: 7453, training loss: 0.37587684050366743\n",
      "Iteration: 7454, training loss: 0.37587410237962277\n",
      "Iteration: 7455, training loss: 0.3758713647399085\n",
      "Iteration: 7456, training loss: 0.3758686275843686\n",
      "Iteration: 7457, training loss: 0.3758658909128469\n",
      "Iteration: 7458, training loss: 0.3758631547251873\n",
      "Iteration: 7459, training loss: 0.37586041902123385\n",
      "Iteration: 7460, training loss: 0.3758576838008308\n",
      "Iteration: 7461, training loss: 0.3758549490638222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7462, training loss: 0.37585221481005243\n",
      "Iteration: 7463, training loss: 0.3758494810393657\n",
      "Iteration: 7464, training loss: 0.3758467477516066\n",
      "Iteration: 7465, training loss: 0.37584401494661945\n",
      "Iteration: 7466, training loss: 0.3758412826242489\n",
      "Iteration: 7467, training loss: 0.37583855078433953\n",
      "Iteration: 7468, training loss: 0.37583581942673616\n",
      "Iteration: 7469, training loss: 0.37583308855128345\n",
      "Iteration: 7470, training loss: 0.37583035815782634\n",
      "Iteration: 7471, training loss: 0.3758276282462097\n",
      "Iteration: 7472, training loss: 0.3758248988162786\n",
      "Iteration: 7473, training loss: 0.375822169867878\n",
      "Iteration: 7474, training loss: 0.3758194414008532\n",
      "Iteration: 7475, training loss: 0.37581671341504924\n",
      "Iteration: 7476, training loss: 0.3758139859103117\n",
      "Iteration: 7477, training loss: 0.37581125888648564\n",
      "Iteration: 7478, training loss: 0.3758085323434167\n",
      "Iteration: 7479, training loss: 0.3758058062809502\n",
      "Iteration: 7480, training loss: 0.375803080698932\n",
      "Iteration: 7481, training loss: 0.3758003555972076\n",
      "Iteration: 7482, training loss: 0.3757976309756227\n",
      "Iteration: 7483, training loss: 0.37579490683402317\n",
      "Iteration: 7484, training loss: 0.37579218317225505\n",
      "Iteration: 7485, training loss: 0.37578945999016394\n",
      "Iteration: 7486, training loss: 0.3757867372875961\n",
      "Iteration: 7487, training loss: 0.37578401506439757\n",
      "Iteration: 7488, training loss: 0.3757812933204146\n",
      "Iteration: 7489, training loss: 0.3757785720554933\n",
      "Iteration: 7490, training loss: 0.3757758512694801\n",
      "Iteration: 7491, training loss: 0.37577313096222126\n",
      "Iteration: 7492, training loss: 0.3757704111335634\n",
      "Iteration: 7493, training loss: 0.37576769178335306\n",
      "Iteration: 7494, training loss: 0.3757649729114366\n",
      "Iteration: 7495, training loss: 0.37576225451766093\n",
      "Iteration: 7496, training loss: 0.3757595366018727\n",
      "Iteration: 7497, training loss: 0.37575681916391884\n",
      "Iteration: 7498, training loss: 0.3757541022036462\n",
      "Iteration: 7499, training loss: 0.3757513857209016\n",
      "Iteration: 7500, training loss: 0.3757486697155323\n",
      "Iteration: 7501, training loss: 0.37574595418738527\n",
      "Iteration: 7502, training loss: 0.37574323913630775\n",
      "Iteration: 7503, training loss: 0.37574052456214696\n",
      "Iteration: 7504, training loss: 0.3757378104647503\n",
      "Iteration: 7505, training loss: 0.3757350968439651\n",
      "Iteration: 7506, training loss: 0.37573238369963885\n",
      "Iteration: 7507, training loss: 0.37572967103161914\n",
      "Iteration: 7508, training loss: 0.37572695883975354\n",
      "Iteration: 7509, training loss: 0.3757242471238897\n",
      "Iteration: 7510, training loss: 0.3757215358838754\n",
      "Iteration: 7511, training loss: 0.37571882511955856\n",
      "Iteration: 7512, training loss: 0.375716114830787\n",
      "Iteration: 7513, training loss: 0.37571340501740863\n",
      "Iteration: 7514, training loss: 0.3757106956792717\n",
      "Iteration: 7515, training loss: 0.3757079868162241\n",
      "Iteration: 7516, training loss: 0.3757052784281142\n",
      "Iteration: 7517, training loss: 0.3757025705147901\n",
      "Iteration: 7518, training loss: 0.3756998630761003\n",
      "Iteration: 7519, training loss: 0.375697156111893\n",
      "Iteration: 7520, training loss: 0.37569444962201687\n",
      "Iteration: 7521, training loss: 0.3756917436063204\n",
      "Iteration: 7522, training loss: 0.37568903806465215\n",
      "Iteration: 7523, training loss: 0.3756863329968607\n",
      "Iteration: 7524, training loss: 0.3756836284027951\n",
      "Iteration: 7525, training loss: 0.375680924282304\n",
      "Iteration: 7526, training loss: 0.3756782206352363\n",
      "Iteration: 7527, training loss: 0.375675517461441\n",
      "Iteration: 7528, training loss: 0.37567281476076714\n",
      "Iteration: 7529, training loss: 0.3756701125330638\n",
      "Iteration: 7530, training loss: 0.3756674107781802\n",
      "Iteration: 7531, training loss: 0.37566470949596564\n",
      "Iteration: 7532, training loss: 0.37566200868626937\n",
      "Iteration: 7533, training loss: 0.37565930834894085\n",
      "Iteration: 7534, training loss: 0.3756566084838295\n",
      "Iteration: 7535, training loss: 0.3756539090907848\n",
      "Iteration: 7536, training loss: 0.3756512101696564\n",
      "Iteration: 7537, training loss: 0.3756485117202941\n",
      "Iteration: 7538, training loss: 0.37564581374254746\n",
      "Iteration: 7539, training loss: 0.37564311623626656\n",
      "Iteration: 7540, training loss: 0.375640419201301\n",
      "Iteration: 7541, training loss: 0.37563772263750084\n",
      "Iteration: 7542, training loss: 0.3756350265447162\n",
      "Iteration: 7543, training loss: 0.3756323309227971\n",
      "Iteration: 7544, training loss: 0.37562963577159375\n",
      "Iteration: 7545, training loss: 0.3756269410909563\n",
      "Iteration: 7546, training loss: 0.3756242468807351\n",
      "Iteration: 7547, training loss: 0.3756215531407806\n",
      "Iteration: 7548, training loss: 0.37561885987094334\n",
      "Iteration: 7549, training loss: 0.3756161670710736\n",
      "Iteration: 7550, training loss: 0.37561347474102214\n",
      "Iteration: 7551, training loss: 0.3756107828806395\n",
      "Iteration: 7552, training loss: 0.37560809148977653\n",
      "Iteration: 7553, training loss: 0.37560540056828395\n",
      "Iteration: 7554, training loss: 0.37560271011601265\n",
      "Iteration: 7555, training loss: 0.3756000201328137\n",
      "Iteration: 7556, training loss: 0.3755973306185379\n",
      "Iteration: 7557, training loss: 0.3755946415730364\n",
      "Iteration: 7558, training loss: 0.37559195299616044\n",
      "Iteration: 7559, training loss: 0.3755892648877612\n",
      "Iteration: 7560, training loss: 0.37558657724768985\n",
      "Iteration: 7561, training loss: 0.3755838900757979\n",
      "Iteration: 7562, training loss: 0.37558120337193673\n",
      "Iteration: 7563, training loss: 0.37557851713595786\n",
      "Iteration: 7564, training loss: 0.3755758313677128\n",
      "Iteration: 7565, training loss: 0.37557314606705317\n",
      "Iteration: 7566, training loss: 0.3755704612338308\n",
      "Iteration: 7567, training loss: 0.37556777686789733\n",
      "Iteration: 7568, training loss: 0.37556509296910473\n",
      "Iteration: 7569, training loss: 0.37556240953730474\n",
      "Iteration: 7570, training loss: 0.37555972657234954\n",
      "Iteration: 7571, training loss: 0.3755570440740911\n",
      "Iteration: 7572, training loss: 0.37555436204238146\n",
      "Iteration: 7573, training loss: 0.3755516804770729\n",
      "Iteration: 7574, training loss: 0.37554899937801767\n",
      "Iteration: 7575, training loss: 0.37554631874506805\n",
      "Iteration: 7576, training loss: 0.37554363857807654\n",
      "Iteration: 7577, training loss: 0.3755409588768955\n",
      "Iteration: 7578, training loss: 0.3755382796413775\n",
      "Iteration: 7579, training loss: 0.37553560087137516\n",
      "Iteration: 7580, training loss: 0.3755329225667412\n",
      "Iteration: 7581, training loss: 0.3755302447273282\n",
      "Iteration: 7582, training loss: 0.3755275673529891\n",
      "Iteration: 7583, training loss: 0.37552489044357673\n",
      "Iteration: 7584, training loss: 0.3755222139989441\n",
      "Iteration: 7585, training loss: 0.3755195380189441\n",
      "Iteration: 7586, training loss: 0.37551686250342997\n",
      "Iteration: 7587, training loss: 0.37551418745225473\n",
      "Iteration: 7588, training loss: 0.3755115128652717\n",
      "Iteration: 7589, training loss: 0.3755088387423341\n",
      "Iteration: 7590, training loss: 0.3755061650832953\n",
      "Iteration: 7591, training loss: 0.3755034918880087\n",
      "Iteration: 7592, training loss: 0.37550081915632794\n",
      "Iteration: 7593, training loss: 0.3754981468881064\n",
      "Iteration: 7594, training loss: 0.3754954750831977\n",
      "Iteration: 7595, training loss: 0.37549280374145566\n",
      "Iteration: 7596, training loss: 0.37549013286273397\n",
      "Iteration: 7597, training loss: 0.37548746244688663\n",
      "Iteration: 7598, training loss: 0.37548479249376726\n",
      "Iteration: 7599, training loss: 0.3754821230032301\n",
      "Iteration: 7600, training loss: 0.37547945397512894\n",
      "Iteration: 7601, training loss: 0.3754767854093181\n",
      "Iteration: 7602, training loss: 0.3754741173056516\n",
      "Iteration: 7603, training loss: 0.3754714496639837\n",
      "Iteration: 7604, training loss: 0.3754687824841689\n",
      "Iteration: 7605, training loss: 0.37546611576606137\n",
      "Iteration: 7606, training loss: 0.3754634495095156\n",
      "Iteration: 7607, training loss: 0.375460783714386\n",
      "Iteration: 7608, training loss: 0.3754581183805274\n",
      "Iteration: 7609, training loss: 0.3754554535077943\n",
      "Iteration: 7610, training loss: 0.37545278909604135\n",
      "Iteration: 7611, training loss: 0.37545012514512355\n",
      "Iteration: 7612, training loss: 0.3754474616548954\n",
      "Iteration: 7613, training loss: 0.37544479862521213\n",
      "Iteration: 7614, training loss: 0.37544213605592863\n",
      "Iteration: 7615, training loss: 0.37543947394689997\n",
      "Iteration: 7616, training loss: 0.3754368122979811\n",
      "Iteration: 7617, training loss: 0.3754341511090275\n",
      "Iteration: 7618, training loss: 0.3754314903798942\n",
      "Iteration: 7619, training loss: 0.37542883011043654\n",
      "Iteration: 7620, training loss: 0.37542617030050995\n",
      "Iteration: 7621, training loss: 0.37542351094996995\n",
      "Iteration: 7622, training loss: 0.37542085205867193\n",
      "Iteration: 7623, training loss: 0.3754181936264716\n",
      "Iteration: 7624, training loss: 0.3754155356532245\n",
      "Iteration: 7625, training loss: 0.3754128781387865\n",
      "Iteration: 7626, training loss: 0.3754102210830132\n",
      "Iteration: 7627, training loss: 0.3754075644857605\n",
      "Iteration: 7628, training loss: 0.3754049083468845\n",
      "Iteration: 7629, training loss: 0.375402252666241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7630, training loss: 0.3753995974436862\n",
      "Iteration: 7631, training loss: 0.3753969426790761\n",
      "Iteration: 7632, training loss: 0.3753942883722668\n",
      "Iteration: 7633, training loss: 0.3753916345231148\n",
      "Iteration: 7634, training loss: 0.37538898113147634\n",
      "Iteration: 7635, training loss: 0.37538632819720774\n",
      "Iteration: 7636, training loss: 0.37538367572016546\n",
      "Iteration: 7637, training loss: 0.3753810237002061\n",
      "Iteration: 7638, training loss: 0.3753783721371862\n",
      "Iteration: 7639, training loss: 0.37537572103096245\n",
      "Iteration: 7640, training loss: 0.3753730703813914\n",
      "Iteration: 7641, training loss: 0.3753704201883301\n",
      "Iteration: 7642, training loss: 0.37536777045163516\n",
      "Iteration: 7643, training loss: 0.3753651211711638\n",
      "Iteration: 7644, training loss: 0.3753624723467726\n",
      "Iteration: 7645, training loss: 0.3753598239783189\n",
      "Iteration: 7646, training loss: 0.3753571760656597\n",
      "Iteration: 7647, training loss: 0.37535452860865226\n",
      "Iteration: 7648, training loss: 0.37535188160715394\n",
      "Iteration: 7649, training loss: 0.37534923506102175\n",
      "Iteration: 7650, training loss: 0.3753465889701132\n",
      "Iteration: 7651, training loss: 0.3753439433342859\n",
      "Iteration: 7652, training loss: 0.3753412981533972\n",
      "Iteration: 7653, training loss: 0.3753386534273046\n",
      "Iteration: 7654, training loss: 0.37533600915586596\n",
      "Iteration: 7655, training loss: 0.37533336533893885\n",
      "Iteration: 7656, training loss: 0.37533072197638095\n",
      "Iteration: 7657, training loss: 0.3753280790680505\n",
      "Iteration: 7658, training loss: 0.3753254366138049\n",
      "Iteration: 7659, training loss: 0.37532279461350243\n",
      "Iteration: 7660, training loss: 0.3753201530670011\n",
      "Iteration: 7661, training loss: 0.37531751197415886\n",
      "Iteration: 7662, training loss: 0.37531487133483415\n",
      "Iteration: 7663, training loss: 0.3753122311488849\n",
      "Iteration: 7664, training loss: 0.3753095914161696\n",
      "Iteration: 7665, training loss: 0.3753069521365465\n",
      "Iteration: 7666, training loss: 0.3753043133098742\n",
      "Iteration: 7667, training loss: 0.37530167493601096\n",
      "Iteration: 7668, training loss: 0.37529903701481554\n",
      "Iteration: 7669, training loss: 0.37529639954614646\n",
      "Iteration: 7670, training loss: 0.37529376252986246\n",
      "Iteration: 7671, training loss: 0.37529112596582226\n",
      "Iteration: 7672, training loss: 0.3752884898538847\n",
      "Iteration: 7673, training loss: 0.3752858541939087\n",
      "Iteration: 7674, training loss: 0.37528321898575306\n",
      "Iteration: 7675, training loss: 0.37528058422927696\n",
      "Iteration: 7676, training loss: 0.37527794992433944\n",
      "Iteration: 7677, training loss: 0.37527531607079967\n",
      "Iteration: 7678, training loss: 0.37527268266851677\n",
      "Iteration: 7679, training loss: 0.37527004971735006\n",
      "Iteration: 7680, training loss: 0.37526741721715895\n",
      "Iteration: 7681, training loss: 0.3752647851678027\n",
      "Iteration: 7682, training loss: 0.3752621535691409\n",
      "Iteration: 7683, training loss: 0.375259522421033\n",
      "Iteration: 7684, training loss: 0.37525689172333865\n",
      "Iteration: 7685, training loss: 0.3752542614759174\n",
      "Iteration: 7686, training loss: 0.37525163167862924\n",
      "Iteration: 7687, training loss: 0.3752490023313338\n",
      "Iteration: 7688, training loss: 0.3752463734338907\n",
      "Iteration: 7689, training loss: 0.37524374498616037\n",
      "Iteration: 7690, training loss: 0.37524111698800233\n",
      "Iteration: 7691, training loss: 0.3752384894392769\n",
      "Iteration: 7692, training loss: 0.3752358623398441\n",
      "Iteration: 7693, training loss: 0.375233235689564\n",
      "Iteration: 7694, training loss: 0.375230609488297\n",
      "Iteration: 7695, training loss: 0.37522798373590344\n",
      "Iteration: 7696, training loss: 0.3752253584322435\n",
      "Iteration: 7697, training loss: 0.3752227335771778\n",
      "Iteration: 7698, training loss: 0.37522010917056664\n",
      "Iteration: 7699, training loss: 0.3752174852122708\n",
      "Iteration: 7700, training loss: 0.3752148617021508\n",
      "Iteration: 7701, training loss: 0.37521223864006725\n",
      "Iteration: 7702, training loss: 0.37520961602588104\n",
      "Iteration: 7703, training loss: 0.37520699385945294\n",
      "Iteration: 7704, training loss: 0.3752043721406438\n",
      "Iteration: 7705, training loss: 0.3752017508693145\n",
      "Iteration: 7706, training loss: 0.37519913004532635\n",
      "Iteration: 7707, training loss: 0.37519650966854\n",
      "Iteration: 7708, training loss: 0.37519388973881684\n",
      "Iteration: 7709, training loss: 0.375191270256018\n",
      "Iteration: 7710, training loss: 0.37518865122000483\n",
      "Iteration: 7711, training loss: 0.3751860326306385\n",
      "Iteration: 7712, training loss: 0.3751834144877806\n",
      "Iteration: 7713, training loss: 0.3751807967912923\n",
      "Iteration: 7714, training loss: 0.3751781795410354\n",
      "Iteration: 7715, training loss: 0.37517556273687125\n",
      "Iteration: 7716, training loss: 0.3751729463786618\n",
      "Iteration: 7717, training loss: 0.3751703304662684\n",
      "Iteration: 7718, training loss: 0.37516771499955304\n",
      "Iteration: 7719, training loss: 0.3751650999783775\n",
      "Iteration: 7720, training loss: 0.37516248540260366\n",
      "Iteration: 7721, training loss: 0.37515987127209344\n",
      "Iteration: 7722, training loss: 0.3751572575867089\n",
      "Iteration: 7723, training loss: 0.37515464434631224\n",
      "Iteration: 7724, training loss: 0.37515203155076543\n",
      "Iteration: 7725, training loss: 0.3751494191999308\n",
      "Iteration: 7726, training loss: 0.37514680729367056\n",
      "Iteration: 7727, training loss: 0.37514419583184705\n",
      "Iteration: 7728, training loss: 0.3751415848143226\n",
      "Iteration: 7729, training loss: 0.3751389742409599\n",
      "Iteration: 7730, training loss: 0.37513636411162127\n",
      "Iteration: 7731, training loss: 0.37513375442616936\n",
      "Iteration: 7732, training loss: 0.3751311451844668\n",
      "Iteration: 7733, training loss: 0.37512853638637633\n",
      "Iteration: 7734, training loss: 0.3751259280317608\n",
      "Iteration: 7735, training loss: 0.3751233201204829\n",
      "Iteration: 7736, training loss: 0.3751207126524056\n",
      "Iteration: 7737, training loss: 0.37511810562739195\n",
      "Iteration: 7738, training loss: 0.3751154990453048\n",
      "Iteration: 7739, training loss: 0.3751128929060073\n",
      "Iteration: 7740, training loss: 0.37511028720936274\n",
      "Iteration: 7741, training loss: 0.3751076819552342\n",
      "Iteration: 7742, training loss: 0.37510507714348496\n",
      "Iteration: 7743, training loss: 0.3751024727739784\n",
      "Iteration: 7744, training loss: 0.3750998688465779\n",
      "Iteration: 7745, training loss: 0.37509726536114696\n",
      "Iteration: 7746, training loss: 0.375094662317549\n",
      "Iteration: 7747, training loss: 0.37509205971564774\n",
      "Iteration: 7748, training loss: 0.3750894575553067\n",
      "Iteration: 7749, training loss: 0.3750868558363897\n",
      "Iteration: 7750, training loss: 0.3750842545587605\n",
      "Iteration: 7751, training loss: 0.37508165372228286\n",
      "Iteration: 7752, training loss: 0.3750790533268209\n",
      "Iteration: 7753, training loss: 0.3750764533722382\n",
      "Iteration: 7754, training loss: 0.37507385385839903\n",
      "Iteration: 7755, training loss: 0.37507125478516745\n",
      "Iteration: 7756, training loss: 0.3750686561524077\n",
      "Iteration: 7757, training loss: 0.37506605795998377\n",
      "Iteration: 7758, training loss: 0.37506346020776\n",
      "Iteration: 7759, training loss: 0.3750608628956008\n",
      "Iteration: 7760, training loss: 0.37505826602337056\n",
      "Iteration: 7761, training loss: 0.37505566959093367\n",
      "Iteration: 7762, training loss: 0.37505307359815465\n",
      "Iteration: 7763, training loss: 0.3750504780448981\n",
      "Iteration: 7764, training loss: 0.3750478829310287\n",
      "Iteration: 7765, training loss: 0.37504528825641104\n",
      "Iteration: 7766, training loss: 0.37504269402090995\n",
      "Iteration: 7767, training loss: 0.3750401002243903\n",
      "Iteration: 7768, training loss: 0.3750375068667169\n",
      "Iteration: 7769, training loss: 0.37503491394775473\n",
      "Iteration: 7770, training loss: 0.3750323214673688\n",
      "Iteration: 7771, training loss: 0.3750297294254242\n",
      "Iteration: 7772, training loss: 0.37502713782178587\n",
      "Iteration: 7773, training loss: 0.37502454665631935\n",
      "Iteration: 7774, training loss: 0.37502195592888954\n",
      "Iteration: 7775, training loss: 0.375019365639362\n",
      "Iteration: 7776, training loss: 0.375016775787602\n",
      "Iteration: 7777, training loss: 0.375014186373475\n",
      "Iteration: 7778, training loss: 0.37501159739684653\n",
      "Iteration: 7779, training loss: 0.3750090088575821\n",
      "Iteration: 7780, training loss: 0.3750064207555473\n",
      "Iteration: 7781, training loss: 0.37500383309060786\n",
      "Iteration: 7782, training loss: 0.37500124586262945\n",
      "Iteration: 7783, training loss: 0.37499865907147806\n",
      "Iteration: 7784, training loss: 0.37499607271701935\n",
      "Iteration: 7785, training loss: 0.3749934867991192\n",
      "Iteration: 7786, training loss: 0.37499090131764395\n",
      "Iteration: 7787, training loss: 0.37498831627245927\n",
      "Iteration: 7788, training loss: 0.37498573166343147\n",
      "Iteration: 7789, training loss: 0.3749831474904265\n",
      "Iteration: 7790, training loss: 0.3749805637533108\n",
      "Iteration: 7791, training loss: 0.37497798045195063\n",
      "Iteration: 7792, training loss: 0.3749753975862123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7793, training loss: 0.37497281515596215\n",
      "Iteration: 7794, training loss: 0.37497023316106676\n",
      "Iteration: 7795, training loss: 0.37496765160139256\n",
      "Iteration: 7796, training loss: 0.37496507047680616\n",
      "Iteration: 7797, training loss: 0.3749624897871744\n",
      "Iteration: 7798, training loss: 0.37495990953236363\n",
      "Iteration: 7799, training loss: 0.374957329712241\n",
      "Iteration: 7800, training loss: 0.3749547503266731\n",
      "Iteration: 7801, training loss: 0.3749521713755269\n",
      "Iteration: 7802, training loss: 0.37494959285866925\n",
      "Iteration: 7803, training loss: 0.3749470147759673\n",
      "Iteration: 7804, training loss: 0.37494443712728803\n",
      "Iteration: 7805, training loss: 0.3749418599124987\n",
      "Iteration: 7806, training loss: 0.3749392831314663\n",
      "Iteration: 7807, training loss: 0.37493670678405816\n",
      "Iteration: 7808, training loss: 0.37493413087014177\n",
      "Iteration: 7809, training loss: 0.3749315553895843\n",
      "Iteration: 7810, training loss: 0.3749289803422532\n",
      "Iteration: 7811, training loss: 0.374926405728016\n",
      "Iteration: 7812, training loss: 0.37492383154674025\n",
      "Iteration: 7813, training loss: 0.37492125779829344\n",
      "Iteration: 7814, training loss: 0.3749186844825435\n",
      "Iteration: 7815, training loss: 0.374916111599358\n",
      "Iteration: 7816, training loss: 0.37491353914860476\n",
      "Iteration: 7817, training loss: 0.37491096713015154\n",
      "Iteration: 7818, training loss: 0.3749083955438664\n",
      "Iteration: 7819, training loss: 0.3749058243896172\n",
      "Iteration: 7820, training loss: 0.37490325366727206\n",
      "Iteration: 7821, training loss: 0.3749006833766989\n",
      "Iteration: 7822, training loss: 0.3748981135177661\n",
      "Iteration: 7823, training loss: 0.3748955440903416\n",
      "Iteration: 7824, training loss: 0.3748929750942938\n",
      "Iteration: 7825, training loss: 0.3748904065294912\n",
      "Iteration: 7826, training loss: 0.3748878383958019\n",
      "Iteration: 7827, training loss: 0.3748852706930946\n",
      "Iteration: 7828, training loss: 0.37488270342123753\n",
      "Iteration: 7829, training loss: 0.3748801365800994\n",
      "Iteration: 7830, training loss: 0.3748775701695488\n",
      "Iteration: 7831, training loss: 0.37487500418945446\n",
      "Iteration: 7832, training loss: 0.3748724386396851\n",
      "Iteration: 7833, training loss: 0.3748698735201096\n",
      "Iteration: 7834, training loss: 0.3748673088305966\n",
      "Iteration: 7835, training loss: 0.3748647445710152\n",
      "Iteration: 7836, training loss: 0.3748621807412343\n",
      "Iteration: 7837, training loss: 0.37485961734112294\n",
      "Iteration: 7838, training loss: 0.3748570543705503\n",
      "Iteration: 7839, training loss: 0.37485449182938546\n",
      "Iteration: 7840, training loss: 0.3748519297174976\n",
      "Iteration: 7841, training loss: 0.374849368034756\n",
      "Iteration: 7842, training loss: 0.37484680678103005\n",
      "Iteration: 7843, training loss: 0.37484424595618915\n",
      "Iteration: 7844, training loss: 0.3748416855601026\n",
      "Iteration: 7845, training loss: 0.37483912559264004\n",
      "Iteration: 7846, training loss: 0.37483656605367105\n",
      "Iteration: 7847, training loss: 0.3748340069430652\n",
      "Iteration: 7848, training loss: 0.3748314482606922\n",
      "Iteration: 7849, training loss: 0.37482889000642167\n",
      "Iteration: 7850, training loss: 0.3748263321801235\n",
      "Iteration: 7851, training loss: 0.3748237747816677\n",
      "Iteration: 7852, training loss: 0.3748212178109239\n",
      "Iteration: 7853, training loss: 0.3748186612677623\n",
      "Iteration: 7854, training loss: 0.37481610515205277\n",
      "Iteration: 7855, training loss: 0.37481354946366546\n",
      "Iteration: 7856, training loss: 0.37481099420247066\n",
      "Iteration: 7857, training loss: 0.37480843936833824\n",
      "Iteration: 7858, training loss: 0.37480588496113876\n",
      "Iteration: 7859, training loss: 0.37480333098074253\n",
      "Iteration: 7860, training loss: 0.37480077742701984\n",
      "Iteration: 7861, training loss: 0.37479822429984105\n",
      "Iteration: 7862, training loss: 0.3747956715990769\n",
      "Iteration: 7863, training loss: 0.37479311932459775\n",
      "Iteration: 7864, training loss: 0.3747905674762742\n",
      "Iteration: 7865, training loss: 0.374788016053977\n",
      "Iteration: 7866, training loss: 0.3747854650575769\n",
      "Iteration: 7867, training loss: 0.37478291448694473\n",
      "Iteration: 7868, training loss: 0.3747803643419511\n",
      "Iteration: 7869, training loss: 0.37477781462246723\n",
      "Iteration: 7870, training loss: 0.37477526532836386\n",
      "Iteration: 7871, training loss: 0.3747727164595121\n",
      "Iteration: 7872, training loss: 0.37477016801578295\n",
      "Iteration: 7873, training loss: 0.37476761999704766\n",
      "Iteration: 7874, training loss: 0.3747650724031772\n",
      "Iteration: 7875, training loss: 0.3747625252340432\n",
      "Iteration: 7876, training loss: 0.37475997848951664\n",
      "Iteration: 7877, training loss: 0.37475743216946894\n",
      "Iteration: 7878, training loss: 0.3747548862737717\n",
      "Iteration: 7879, training loss: 0.3747523408022962\n",
      "Iteration: 7880, training loss: 0.3747497957549141\n",
      "Iteration: 7881, training loss: 0.3747472511314969\n",
      "Iteration: 7882, training loss: 0.3747447069319163\n",
      "Iteration: 7883, training loss: 0.374742163156044\n",
      "Iteration: 7884, training loss: 0.3747396198037518\n",
      "Iteration: 7885, training loss: 0.3747370768749115\n",
      "Iteration: 7886, training loss: 0.374734534369395\n",
      "Iteration: 7887, training loss: 0.3747319922870742\n",
      "Iteration: 7888, training loss: 0.37472945062782115\n",
      "Iteration: 7889, training loss: 0.3747269093915078\n",
      "Iteration: 7890, training loss: 0.37472436857800645\n",
      "Iteration: 7891, training loss: 0.374721828187189\n",
      "Iteration: 7892, training loss: 0.37471928821892797\n",
      "Iteration: 7893, training loss: 0.37471674867309546\n",
      "Iteration: 7894, training loss: 0.37471420954956386\n",
      "Iteration: 7895, training loss: 0.3747116708482055\n",
      "Iteration: 7896, training loss: 0.3747091325688929\n",
      "Iteration: 7897, training loss: 0.3747065947114986\n",
      "Iteration: 7898, training loss: 0.3747040572758951\n",
      "Iteration: 7899, training loss: 0.3747015202619551\n",
      "Iteration: 7900, training loss: 0.3746989836695511\n",
      "Iteration: 7901, training loss: 0.3746964474985561\n",
      "Iteration: 7902, training loss: 0.3746939117488427\n",
      "Iteration: 7903, training loss: 0.3746913764202838\n",
      "Iteration: 7904, training loss: 0.3746888415127523\n",
      "Iteration: 7905, training loss: 0.37468630702612127\n",
      "Iteration: 7906, training loss: 0.37468377296026356\n",
      "Iteration: 7907, training loss: 0.3746812393150524\n",
      "Iteration: 7908, training loss: 0.37467870609036075\n",
      "Iteration: 7909, training loss: 0.37467617328606195\n",
      "Iteration: 7910, training loss: 0.3746736409020292\n",
      "Iteration: 7911, training loss: 0.37467110893813577\n",
      "Iteration: 7912, training loss: 0.37466857739425513\n",
      "Iteration: 7913, training loss: 0.3746660462702607\n",
      "Iteration: 7914, training loss: 0.3746635155660257\n",
      "Iteration: 7915, training loss: 0.37466098528142383\n",
      "Iteration: 7916, training loss: 0.37465845541632875\n",
      "Iteration: 7917, training loss: 0.374655925970614\n",
      "Iteration: 7918, training loss: 0.37465339694415334\n",
      "Iteration: 7919, training loss: 0.37465086833682043\n",
      "Iteration: 7920, training loss: 0.3746483401484892\n",
      "Iteration: 7921, training loss: 0.3746458123790334\n",
      "Iteration: 7922, training loss: 0.37464328502832706\n",
      "Iteration: 7923, training loss: 0.3746407580962441\n",
      "Iteration: 7924, training loss: 0.3746382315826585\n",
      "Iteration: 7925, training loss: 0.37463570548744446\n",
      "Iteration: 7926, training loss: 0.3746331798104761\n",
      "Iteration: 7927, training loss: 0.3746306545516276\n",
      "Iteration: 7928, training loss: 0.3746281297107731\n",
      "Iteration: 7929, training loss: 0.3746256052877871\n",
      "Iteration: 7930, training loss: 0.3746230812825439\n",
      "Iteration: 7931, training loss: 0.3746205576949179\n",
      "Iteration: 7932, training loss: 0.3746180345247837\n",
      "Iteration: 7933, training loss: 0.3746155117720157\n",
      "Iteration: 7934, training loss: 0.3746129894364884\n",
      "Iteration: 7935, training loss: 0.3746104675180767\n",
      "Iteration: 7936, training loss: 0.37460794601665526\n",
      "Iteration: 7937, training loss: 0.3746054249320987\n",
      "Iteration: 7938, training loss: 0.3746029042642819\n",
      "Iteration: 7939, training loss: 0.37460038401307966\n",
      "Iteration: 7940, training loss: 0.3745978641783671\n",
      "Iteration: 7941, training loss: 0.374595344760019\n",
      "Iteration: 7942, training loss: 0.3745928257579105\n",
      "Iteration: 7943, training loss: 0.3745903071719167\n",
      "Iteration: 7944, training loss: 0.37458778900191264\n",
      "Iteration: 7945, training loss: 0.37458527124777374\n",
      "Iteration: 7946, training loss: 0.3745827539093749\n",
      "Iteration: 7947, training loss: 0.3745802369865919\n",
      "Iteration: 7948, training loss: 0.37457772047929977\n",
      "Iteration: 7949, training loss: 0.37457520438737396\n",
      "Iteration: 7950, training loss: 0.3745726887106901\n",
      "Iteration: 7951, training loss: 0.3745701734491236\n",
      "Iteration: 7952, training loss: 0.37456765860255015\n",
      "Iteration: 7953, training loss: 0.3745651441708453\n",
      "Iteration: 7954, training loss: 0.37456263015388475\n",
      "Iteration: 7955, training loss: 0.3745601165515443\n",
      "Iteration: 7956, training loss: 0.3745576033636998\n",
      "Iteration: 7957, training loss: 0.37455509059022707\n",
      "Iteration: 7958, training loss: 0.3745525782310019\n",
      "Iteration: 7959, training loss: 0.37455006628590054\n",
      "Iteration: 7960, training loss: 0.37454755475479884\n",
      "Iteration: 7961, training loss: 0.37454504363757285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7962, training loss: 0.3745425329340989\n",
      "Iteration: 7963, training loss: 0.374540022644253\n",
      "Iteration: 7964, training loss: 0.3745375127679114\n",
      "Iteration: 7965, training loss: 0.3745350033049506\n",
      "Iteration: 7966, training loss: 0.3745324942552467\n",
      "Iteration: 7967, training loss: 0.37452998561867623\n",
      "Iteration: 7968, training loss: 0.37452747739511577\n",
      "Iteration: 7969, training loss: 0.3745249695844417\n",
      "Iteration: 7970, training loss: 0.3745224621865305\n",
      "Iteration: 7971, training loss: 0.3745199552012591\n",
      "Iteration: 7972, training loss: 0.3745174486285039\n",
      "Iteration: 7973, training loss: 0.3745149424681418\n",
      "Iteration: 7974, training loss: 0.3745124367200496\n",
      "Iteration: 7975, training loss: 0.37450993138410404\n",
      "Iteration: 7976, training loss: 0.37450742646018204\n",
      "Iteration: 7977, training loss: 0.3745049219481607\n",
      "Iteration: 7978, training loss: 0.3745024178479169\n",
      "Iteration: 7979, training loss: 0.3744999141593277\n",
      "Iteration: 7980, training loss: 0.3744974108822703\n",
      "Iteration: 7981, training loss: 0.37449490801662183\n",
      "Iteration: 7982, training loss: 0.3744924055622596\n",
      "Iteration: 7983, training loss: 0.37448990351906075\n",
      "Iteration: 7984, training loss: 0.3744874018869026\n",
      "Iteration: 7985, training loss: 0.37448490066566276\n",
      "Iteration: 7986, training loss: 0.3744823998552184\n",
      "Iteration: 7987, training loss: 0.3744798994554473\n",
      "Iteration: 7988, training loss: 0.3744773994662268\n",
      "Iteration: 7989, training loss: 0.37447489988743454\n",
      "Iteration: 7990, training loss: 0.3744724007189484\n",
      "Iteration: 7991, training loss: 0.3744699019606457\n",
      "Iteration: 7992, training loss: 0.37446740361240455\n",
      "Iteration: 7993, training loss: 0.37446490567410257\n",
      "Iteration: 7994, training loss: 0.3744624081456178\n",
      "Iteration: 7995, training loss: 0.37445991102682813\n",
      "Iteration: 7996, training loss: 0.37445741431761137\n",
      "Iteration: 7997, training loss: 0.3744549180178458\n",
      "Iteration: 7998, training loss: 0.37445242212740926\n",
      "Iteration: 7999, training loss: 0.37444992664618015\n",
      "Iteration: 8000, training loss: 0.37444743157403654\n",
      "Iteration: 8001, training loss: 0.3744449369108566\n",
      "Iteration: 8002, training loss: 0.3744424426565188\n",
      "Iteration: 8003, training loss: 0.3744399488109015\n",
      "Iteration: 8004, training loss: 0.37443745537388295\n",
      "Iteration: 8005, training loss: 0.3744349623453417\n",
      "Iteration: 8006, training loss: 0.37443246972515626\n",
      "Iteration: 8007, training loss: 0.3744299775132053\n",
      "Iteration: 8008, training loss: 0.3744274857093673\n",
      "Iteration: 8009, training loss: 0.37442499431352105\n",
      "Iteration: 8010, training loss: 0.3744225033255453\n",
      "Iteration: 8011, training loss: 0.3744200127453187\n",
      "Iteration: 8012, training loss: 0.3744175225727201\n",
      "Iteration: 8013, training loss: 0.3744150328076286\n",
      "Iteration: 8014, training loss: 0.3744125434499229\n",
      "Iteration: 8015, training loss: 0.37441005449948217\n",
      "Iteration: 8016, training loss: 0.3744075659561854\n",
      "Iteration: 8017, training loss: 0.3744050778199116\n",
      "Iteration: 8018, training loss: 0.37440259009054017\n",
      "Iteration: 8019, training loss: 0.3744001027679501\n",
      "Iteration: 8020, training loss: 0.3743976158520207\n",
      "Iteration: 8021, training loss: 0.3743951293426313\n",
      "Iteration: 8022, training loss: 0.3743926432396613\n",
      "Iteration: 8023, training loss: 0.3743901575429902\n",
      "Iteration: 8024, training loss: 0.3743876722524973\n",
      "Iteration: 8025, training loss: 0.37438518736806226\n",
      "Iteration: 8026, training loss: 0.3743827028895646\n",
      "Iteration: 8027, training loss: 0.374380218816884\n",
      "Iteration: 8028, training loss: 0.3743777351499001\n",
      "Iteration: 8029, training loss: 0.3743752518884926\n",
      "Iteration: 8030, training loss: 0.3743727690325414\n",
      "Iteration: 8031, training loss: 0.3743702865819262\n",
      "Iteration: 8032, training loss: 0.3743678045365272\n",
      "Iteration: 8033, training loss: 0.374365322896224\n",
      "Iteration: 8034, training loss: 0.37436284166089673\n",
      "Iteration: 8035, training loss: 0.3743603608304255\n",
      "Iteration: 8036, training loss: 0.3743578804046904\n",
      "Iteration: 8037, training loss: 0.37435540038357157\n",
      "Iteration: 8038, training loss: 0.37435292076694926\n",
      "Iteration: 8039, training loss: 0.37435044155470365\n",
      "Iteration: 8040, training loss: 0.3743479627467151\n",
      "Iteration: 8041, training loss: 0.374345484342864\n",
      "Iteration: 8042, training loss: 0.37434300634303086\n",
      "Iteration: 8043, training loss: 0.374340528747096\n",
      "Iteration: 8044, training loss: 0.37433805155494004\n",
      "Iteration: 8045, training loss: 0.3743355747664434\n",
      "Iteration: 8046, training loss: 0.37433309838148693\n",
      "Iteration: 8047, training loss: 0.3743306223999512\n",
      "Iteration: 8048, training loss: 0.374328146821717\n",
      "Iteration: 8049, training loss: 0.3743256716466651\n",
      "Iteration: 8050, training loss: 0.3743231968746763\n",
      "Iteration: 8051, training loss: 0.37432072250563153\n",
      "Iteration: 8052, training loss: 0.3743182485394117\n",
      "Iteration: 8053, training loss: 0.37431577497589785\n",
      "Iteration: 8054, training loss: 0.374313301814971\n",
      "Iteration: 8055, training loss: 0.37431082905651225\n",
      "Iteration: 8056, training loss: 0.37430835670040286\n",
      "Iteration: 8057, training loss: 0.3743058847465238\n",
      "Iteration: 8058, training loss: 0.3743034131947566\n",
      "Iteration: 8059, training loss: 0.3743009420449823\n",
      "Iteration: 8060, training loss: 0.3742984712970825\n",
      "Iteration: 8061, training loss: 0.3742960009509385\n",
      "Iteration: 8062, training loss: 0.3742935310064317\n",
      "Iteration: 8063, training loss: 0.37429106146344376\n",
      "Iteration: 8064, training loss: 0.374288592321856\n",
      "Iteration: 8065, training loss: 0.37428612358155033\n",
      "Iteration: 8066, training loss: 0.37428365524240836\n",
      "Iteration: 8067, training loss: 0.3742811873043116\n",
      "Iteration: 8068, training loss: 0.37427871976714217\n",
      "Iteration: 8069, training loss: 0.3742762526307815\n",
      "Iteration: 8070, training loss: 0.37427378589511173\n",
      "Iteration: 8071, training loss: 0.3742713195600147\n",
      "Iteration: 8072, training loss: 0.3742688536253725\n",
      "Iteration: 8073, training loss: 0.374266388091067\n",
      "Iteration: 8074, training loss: 0.37426392295698035\n",
      "Iteration: 8075, training loss: 0.3742614582229946\n",
      "Iteration: 8076, training loss: 0.3742589938889922\n",
      "Iteration: 8077, training loss: 0.37425652995485514\n",
      "Iteration: 8078, training loss: 0.3742540664204656\n",
      "Iteration: 8079, training loss: 0.3742516032857064\n",
      "Iteration: 8080, training loss: 0.3742491405504594\n",
      "Iteration: 8081, training loss: 0.37424667821460733\n",
      "Iteration: 8082, training loss: 0.37424421627803256\n",
      "Iteration: 8083, training loss: 0.3742417547406177\n",
      "Iteration: 8084, training loss: 0.3742392936022454\n",
      "Iteration: 8085, training loss: 0.37423683286279813\n",
      "Iteration: 8086, training loss: 0.37423437252215874\n",
      "Iteration: 8087, training loss: 0.3742319125802099\n",
      "Iteration: 8088, training loss: 0.37422945303683447\n",
      "Iteration: 8089, training loss: 0.3742269938919152\n",
      "Iteration: 8090, training loss: 0.37422453514533516\n",
      "Iteration: 8091, training loss: 0.3742220767969773\n",
      "Iteration: 8092, training loss: 0.3742196188467243\n",
      "Iteration: 8093, training loss: 0.37421716129445953\n",
      "Iteration: 8094, training loss: 0.37421470414006586\n",
      "Iteration: 8095, training loss: 0.3742122473834268\n",
      "Iteration: 8096, training loss: 0.37420979102442514\n",
      "Iteration: 8097, training loss: 0.37420733506294435\n",
      "Iteration: 8098, training loss: 0.3742048794988679\n",
      "Iteration: 8099, training loss: 0.3742024243320788\n",
      "Iteration: 8100, training loss: 0.37419996956246054\n",
      "Iteration: 8101, training loss: 0.37419751518989675\n",
      "Iteration: 8102, training loss: 0.3741950612142709\n",
      "Iteration: 8103, training loss: 0.3741926076354664\n",
      "Iteration: 8104, training loss: 0.374190154453367\n",
      "Iteration: 8105, training loss: 0.3741877016678564\n",
      "Iteration: 8106, training loss: 0.3741852492788181\n",
      "Iteration: 8107, training loss: 0.374182797286136\n",
      "Iteration: 8108, training loss: 0.374180345689694\n",
      "Iteration: 8109, training loss: 0.3741778944893757\n",
      "Iteration: 8110, training loss: 0.37417544368506533\n",
      "Iteration: 8111, training loss: 0.3741729932766466\n",
      "Iteration: 8112, training loss: 0.37417054326400373\n",
      "Iteration: 8113, training loss: 0.37416809364702047\n",
      "Iteration: 8114, training loss: 0.37416564442558126\n",
      "Iteration: 8115, training loss: 0.3741631955995702\n",
      "Iteration: 8116, training loss: 0.37416074716887127\n",
      "Iteration: 8117, training loss: 0.37415829913336907\n",
      "Iteration: 8118, training loss: 0.37415585149294767\n",
      "Iteration: 8119, training loss: 0.37415340424749144\n",
      "Iteration: 8120, training loss: 0.3741509573968851\n",
      "Iteration: 8121, training loss: 0.3741485109410126\n",
      "Iteration: 8122, training loss: 0.37414606487975893\n",
      "Iteration: 8123, training loss: 0.37414361921300837\n",
      "Iteration: 8124, training loss: 0.3741411739406457\n",
      "Iteration: 8125, training loss: 0.3741387290625554\n",
      "Iteration: 8126, training loss: 0.3741362845786223\n",
      "Iteration: 8127, training loss: 0.3741338404887313\n",
      "Iteration: 8128, training loss: 0.37413139679276686\n",
      "Iteration: 8129, training loss: 0.3741289534906142\n",
      "Iteration: 8130, training loss: 0.37412651058215807\n",
      "Iteration: 8131, training loss: 0.3741240680672834\n",
      "Iteration: 8132, training loss: 0.3741216259458753\n",
      "Iteration: 8133, training loss: 0.3741191842178187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8134, training loss: 0.3741167428829989\n",
      "Iteration: 8135, training loss: 0.3741143019413008\n",
      "Iteration: 8136, training loss: 0.37411186139260993\n",
      "Iteration: 8137, training loss: 0.3741094212368113\n",
      "Iteration: 8138, training loss: 0.37410698147379023\n",
      "Iteration: 8139, training loss: 0.3741045421034323\n",
      "Iteration: 8140, training loss: 0.37410210312562275\n",
      "Iteration: 8141, training loss: 0.374099664540247\n",
      "Iteration: 8142, training loss: 0.37409722634719067\n",
      "Iteration: 8143, training loss: 0.37409478854633926\n",
      "Iteration: 8144, training loss: 0.3740923511375784\n",
      "Iteration: 8145, training loss: 0.37408991412079373\n",
      "Iteration: 8146, training loss: 0.37408747749587096\n",
      "Iteration: 8147, training loss: 0.37408504126269587\n",
      "Iteration: 8148, training loss: 0.3740826054211542\n",
      "Iteration: 8149, training loss: 0.37408016997113175\n",
      "Iteration: 8150, training loss: 0.37407773491251456\n",
      "Iteration: 8151, training loss: 0.3740753002451886\n",
      "Iteration: 8152, training loss: 0.3740728659690398\n",
      "Iteration: 8153, training loss: 0.3740704320839542\n",
      "Iteration: 8154, training loss: 0.37406799858981793\n",
      "Iteration: 8155, training loss: 0.3740655654865171\n",
      "Iteration: 8156, training loss: 0.3740631327739379\n",
      "Iteration: 8157, training loss: 0.3740607004519666\n",
      "Iteration: 8158, training loss: 0.37405826852048957\n",
      "Iteration: 8159, training loss: 0.374055836979393\n",
      "Iteration: 8160, training loss: 0.3740534058285634\n",
      "Iteration: 8161, training loss: 0.3740509750678872\n",
      "Iteration: 8162, training loss: 0.37404854469725074\n",
      "Iteration: 8163, training loss: 0.37404611471654087\n",
      "Iteration: 8164, training loss: 0.3740436851256439\n",
      "Iteration: 8165, training loss: 0.37404125592444665\n",
      "Iteration: 8166, training loss: 0.3740388271128357\n",
      "Iteration: 8167, training loss: 0.37403639869069777\n",
      "Iteration: 8168, training loss: 0.3740339706579197\n",
      "Iteration: 8169, training loss: 0.3740315430143883\n",
      "Iteration: 8170, training loss: 0.37402911575999037\n",
      "Iteration: 8171, training loss: 0.374026688894613\n",
      "Iteration: 8172, training loss: 0.37402426241814307\n",
      "Iteration: 8173, training loss: 0.3740218363304676\n",
      "Iteration: 8174, training loss: 0.3740194106314737\n",
      "Iteration: 8175, training loss: 0.3740169853210486\n",
      "Iteration: 8176, training loss: 0.3740145603990792\n",
      "Iteration: 8177, training loss: 0.37401213586545295\n",
      "Iteration: 8178, training loss: 0.37400971172005704\n",
      "Iteration: 8179, training loss: 0.37400728796277877\n",
      "Iteration: 8180, training loss: 0.37400486459350557\n",
      "Iteration: 8181, training loss: 0.37400244161212487\n",
      "Iteration: 8182, training loss: 0.374000019018524\n",
      "Iteration: 8183, training loss: 0.3739975968125905\n",
      "Iteration: 8184, training loss: 0.3739951749942121\n",
      "Iteration: 8185, training loss: 0.3739927535632761\n",
      "Iteration: 8186, training loss: 0.37399033251967045\n",
      "Iteration: 8187, training loss: 0.3739879118632828\n",
      "Iteration: 8188, training loss: 0.3739854915940008\n",
      "Iteration: 8189, training loss: 0.3739830717117123\n",
      "Iteration: 8190, training loss: 0.3739806522163051\n",
      "Iteration: 8191, training loss: 0.37397823310766715\n",
      "Iteration: 8192, training loss: 0.3739758143856864\n",
      "Iteration: 8193, training loss: 0.37397339605025076\n",
      "Iteration: 8194, training loss: 0.37397097810124846\n",
      "Iteration: 8195, training loss: 0.37396856053856736\n",
      "Iteration: 8196, training loss: 0.3739661433620957\n",
      "Iteration: 8197, training loss: 0.3739637265717217\n",
      "Iteration: 8198, training loss: 0.3739613101673336\n",
      "Iteration: 8199, training loss: 0.37395889414881955\n",
      "Iteration: 8200, training loss: 0.373956478516068\n",
      "Iteration: 8201, training loss: 0.37395406326896735\n",
      "Iteration: 8202, training loss: 0.37395164840740586\n",
      "Iteration: 8203, training loss: 0.37394923393127216\n",
      "Iteration: 8204, training loss: 0.3739468198404547\n",
      "Iteration: 8205, training loss: 0.37394440613484203\n",
      "Iteration: 8206, training loss: 0.37394199281432283\n",
      "Iteration: 8207, training loss: 0.3739395798787857\n",
      "Iteration: 8208, training loss: 0.3739371673281194\n",
      "Iteration: 8209, training loss: 0.37393475516221264\n",
      "Iteration: 8210, training loss: 0.37393234338095427\n",
      "Iteration: 8211, training loss: 0.3739299319842331\n",
      "Iteration: 8212, training loss: 0.37392752097193793\n",
      "Iteration: 8213, training loss: 0.373925110343958\n",
      "Iteration: 8214, training loss: 0.373922700100182\n",
      "Iteration: 8215, training loss: 0.37392029024049916\n",
      "Iteration: 8216, training loss: 0.37391788076479854\n",
      "Iteration: 8217, training loss: 0.37391547167296924\n",
      "Iteration: 8218, training loss: 0.37391306296490034\n",
      "Iteration: 8219, training loss: 0.3739106546404813\n",
      "Iteration: 8220, training loss: 0.3739082466996012\n",
      "Iteration: 8221, training loss: 0.3739058391421495\n",
      "Iteration: 8222, training loss: 0.3739034319680155\n",
      "Iteration: 8223, training loss: 0.3739010251770887\n",
      "Iteration: 8224, training loss: 0.37389861876925834\n",
      "Iteration: 8225, training loss: 0.3738962127444143\n",
      "Iteration: 8226, training loss: 0.37389380710244585\n",
      "Iteration: 8227, training loss: 0.37389140184324265\n",
      "Iteration: 8228, training loss: 0.3738889969666945\n",
      "Iteration: 8229, training loss: 0.3738865924726909\n",
      "Iteration: 8230, training loss: 0.37388418836112175\n",
      "Iteration: 8231, training loss: 0.37388178463187677\n",
      "Iteration: 8232, training loss: 0.3738793812848459\n",
      "Iteration: 8233, training loss: 0.3738769783199189\n",
      "Iteration: 8234, training loss: 0.3738745757369858\n",
      "Iteration: 8235, training loss: 0.37387217353593655\n",
      "Iteration: 8236, training loss: 0.3738697717166612\n",
      "Iteration: 8237, training loss: 0.37386737027904976\n",
      "Iteration: 8238, training loss: 0.37386496922299245\n",
      "Iteration: 8239, training loss: 0.37386256854837946\n",
      "Iteration: 8240, training loss: 0.373860168255101\n",
      "Iteration: 8241, training loss: 0.3738577683430471\n",
      "Iteration: 8242, training loss: 0.37385536881210846\n",
      "Iteration: 8243, training loss: 0.37385296966217507\n",
      "Iteration: 8244, training loss: 0.3738505708931376\n",
      "Iteration: 8245, training loss: 0.37384817250488644\n",
      "Iteration: 8246, training loss: 0.373845774497312\n",
      "Iteration: 8247, training loss: 0.3738433768703049\n",
      "Iteration: 8248, training loss: 0.37384097962375573\n",
      "Iteration: 8249, training loss: 0.3738385827575552\n",
      "Iteration: 8250, training loss: 0.37383618627159376\n",
      "Iteration: 8251, training loss: 0.3738337901657624\n",
      "Iteration: 8252, training loss: 0.37383139443995167\n",
      "Iteration: 8253, training loss: 0.3738289990940527\n",
      "Iteration: 8254, training loss: 0.3738266041279561\n",
      "Iteration: 8255, training loss: 0.3738242095415528\n",
      "Iteration: 8256, training loss: 0.37382181533473385\n",
      "Iteration: 8257, training loss: 0.37381942150739034\n",
      "Iteration: 8258, training loss: 0.37381702805941314\n",
      "Iteration: 8259, training loss: 0.37381463499069345\n",
      "Iteration: 8260, training loss: 0.37381224230112237\n",
      "Iteration: 8261, training loss: 0.37380984999059114\n",
      "Iteration: 8262, training loss: 0.3738074580589911\n",
      "Iteration: 8263, training loss: 0.37380506650621337\n",
      "Iteration: 8264, training loss: 0.37380267533214934\n",
      "Iteration: 8265, training loss: 0.3738002845366904\n",
      "Iteration: 8266, training loss: 0.373797894119728\n",
      "Iteration: 8267, training loss: 0.37379550408115353\n",
      "Iteration: 8268, training loss: 0.3737931144208586\n",
      "Iteration: 8269, training loss: 0.3737907251387348\n",
      "Iteration: 8270, training loss: 0.3737883362346736\n",
      "Iteration: 8271, training loss: 0.3737859477085667\n",
      "Iteration: 8272, training loss: 0.3737835595603059\n",
      "Iteration: 8273, training loss: 0.3737811717897829\n",
      "Iteration: 8274, training loss: 0.3737787843968894\n",
      "Iteration: 8275, training loss: 0.37377639738151736\n",
      "Iteration: 8276, training loss: 0.37377401074355865\n",
      "Iteration: 8277, training loss: 0.37377162448290513\n",
      "Iteration: 8278, training loss: 0.3737692385994488\n",
      "Iteration: 8279, training loss: 0.3737668530930817\n",
      "Iteration: 8280, training loss: 0.373764467963696\n",
      "Iteration: 8281, training loss: 0.3737620832111836\n",
      "Iteration: 8282, training loss: 0.3737596988354369\n",
      "Iteration: 8283, training loss: 0.37375731483634794\n",
      "Iteration: 8284, training loss: 0.373754931213809\n",
      "Iteration: 8285, training loss: 0.3737525479677124\n",
      "Iteration: 8286, training loss: 0.37375016509795034\n",
      "Iteration: 8287, training loss: 0.3737477826044155\n",
      "Iteration: 8288, training loss: 0.3737454004870001\n",
      "Iteration: 8289, training loss: 0.3737430187455967\n",
      "Iteration: 8290, training loss: 0.3737406373800978\n",
      "Iteration: 8291, training loss: 0.37373825639039593\n",
      "Iteration: 8292, training loss: 0.3737358757763837\n",
      "Iteration: 8293, training loss: 0.3737334955379539\n",
      "Iteration: 8294, training loss: 0.37373111567499917\n",
      "Iteration: 8295, training loss: 0.37372873618741215\n",
      "Iteration: 8296, training loss: 0.3737263570750858\n",
      "Iteration: 8297, training loss: 0.3737239783379128\n",
      "Iteration: 8298, training loss: 0.37372159997578625\n",
      "Iteration: 8299, training loss: 0.37371922198859886\n",
      "Iteration: 8300, training loss: 0.37371684437624375\n",
      "Iteration: 8301, training loss: 0.3737144671386139\n",
      "Iteration: 8302, training loss: 0.3737120902756024\n",
      "Iteration: 8303, training loss: 0.3737097137871024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8304, training loss: 0.3737073376730069\n",
      "Iteration: 8305, training loss: 0.37370496193320923\n",
      "Iteration: 8306, training loss: 0.3737025865676027\n",
      "Iteration: 8307, training loss: 0.37370021157608035\n",
      "Iteration: 8308, training loss: 0.3736978369585358\n",
      "Iteration: 8309, training loss: 0.37369546271486237\n",
      "Iteration: 8310, training loss: 0.37369308884495345\n",
      "Iteration: 8311, training loss: 0.3736907153487023\n",
      "Iteration: 8312, training loss: 0.37368834222600283\n",
      "Iteration: 8313, training loss: 0.3736859694767483\n",
      "Iteration: 8314, training loss: 0.37368359710083254\n",
      "Iteration: 8315, training loss: 0.3736812250981491\n",
      "Iteration: 8316, training loss: 0.37367885346859164\n",
      "Iteration: 8317, training loss: 0.3736764822120539\n",
      "Iteration: 8318, training loss: 0.37367411132842976\n",
      "Iteration: 8319, training loss: 0.37367174081761295\n",
      "Iteration: 8320, training loss: 0.3736693706794974\n",
      "Iteration: 8321, training loss: 0.3736670009139771\n",
      "Iteration: 8322, training loss: 0.37366463152094587\n",
      "Iteration: 8323, training loss: 0.3736622625002978\n",
      "Iteration: 8324, training loss: 0.37365989385192705\n",
      "Iteration: 8325, training loss: 0.3736575255757275\n",
      "Iteration: 8326, training loss: 0.37365515767159346\n",
      "Iteration: 8327, training loss: 0.3736527901394191\n",
      "Iteration: 8328, training loss: 0.3736504229790985\n",
      "Iteration: 8329, training loss: 0.37364805619052616\n",
      "Iteration: 8330, training loss: 0.3736456897735963\n",
      "Iteration: 8331, training loss: 0.37364332372820325\n",
      "Iteration: 8332, training loss: 0.37364095805424147\n",
      "Iteration: 8333, training loss: 0.37363859275160544\n",
      "Iteration: 8334, training loss: 0.3736362278201896\n",
      "Iteration: 8335, training loss: 0.37363386325988857\n",
      "Iteration: 8336, training loss: 0.3736314990705969\n",
      "Iteration: 8337, training loss: 0.3736291352522092\n",
      "Iteration: 8338, training loss: 0.3736267718046201\n",
      "Iteration: 8339, training loss: 0.37362440872772446\n",
      "Iteration: 8340, training loss: 0.373622046021417\n",
      "Iteration: 8341, training loss: 0.37361968368559245\n",
      "Iteration: 8342, training loss: 0.3736173217201457\n",
      "Iteration: 8343, training loss: 0.3736149601249717\n",
      "Iteration: 8344, training loss: 0.37361259889996545\n",
      "Iteration: 8345, training loss: 0.3736102380450217\n",
      "Iteration: 8346, training loss: 0.3736078775600357\n",
      "Iteration: 8347, training loss: 0.37360551744490256\n",
      "Iteration: 8348, training loss: 0.3736031576995172\n",
      "Iteration: 8349, training loss: 0.37360079832377485\n",
      "Iteration: 8350, training loss: 0.37359843931757075\n",
      "Iteration: 8351, training loss: 0.3735960806808002\n",
      "Iteration: 8352, training loss: 0.3735937224133584\n",
      "Iteration: 8353, training loss: 0.3735913645151408\n",
      "Iteration: 8354, training loss: 0.3735890069860427\n",
      "Iteration: 8355, training loss: 0.3735866498259595\n",
      "Iteration: 8356, training loss: 0.3735842930347868\n",
      "Iteration: 8357, training loss: 0.37358193661241995\n",
      "Iteration: 8358, training loss: 0.37357958055875456\n",
      "Iteration: 8359, training loss: 0.3735772248736864\n",
      "Iteration: 8360, training loss: 0.3735748695571109\n",
      "Iteration: 8361, training loss: 0.3735725146089237\n",
      "Iteration: 8362, training loss: 0.3735701600290208\n",
      "Iteration: 8363, training loss: 0.37356780581729787\n",
      "Iteration: 8364, training loss: 0.3735654519736506\n",
      "Iteration: 8365, training loss: 0.373563098497975\n",
      "Iteration: 8366, training loss: 0.373560745390167\n",
      "Iteration: 8367, training loss: 0.3735583926501224\n",
      "Iteration: 8368, training loss: 0.3735560402777373\n",
      "Iteration: 8369, training loss: 0.37355368827290775\n",
      "Iteration: 8370, training loss: 0.3735513366355297\n",
      "Iteration: 8371, training loss: 0.37354898536549946\n",
      "Iteration: 8372, training loss: 0.3735466344627131\n",
      "Iteration: 8373, training loss: 0.37354428392706684\n",
      "Iteration: 8374, training loss: 0.3735419337584569\n",
      "Iteration: 8375, training loss: 0.37353958395677966\n",
      "Iteration: 8376, training loss: 0.3735372345219314\n",
      "Iteration: 8377, training loss: 0.37353488545380853\n",
      "Iteration: 8378, training loss: 0.3735325367523075\n",
      "Iteration: 8379, training loss: 0.3735301884173248\n",
      "Iteration: 8380, training loss: 0.37352784044875686\n",
      "Iteration: 8381, training loss: 0.37352549284650033\n",
      "Iteration: 8382, training loss: 0.37352314561045175\n",
      "Iteration: 8383, training loss: 0.3735207987405078\n",
      "Iteration: 8384, training loss: 0.373518452236565\n",
      "Iteration: 8385, training loss: 0.37351610609852043\n",
      "Iteration: 8386, training loss: 0.3735137603262706\n",
      "Iteration: 8387, training loss: 0.37351141491971246\n",
      "Iteration: 8388, training loss: 0.3735090698787427\n",
      "Iteration: 8389, training loss: 0.3735067252032584\n",
      "Iteration: 8390, training loss: 0.3735043808931564\n",
      "Iteration: 8391, training loss: 0.3735020369483337\n",
      "Iteration: 8392, training loss: 0.37349969336868744\n",
      "Iteration: 8393, training loss: 0.3734973501541146\n",
      "Iteration: 8394, training loss: 0.37349500730451235\n",
      "Iteration: 8395, training loss: 0.37349266481977783\n",
      "Iteration: 8396, training loss: 0.37349032269980814\n",
      "Iteration: 8397, training loss: 0.37348798094450075\n",
      "Iteration: 8398, training loss: 0.37348563955375275\n",
      "Iteration: 8399, training loss: 0.37348329852746154\n",
      "Iteration: 8400, training loss: 0.37348095786552443\n",
      "Iteration: 8401, training loss: 0.37347861756783907\n",
      "Iteration: 8402, training loss: 0.3734762776343027\n",
      "Iteration: 8403, training loss: 0.3734739380648128\n",
      "Iteration: 8404, training loss: 0.3734715988592672\n",
      "Iteration: 8405, training loss: 0.37346926001756314\n",
      "Iteration: 8406, training loss: 0.3734669215395985\n",
      "Iteration: 8407, training loss: 0.3734645834252708\n",
      "Iteration: 8408, training loss: 0.3734622456744778\n",
      "Iteration: 8409, training loss: 0.3734599082871173\n",
      "Iteration: 8410, training loss: 0.3734575712630871\n",
      "Iteration: 8411, training loss: 0.3734552346022851\n",
      "Iteration: 8412, training loss: 0.373452898304609\n",
      "Iteration: 8413, training loss: 0.3734505623699569\n",
      "Iteration: 8414, training loss: 0.3734482267982267\n",
      "Iteration: 8415, training loss: 0.37344589158931646\n",
      "Iteration: 8416, training loss: 0.3734435567431242\n",
      "Iteration: 8417, training loss: 0.37344122225954796\n",
      "Iteration: 8418, training loss: 0.3734388881384861\n",
      "Iteration: 8419, training loss: 0.3734365543798365\n",
      "Iteration: 8420, training loss: 0.37343422098349766\n",
      "Iteration: 8421, training loss: 0.3734318879493677\n",
      "Iteration: 8422, training loss: 0.37342955527734495\n",
      "Iteration: 8423, training loss: 0.3734272229673278\n",
      "Iteration: 8424, training loss: 0.3734248910192146\n",
      "Iteration: 8425, training loss: 0.37342255943290387\n",
      "Iteration: 8426, training loss: 0.37342022820829396\n",
      "Iteration: 8427, training loss: 0.3734178973452836\n",
      "Iteration: 8428, training loss: 0.3734155668437711\n",
      "Iteration: 8429, training loss: 0.3734132367036552\n",
      "Iteration: 8430, training loss: 0.3734109069248346\n",
      "Iteration: 8431, training loss: 0.3734085775072079\n",
      "Iteration: 8432, training loss: 0.3734062484506738\n",
      "Iteration: 8433, training loss: 0.3734039197551313\n",
      "Iteration: 8434, training loss: 0.3734015914204789\n",
      "Iteration: 8435, training loss: 0.37339926344661567\n",
      "Iteration: 8436, training loss: 0.37339693583344036\n",
      "Iteration: 8437, training loss: 0.37339460858085216\n",
      "Iteration: 8438, training loss: 0.3733922816887498\n",
      "Iteration: 8439, training loss: 0.3733899551570325\n",
      "Iteration: 8440, training loss: 0.37338762898559924\n",
      "Iteration: 8441, training loss: 0.373385303174349\n",
      "Iteration: 8442, training loss: 0.37338297772318113\n",
      "Iteration: 8443, training loss: 0.3733806526319947\n",
      "Iteration: 8444, training loss: 0.3733783279006891\n",
      "Iteration: 8445, training loss: 0.37337600352916345\n",
      "Iteration: 8446, training loss: 0.37337367951731715\n",
      "Iteration: 8447, training loss: 0.3733713558650495\n",
      "Iteration: 8448, training loss: 0.37336903257225995\n",
      "Iteration: 8449, training loss: 0.37336670963884794\n",
      "Iteration: 8450, training loss: 0.373364387064713\n",
      "Iteration: 8451, training loss: 0.3733620648497545\n",
      "Iteration: 8452, training loss: 0.37335974299387215\n",
      "Iteration: 8453, training loss: 0.37335742149696566\n",
      "Iteration: 8454, training loss: 0.37335510035893443\n",
      "Iteration: 8455, training loss: 0.3733527795796783\n",
      "Iteration: 8456, training loss: 0.37335045915909687\n",
      "Iteration: 8457, training loss: 0.3733481390970901\n",
      "Iteration: 8458, training loss: 0.3733458193935578\n",
      "Iteration: 8459, training loss: 0.37334350004839967\n",
      "Iteration: 8460, training loss: 0.37334118106151576\n",
      "Iteration: 8461, training loss: 0.37333886243280595\n",
      "Iteration: 8462, training loss: 0.37333654416217016\n",
      "Iteration: 8463, training loss: 0.37333422624950846\n",
      "Iteration: 8464, training loss: 0.3733319086947211\n",
      "Iteration: 8465, training loss: 0.3733295914977079\n",
      "Iteration: 8466, training loss: 0.3733272746583692\n",
      "Iteration: 8467, training loss: 0.3733249581766051\n",
      "Iteration: 8468, training loss: 0.37332264205231586\n",
      "Iteration: 8469, training loss: 0.37332032628540174\n",
      "Iteration: 8470, training loss: 0.37331801087576305\n",
      "Iteration: 8471, training loss: 0.37331569582330015\n",
      "Iteration: 8472, training loss: 0.37331338112791346\n",
      "Iteration: 8473, training loss: 0.37331106678950343\n",
      "Iteration: 8474, training loss: 0.3733087528079705\n",
      "Iteration: 8475, training loss: 0.3733064391832152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8476, training loss: 0.3733041259151381\n",
      "Iteration: 8477, training loss: 0.3733018130036397\n",
      "Iteration: 8478, training loss: 0.37329950044862087\n",
      "Iteration: 8479, training loss: 0.373297188249982\n",
      "Iteration: 8480, training loss: 0.3732948764076241\n",
      "Iteration: 8481, training loss: 0.37329256492144774\n",
      "Iteration: 8482, training loss: 0.3732902537913538\n",
      "Iteration: 8483, training loss: 0.37328794301724316\n",
      "Iteration: 8484, training loss: 0.3732856325990166\n",
      "Iteration: 8485, training loss: 0.37328332253657515\n",
      "Iteration: 8486, training loss: 0.3732810128298197\n",
      "Iteration: 8487, training loss: 0.3732787034786513\n",
      "Iteration: 8488, training loss: 0.37327639448297106\n",
      "Iteration: 8489, training loss: 0.37327408584268007\n",
      "Iteration: 8490, training loss: 0.37327177755767926\n",
      "Iteration: 8491, training loss: 0.37326946962787\n",
      "Iteration: 8492, training loss: 0.37326716205315347\n",
      "Iteration: 8493, training loss: 0.3732648548334309\n",
      "Iteration: 8494, training loss: 0.37326254796860364\n",
      "Iteration: 8495, training loss: 0.37326024145857295\n",
      "Iteration: 8496, training loss: 0.3732579353032403\n",
      "Iteration: 8497, training loss: 0.3732556295025069\n",
      "Iteration: 8498, training loss: 0.3732533240562744\n",
      "Iteration: 8499, training loss: 0.3732510189644443\n",
      "Iteration: 8500, training loss: 0.37324871422691813\n",
      "Iteration: 8501, training loss: 0.3732464098435974\n",
      "Iteration: 8502, training loss: 0.37324410581438366\n",
      "Iteration: 8503, training loss: 0.3732418021391788\n",
      "Iteration: 8504, training loss: 0.3732394988178842\n",
      "Iteration: 8505, training loss: 0.373237195850402\n",
      "Iteration: 8506, training loss: 0.3732348932366336\n",
      "Iteration: 8507, training loss: 0.3732325909764811\n",
      "Iteration: 8508, training loss: 0.37323028906984623\n",
      "Iteration: 8509, training loss: 0.3732279875166309\n",
      "Iteration: 8510, training loss: 0.373225686316737\n",
      "Iteration: 8511, training loss: 0.3732233854700666\n",
      "Iteration: 8512, training loss: 0.3732210849765218\n",
      "Iteration: 8513, training loss: 0.37321878483600446\n",
      "Iteration: 8514, training loss: 0.3732164850484169\n",
      "Iteration: 8515, training loss: 0.37321418561366104\n",
      "Iteration: 8516, training loss: 0.3732118865316392\n",
      "Iteration: 8517, training loss: 0.37320958780225366\n",
      "Iteration: 8518, training loss: 0.37320728942540654\n",
      "Iteration: 8519, training loss: 0.3732049914010002\n",
      "Iteration: 8520, training loss: 0.373202693728937\n",
      "Iteration: 8521, training loss: 0.3732003964091193\n",
      "Iteration: 8522, training loss: 0.37319809944144955\n",
      "Iteration: 8523, training loss: 0.37319580282583026\n",
      "Iteration: 8524, training loss: 0.3731935065621637\n",
      "Iteration: 8525, training loss: 0.3731912106503528\n",
      "Iteration: 8526, training loss: 0.37318891509029983\n",
      "Iteration: 8527, training loss: 0.37318661988190754\n",
      "Iteration: 8528, training loss: 0.3731843250250786\n",
      "Iteration: 8529, training loss: 0.37318203051971566\n",
      "Iteration: 8530, training loss: 0.3731797363657214\n",
      "Iteration: 8531, training loss: 0.37317744256299884\n",
      "Iteration: 8532, training loss: 0.37317514911145055\n",
      "Iteration: 8533, training loss: 0.37317285601097955\n",
      "Iteration: 8534, training loss: 0.37317056326148873\n",
      "Iteration: 8535, training loss: 0.37316827086288096\n",
      "Iteration: 8536, training loss: 0.37316597881505914\n",
      "Iteration: 8537, training loss: 0.3731636871179265\n",
      "Iteration: 8538, training loss: 0.3731613957713861\n",
      "Iteration: 8539, training loss: 0.37315910477534087\n",
      "Iteration: 8540, training loss: 0.37315681412969404\n",
      "Iteration: 8541, training loss: 0.37315452383434883\n",
      "Iteration: 8542, training loss: 0.3731522338892084\n",
      "Iteration: 8543, training loss: 0.373149944294176\n",
      "Iteration: 8544, training loss: 0.3731476550491548\n",
      "Iteration: 8545, training loss: 0.37314536615404853\n",
      "Iteration: 8546, training loss: 0.37314307760876025\n",
      "Iteration: 8547, training loss: 0.37314078941319334\n",
      "Iteration: 8548, training loss: 0.37313850156725153\n",
      "Iteration: 8549, training loss: 0.373136214070838\n",
      "Iteration: 8550, training loss: 0.37313392692385666\n",
      "Iteration: 8551, training loss: 0.37313164012621075\n",
      "Iteration: 8552, training loss: 0.373129353677804\n",
      "Iteration: 8553, training loss: 0.3731270675785401\n",
      "Iteration: 8554, training loss: 0.37312478182832265\n",
      "Iteration: 8555, training loss: 0.37312249642705553\n",
      "Iteration: 8556, training loss: 0.3731202113746424\n",
      "Iteration: 8557, training loss: 0.3731179266709871\n",
      "Iteration: 8558, training loss: 0.3731156423159935\n",
      "Iteration: 8559, training loss: 0.3731133583095654\n",
      "Iteration: 8560, training loss: 0.3731110746516068\n",
      "Iteration: 8561, training loss: 0.37310879134202174\n",
      "Iteration: 8562, training loss: 0.3731065083807142\n",
      "Iteration: 8563, training loss: 0.3731042257675881\n",
      "Iteration: 8564, training loss: 0.37310194350254766\n",
      "Iteration: 8565, training loss: 0.37309966158549684\n",
      "Iteration: 8566, training loss: 0.37309738001634\n",
      "Iteration: 8567, training loss: 0.3730950987949813\n",
      "Iteration: 8568, training loss: 0.37309281792132487\n",
      "Iteration: 8569, training loss: 0.3730905373952751\n",
      "Iteration: 8570, training loss: 0.37308825721673633\n",
      "Iteration: 8571, training loss: 0.3730859773856127\n",
      "Iteration: 8572, training loss: 0.37308369790180884\n",
      "Iteration: 8573, training loss: 0.37308141876522916\n",
      "Iteration: 8574, training loss: 0.37307913997577813\n",
      "Iteration: 8575, training loss: 0.37307686153336017\n",
      "Iteration: 8576, training loss: 0.3730745834378799\n",
      "Iteration: 8577, training loss: 0.3730723056892418\n",
      "Iteration: 8578, training loss: 0.3730700282873508\n",
      "Iteration: 8579, training loss: 0.3730677512321112\n",
      "Iteration: 8580, training loss: 0.373065474523428\n",
      "Iteration: 8581, training loss: 0.3730631981612058\n",
      "Iteration: 8582, training loss: 0.3730609221453494\n",
      "Iteration: 8583, training loss: 0.37305864647576353\n",
      "Iteration: 8584, training loss: 0.3730563711523534\n",
      "Iteration: 8585, training loss: 0.37305409617502355\n",
      "Iteration: 8586, training loss: 0.37305182154367905\n",
      "Iteration: 8587, training loss: 0.373049547258225\n",
      "Iteration: 8588, training loss: 0.37304727331856613\n",
      "Iteration: 8589, training loss: 0.3730449997246077\n",
      "Iteration: 8590, training loss: 0.37304272647625486\n",
      "Iteration: 8591, training loss: 0.37304045357341253\n",
      "Iteration: 8592, training loss: 0.37303818101598607\n",
      "Iteration: 8593, training loss: 0.37303590880388066\n",
      "Iteration: 8594, training loss: 0.37303363693700153\n",
      "Iteration: 8595, training loss: 0.37303136541525383\n",
      "Iteration: 8596, training loss: 0.37302909423854314\n",
      "Iteration: 8597, training loss: 0.3730268234067746\n",
      "Iteration: 8598, training loss: 0.37302455291985376\n",
      "Iteration: 8599, training loss: 0.373022282777686\n",
      "Iteration: 8600, training loss: 0.3730200129801769\n",
      "Iteration: 8601, training loss: 0.3730177435272318\n",
      "Iteration: 8602, training loss: 0.3730154744187564\n",
      "Iteration: 8603, training loss: 0.3730132056546562\n",
      "Iteration: 8604, training loss: 0.37301093723483686\n",
      "Iteration: 8605, training loss: 0.3730086691592041\n",
      "Iteration: 8606, training loss: 0.3730064014276636\n",
      "Iteration: 8607, training loss: 0.37300413404012117\n",
      "Iteration: 8608, training loss: 0.3730018669964824\n",
      "Iteration: 8609, training loss: 0.37299960029665336\n",
      "Iteration: 8610, training loss: 0.37299733394053974\n",
      "Iteration: 8611, training loss: 0.37299506792804754\n",
      "Iteration: 8612, training loss: 0.3729928022590826\n",
      "Iteration: 8613, training loss: 0.372990536933551\n",
      "Iteration: 8614, training loss: 0.3729882719513588\n",
      "Iteration: 8615, training loss: 0.37298600731241177\n",
      "Iteration: 8616, training loss: 0.3729837430166164\n",
      "Iteration: 8617, training loss: 0.37298147906387846\n",
      "Iteration: 8618, training loss: 0.37297921545410434\n",
      "Iteration: 8619, training loss: 0.37297695218720023\n",
      "Iteration: 8620, training loss: 0.37297468926307226\n",
      "Iteration: 8621, training loss: 0.3729724266816268\n",
      "Iteration: 8622, training loss: 0.3729701644427701\n",
      "Iteration: 8623, training loss: 0.37296790254640866\n",
      "Iteration: 8624, training loss: 0.37296564099244867\n",
      "Iteration: 8625, training loss: 0.3729633797807967\n",
      "Iteration: 8626, training loss: 0.37296111891135925\n",
      "Iteration: 8627, training loss: 0.3729588583840427\n",
      "Iteration: 8628, training loss: 0.3729565981987537\n",
      "Iteration: 8629, training loss: 0.37295433835539876\n",
      "Iteration: 8630, training loss: 0.3729520788538846\n",
      "Iteration: 8631, training loss: 0.3729498196941178\n",
      "Iteration: 8632, training loss: 0.37294756087600506\n",
      "Iteration: 8633, training loss: 0.3729453023994531\n",
      "Iteration: 8634, training loss: 0.37294304426436875\n",
      "Iteration: 8635, training loss: 0.3729407864706588\n",
      "Iteration: 8636, training loss: 0.3729385290182301\n",
      "Iteration: 8637, training loss: 0.3729362719069896\n",
      "Iteration: 8638, training loss: 0.3729340151368441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8639, training loss: 0.3729317587077005\n",
      "Iteration: 8640, training loss: 0.372929502619466\n",
      "Iteration: 8641, training loss: 0.37292724687204754\n",
      "Iteration: 8642, training loss: 0.37292499146535224\n",
      "Iteration: 8643, training loss: 0.3729227363992872\n",
      "Iteration: 8644, training loss: 0.3729204816737594\n",
      "Iteration: 8645, training loss: 0.37291822728867624\n",
      "Iteration: 8646, training loss: 0.3729159732439449\n",
      "Iteration: 8647, training loss: 0.37291371953947255\n",
      "Iteration: 8648, training loss: 0.3729114661751665\n",
      "Iteration: 8649, training loss: 0.3729092131509343\n",
      "Iteration: 8650, training loss: 0.37290696046668304\n",
      "Iteration: 8651, training loss: 0.37290470812232024\n",
      "Iteration: 8652, training loss: 0.37290245611775347\n",
      "Iteration: 8653, training loss: 0.37290020445288996\n",
      "Iteration: 8654, training loss: 0.37289795312763746\n",
      "Iteration: 8655, training loss: 0.3728957021419033\n",
      "Iteration: 8656, training loss: 0.37289345149559533\n",
      "Iteration: 8657, training loss: 0.3728912011886211\n",
      "Iteration: 8658, training loss: 0.3728889512208881\n",
      "Iteration: 8659, training loss: 0.3728867015923043\n",
      "Iteration: 8660, training loss: 0.37288445230277734\n",
      "Iteration: 8661, training loss: 0.37288220335221495\n",
      "Iteration: 8662, training loss: 0.37287995474052504\n",
      "Iteration: 8663, training loss: 0.3728777064676153\n",
      "Iteration: 8664, training loss: 0.37287545853339393\n",
      "Iteration: 8665, training loss: 0.3728732109377685\n",
      "Iteration: 8666, training loss: 0.3728709636806472\n",
      "Iteration: 8667, training loss: 0.37286871676193806\n",
      "Iteration: 8668, training loss: 0.372866470181549\n",
      "Iteration: 8669, training loss: 0.3728642239393882\n",
      "Iteration: 8670, training loss: 0.37286197803536364\n",
      "Iteration: 8671, training loss: 0.3728597324693836\n",
      "Iteration: 8672, training loss: 0.3728574872413562\n",
      "Iteration: 8673, training loss: 0.37285524235118966\n",
      "Iteration: 8674, training loss: 0.3728529977987923\n",
      "Iteration: 8675, training loss: 0.3728507535840725\n",
      "Iteration: 8676, training loss: 0.37284850970693834\n",
      "Iteration: 8677, training loss: 0.37284626616729843\n",
      "Iteration: 8678, training loss: 0.37284402296506114\n",
      "Iteration: 8679, training loss: 0.37284178010013475\n",
      "Iteration: 8680, training loss: 0.3728395375724279\n",
      "Iteration: 8681, training loss: 0.3728372953818491\n",
      "Iteration: 8682, training loss: 0.3728350535283069\n",
      "Iteration: 8683, training loss: 0.37283281201170987\n",
      "Iteration: 8684, training loss: 0.3728305708319667\n",
      "Iteration: 8685, training loss: 0.3728283299889859\n",
      "Iteration: 8686, training loss: 0.3728260894826764\n",
      "Iteration: 8687, training loss: 0.37282384931294676\n",
      "Iteration: 8688, training loss: 0.3728216094797059\n",
      "Iteration: 8689, training loss: 0.3728193699828624\n",
      "Iteration: 8690, training loss: 0.3728171308223254\n",
      "Iteration: 8691, training loss: 0.3728148919980036\n",
      "Iteration: 8692, training loss: 0.37281265350980597\n",
      "Iteration: 8693, training loss: 0.3728104153576415\n",
      "Iteration: 8694, training loss: 0.37280817754141915\n",
      "Iteration: 8695, training loss: 0.37280594006104806\n",
      "Iteration: 8696, training loss: 0.37280370291643705\n",
      "Iteration: 8697, training loss: 0.37280146610749554\n",
      "Iteration: 8698, training loss: 0.3727992296341324\n",
      "Iteration: 8699, training loss: 0.3727969934962569\n",
      "Iteration: 8700, training loss: 0.3727947576937783\n",
      "Iteration: 8701, training loss: 0.37279252222660597\n",
      "Iteration: 8702, training loss: 0.37279028709464895\n",
      "Iteration: 8703, training loss: 0.3727880522978167\n",
      "Iteration: 8704, training loss: 0.37278581783601844\n",
      "Iteration: 8705, training loss: 0.37278358370916387\n",
      "Iteration: 8706, training loss: 0.3727813499171621\n",
      "Iteration: 8707, training loss: 0.3727791164599228\n",
      "Iteration: 8708, training loss: 0.37277688333735537\n",
      "Iteration: 8709, training loss: 0.3727746505493694\n",
      "Iteration: 8710, training loss: 0.37277241809587447\n",
      "Iteration: 8711, training loss: 0.3727701859767802\n",
      "Iteration: 8712, training loss: 0.37276795419199615\n",
      "Iteration: 8713, training loss: 0.37276572274143216\n",
      "Iteration: 8714, training loss: 0.37276349162499783\n",
      "Iteration: 8715, training loss: 0.372761260842603\n",
      "Iteration: 8716, training loss: 0.37275903039415736\n",
      "Iteration: 8717, training loss: 0.37275680027957087\n",
      "Iteration: 8718, training loss: 0.3727545704987534\n",
      "Iteration: 8719, training loss: 0.3727523410516146\n",
      "Iteration: 8720, training loss: 0.3727501119380647\n",
      "Iteration: 8721, training loss: 0.3727478831580136\n",
      "Iteration: 8722, training loss: 0.3727456547113712\n",
      "Iteration: 8723, training loss: 0.37274342659804766\n",
      "Iteration: 8724, training loss: 0.37274119881795303\n",
      "Iteration: 8725, training loss: 0.37273897137099743\n",
      "Iteration: 8726, training loss: 0.37273674425709097\n",
      "Iteration: 8727, training loss: 0.3727345174761439\n",
      "Iteration: 8728, training loss: 0.3727322910280664\n",
      "Iteration: 8729, training loss: 0.37273006491276883\n",
      "Iteration: 8730, training loss: 0.3727278391301614\n",
      "Iteration: 8731, training loss: 0.3727256136801544\n",
      "Iteration: 8732, training loss: 0.3727233885626583\n",
      "Iteration: 8733, training loss: 0.37272116377758335\n",
      "Iteration: 8734, training loss: 0.3727189393248403\n",
      "Iteration: 8735, training loss: 0.37271671520433935\n",
      "Iteration: 8736, training loss: 0.37271449141599106\n",
      "Iteration: 8737, training loss: 0.3727122679597061\n",
      "Iteration: 8738, training loss: 0.3727100448353948\n",
      "Iteration: 8739, training loss: 0.372707822042968\n",
      "Iteration: 8740, training loss: 0.37270559958233634\n",
      "Iteration: 8741, training loss: 0.3727033774534104\n",
      "Iteration: 8742, training loss: 0.372701155656101\n",
      "Iteration: 8743, training loss: 0.37269893419031885\n",
      "Iteration: 8744, training loss: 0.37269671305597485\n",
      "Iteration: 8745, training loss: 0.3726944922529796\n",
      "Iteration: 8746, training loss: 0.37269227178124426\n",
      "Iteration: 8747, training loss: 0.37269005164067953\n",
      "Iteration: 8748, training loss: 0.37268783183119647\n",
      "Iteration: 8749, training loss: 0.37268561235270586\n",
      "Iteration: 8750, training loss: 0.3726833932051189\n",
      "Iteration: 8751, training loss: 0.3726811743883467\n",
      "Iteration: 8752, training loss: 0.3726789559023001\n",
      "Iteration: 8753, training loss: 0.3726767377468903\n",
      "Iteration: 8754, training loss: 0.37267451992202855\n",
      "Iteration: 8755, training loss: 0.37267230242762595\n",
      "Iteration: 8756, training loss: 0.37267008526359374\n",
      "Iteration: 8757, training loss: 0.3726678684298432\n",
      "Iteration: 8758, training loss: 0.37266565192628553\n",
      "Iteration: 8759, training loss: 0.3726634357528322\n",
      "Iteration: 8760, training loss: 0.37266121990939444\n",
      "Iteration: 8761, training loss: 0.3726590043958838\n",
      "Iteration: 8762, training loss: 0.37265678921221146\n",
      "Iteration: 8763, training loss: 0.3726545743582892\n",
      "Iteration: 8764, training loss: 0.3726523598340283\n",
      "Iteration: 8765, training loss: 0.3726501456393404\n",
      "Iteration: 8766, training loss: 0.37264793177413696\n",
      "Iteration: 8767, training loss: 0.3726457182383298\n",
      "Iteration: 8768, training loss: 0.3726435050318304\n",
      "Iteration: 8769, training loss: 0.37264129215455044\n",
      "Iteration: 8770, training loss: 0.37263907960640164\n",
      "Iteration: 8771, training loss: 0.37263686738729584\n",
      "Iteration: 8772, training loss: 0.37263465549714475\n",
      "Iteration: 8773, training loss: 0.37263244393586026\n",
      "Iteration: 8774, training loss: 0.372630232703354\n",
      "Iteration: 8775, training loss: 0.37262802179953813\n",
      "Iteration: 8776, training loss: 0.3726258112243245\n",
      "Iteration: 8777, training loss: 0.3726236009776249\n",
      "Iteration: 8778, training loss: 0.3726213910593516\n",
      "Iteration: 8779, training loss: 0.37261918146941647\n",
      "Iteration: 8780, training loss: 0.37261697220773154\n",
      "Iteration: 8781, training loss: 0.37261476327420895\n",
      "Iteration: 8782, training loss: 0.37261255466876086\n",
      "Iteration: 8783, training loss: 0.3726103463912994\n",
      "Iteration: 8784, training loss: 0.3726081384417368\n",
      "Iteration: 8785, training loss: 0.37260593081998533\n",
      "Iteration: 8786, training loss: 0.37260372352595716\n",
      "Iteration: 8787, training loss: 0.37260151655956475\n",
      "Iteration: 8788, training loss: 0.37259930992072027\n",
      "Iteration: 8789, training loss: 0.3725971036093362\n",
      "Iteration: 8790, training loss: 0.3725948976253249\n",
      "Iteration: 8791, training loss: 0.37259269196859884\n",
      "Iteration: 8792, training loss: 0.3725904866390706\n",
      "Iteration: 8793, training loss: 0.3725882816366525\n",
      "Iteration: 8794, training loss: 0.3725860769612572\n",
      "Iteration: 8795, training loss: 0.3725838726127973\n",
      "Iteration: 8796, training loss: 0.37258166859118536\n",
      "Iteration: 8797, training loss: 0.372579464896334\n",
      "Iteration: 8798, training loss: 0.37257726152815607\n",
      "Iteration: 8799, training loss: 0.3725750584865642\n",
      "Iteration: 8800, training loss: 0.3725728557714711\n",
      "Iteration: 8801, training loss: 0.3725706533827896\n",
      "Iteration: 8802, training loss: 0.37256845132043254\n",
      "Iteration: 8803, training loss: 0.37256624958431267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8804, training loss: 0.372564048174343\n",
      "Iteration: 8805, training loss: 0.3725618470904366\n",
      "Iteration: 8806, training loss: 0.37255964633250616\n",
      "Iteration: 8807, training loss: 0.3725574459004648\n",
      "Iteration: 8808, training loss: 0.37255524579422555\n",
      "Iteration: 8809, training loss: 0.3725530460137015\n",
      "Iteration: 8810, training loss: 0.37255084655880566\n",
      "Iteration: 8811, training loss: 0.3725486474294512\n",
      "Iteration: 8812, training loss: 0.37254644862555136\n",
      "Iteration: 8813, training loss: 0.37254425014701936\n",
      "Iteration: 8814, training loss: 0.3725420519937683\n",
      "Iteration: 8815, training loss: 0.3725398541657115\n",
      "Iteration: 8816, training loss: 0.37253765666276234\n",
      "Iteration: 8817, training loss: 0.37253545948483413\n",
      "Iteration: 8818, training loss: 0.37253326263184017\n",
      "Iteration: 8819, training loss: 0.37253106610369396\n",
      "Iteration: 8820, training loss: 0.37252886990030887\n",
      "Iteration: 8821, training loss: 0.3725266740215985\n",
      "Iteration: 8822, training loss: 0.37252447846747616\n",
      "Iteration: 8823, training loss: 0.3725222832378556\n",
      "Iteration: 8824, training loss: 0.37252008833265027\n",
      "Iteration: 8825, training loss: 0.3725178937517738\n",
      "Iteration: 8826, training loss: 0.37251569949514\n",
      "Iteration: 8827, training loss: 0.3725135055626622\n",
      "Iteration: 8828, training loss: 0.3725113119542545\n",
      "Iteration: 8829, training loss: 0.3725091186698304\n",
      "Iteration: 8830, training loss: 0.37250692570930377\n",
      "Iteration: 8831, training loss: 0.3725047330725884\n",
      "Iteration: 8832, training loss: 0.37250254075959816\n",
      "Iteration: 8833, training loss: 0.37250034877024696\n",
      "Iteration: 8834, training loss: 0.37249815710444867\n",
      "Iteration: 8835, training loss: 0.37249596576211735\n",
      "Iteration: 8836, training loss: 0.37249377474316686\n",
      "Iteration: 8837, training loss: 0.37249158404751115\n",
      "Iteration: 8838, training loss: 0.3724893936750645\n",
      "Iteration: 8839, training loss: 0.37248720362574084\n",
      "Iteration: 8840, training loss: 0.3724850138994544\n",
      "Iteration: 8841, training loss: 0.37248282449611914\n",
      "Iteration: 8842, training loss: 0.37248063541564946\n",
      "Iteration: 8843, training loss: 0.37247844665795954\n",
      "Iteration: 8844, training loss: 0.3724762582229636\n",
      "Iteration: 8845, training loss: 0.3724740701105758\n",
      "Iteration: 8846, training loss: 0.3724718823207107\n",
      "Iteration: 8847, training loss: 0.3724696948532826\n",
      "Iteration: 8848, training loss: 0.37246750770820575\n",
      "Iteration: 8849, training loss: 0.37246532088539464\n",
      "Iteration: 8850, training loss: 0.3724631343847638\n",
      "Iteration: 8851, training loss: 0.3724609482062277\n",
      "Iteration: 8852, training loss: 0.37245876234970077\n",
      "Iteration: 8853, training loss: 0.37245657681509775\n",
      "Iteration: 8854, training loss: 0.37245439160233323\n",
      "Iteration: 8855, training loss: 0.37245220671132157\n",
      "Iteration: 8856, training loss: 0.37245002214197753\n",
      "Iteration: 8857, training loss: 0.372447837894216\n",
      "Iteration: 8858, training loss: 0.3724456539679515\n",
      "Iteration: 8859, training loss: 0.3724434703630989\n",
      "Iteration: 8860, training loss: 0.3724412870795729\n",
      "Iteration: 8861, training loss: 0.37243910411728853\n",
      "Iteration: 8862, training loss: 0.37243692147616037\n",
      "Iteration: 8863, training loss: 0.3724347391561035\n",
      "Iteration: 8864, training loss: 0.3724325571570327\n",
      "Iteration: 8865, training loss: 0.3724303754788631\n",
      "Iteration: 8866, training loss: 0.3724281941215096\n",
      "Iteration: 8867, training loss: 0.3724260130848873\n",
      "Iteration: 8868, training loss: 0.37242383236891125\n",
      "Iteration: 8869, training loss: 0.3724216519734963\n",
      "Iteration: 8870, training loss: 0.372419471898558\n",
      "Iteration: 8871, training loss: 0.37241729214401115\n",
      "Iteration: 8872, training loss: 0.3724151127097712\n",
      "Iteration: 8873, training loss: 0.3724129335957531\n",
      "Iteration: 8874, training loss: 0.3724107548018724\n",
      "Iteration: 8875, training loss: 0.37240857632804425\n",
      "Iteration: 8876, training loss: 0.3724063981741839\n",
      "Iteration: 8877, training loss: 0.3724042203402068\n",
      "Iteration: 8878, training loss: 0.37240204282602846\n",
      "Iteration: 8879, training loss: 0.37239986563156413\n",
      "Iteration: 8880, training loss: 0.37239768875672924\n",
      "Iteration: 8881, training loss: 0.37239551220143946\n",
      "Iteration: 8882, training loss: 0.3723933359656102\n",
      "Iteration: 8883, training loss: 0.372391160049157\n",
      "Iteration: 8884, training loss: 0.37238898445199553\n",
      "Iteration: 8885, training loss: 0.3723868091740414\n",
      "Iteration: 8886, training loss: 0.3723846342152101\n",
      "Iteration: 8887, training loss: 0.37238245957541755\n",
      "Iteration: 8888, training loss: 0.3723802852545794\n",
      "Iteration: 8889, training loss: 0.37237811125261133\n",
      "Iteration: 8890, training loss: 0.37237593756942927\n",
      "Iteration: 8891, training loss: 0.3723737642049489\n",
      "Iteration: 8892, training loss: 0.37237159115908613\n",
      "Iteration: 8893, training loss: 0.3723694184317569\n",
      "Iteration: 8894, training loss: 0.3723672460228771\n",
      "Iteration: 8895, training loss: 0.3723650739323625\n",
      "Iteration: 8896, training loss: 0.3723629021601293\n",
      "Iteration: 8897, training loss: 0.3723607307060935\n",
      "Iteration: 8898, training loss: 0.3723585595701711\n",
      "Iteration: 8899, training loss: 0.3723563887522782\n",
      "Iteration: 8900, training loss: 0.37235421825233095\n",
      "Iteration: 8901, training loss: 0.37235204807024536\n",
      "Iteration: 8902, training loss: 0.3723498782059378\n",
      "Iteration: 8903, training loss: 0.3723477086593244\n",
      "Iteration: 8904, training loss: 0.37234553943032145\n",
      "Iteration: 8905, training loss: 0.3723433705188451\n",
      "Iteration: 8906, training loss: 0.3723412019248118\n",
      "Iteration: 8907, training loss: 0.37233903364813775\n",
      "Iteration: 8908, training loss: 0.37233686568873964\n",
      "Iteration: 8909, training loss: 0.37233469804653346\n",
      "Iteration: 8910, training loss: 0.37233253072143596\n",
      "Iteration: 8911, training loss: 0.37233036371336337\n",
      "Iteration: 8912, training loss: 0.3723281970222325\n",
      "Iteration: 8913, training loss: 0.37232603064795966\n",
      "Iteration: 8914, training loss: 0.3723238645904615\n",
      "Iteration: 8915, training loss: 0.3723216988496547\n",
      "Iteration: 8916, training loss: 0.3723195334254557\n",
      "Iteration: 8917, training loss: 0.37231736831778134\n",
      "Iteration: 8918, training loss: 0.37231520352654823\n",
      "Iteration: 8919, training loss: 0.37231303905167323\n",
      "Iteration: 8920, training loss: 0.3723108748930729\n",
      "Iteration: 8921, training loss: 0.3723087110506643\n",
      "Iteration: 8922, training loss: 0.3723065475243641\n",
      "Iteration: 8923, training loss: 0.37230438431408913\n",
      "Iteration: 8924, training loss: 0.37230222141975633\n",
      "Iteration: 8925, training loss: 0.37230005884128275\n",
      "Iteration: 8926, training loss: 0.37229789657858525\n",
      "Iteration: 8927, training loss: 0.37229573463158067\n",
      "Iteration: 8928, training loss: 0.3722935730001864\n",
      "Iteration: 8929, training loss: 0.37229141168431906\n",
      "Iteration: 8930, training loss: 0.3722892506838962\n",
      "Iteration: 8931, training loss: 0.37228708999883453\n",
      "Iteration: 8932, training loss: 0.3722849296290515\n",
      "Iteration: 8933, training loss: 0.37228276957446416\n",
      "Iteration: 8934, training loss: 0.3722806098349897\n",
      "Iteration: 8935, training loss: 0.37227845041054547\n",
      "Iteration: 8936, training loss: 0.3722762913010487\n",
      "Iteration: 8937, training loss: 0.3722741325064167\n",
      "Iteration: 8938, training loss: 0.37227197402656687\n",
      "Iteration: 8939, training loss: 0.3722698158614166\n",
      "Iteration: 8940, training loss: 0.37226765801088313\n",
      "Iteration: 8941, training loss: 0.37226550047488416\n",
      "Iteration: 8942, training loss: 0.372263343253337\n",
      "Iteration: 8943, training loss: 0.3722611863461592\n",
      "Iteration: 8944, training loss: 0.37225902975326824\n",
      "Iteration: 8945, training loss: 0.37225687347458175\n",
      "Iteration: 8946, training loss: 0.37225471751001743\n",
      "Iteration: 8947, training loss: 0.3722525618594926\n",
      "Iteration: 8948, training loss: 0.3722504065229254\n",
      "Iteration: 8949, training loss: 0.37224825150023316\n",
      "Iteration: 8950, training loss: 0.37224609679133375\n",
      "Iteration: 8951, training loss: 0.3722439423961449\n",
      "Iteration: 8952, training loss: 0.3722417883145845\n",
      "Iteration: 8953, training loss: 0.3722396345465702\n",
      "Iteration: 8954, training loss: 0.37223748109202\n",
      "Iteration: 8955, training loss: 0.3722353279508517\n",
      "Iteration: 8956, training loss: 0.3722331751229833\n",
      "Iteration: 8957, training loss: 0.37223102260833274\n",
      "Iteration: 8958, training loss: 0.37222887040681796\n",
      "Iteration: 8959, training loss: 0.372226718518357\n",
      "Iteration: 8960, training loss: 0.37222456694286793\n",
      "Iteration: 8961, training loss: 0.3722224156802688\n",
      "Iteration: 8962, training loss: 0.37222026473047787\n",
      "Iteration: 8963, training loss: 0.37221811409341304\n",
      "Iteration: 8964, training loss: 0.37221596376899263\n",
      "Iteration: 8965, training loss: 0.3722138137571348\n",
      "Iteration: 8966, training loss: 0.3722116640577579\n",
      "Iteration: 8967, training loss: 0.37220951467078006\n",
      "Iteration: 8968, training loss: 0.3722073655961196\n",
      "Iteration: 8969, training loss: 0.372205216833695\n",
      "Iteration: 8970, training loss: 0.37220306838342443\n",
      "Iteration: 8971, training loss: 0.37220092024522644\n",
      "Iteration: 8972, training loss: 0.3721987724190194\n",
      "Iteration: 8973, training loss: 0.37219662490472166\n",
      "Iteration: 8974, training loss: 0.37219447770225195\n",
      "Iteration: 8975, training loss: 0.3721923308115287\n",
      "Iteration: 8976, training loss: 0.3721901842324703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8977, training loss: 0.3721880379649955\n",
      "Iteration: 8978, training loss: 0.37218589200902297\n",
      "Iteration: 8979, training loss: 0.37218374636447116\n",
      "Iteration: 8980, training loss: 0.37218160103125897\n",
      "Iteration: 8981, training loss: 0.37217945600930497\n",
      "Iteration: 8982, training loss: 0.37217731129852794\n",
      "Iteration: 8983, training loss: 0.37217516689884667\n",
      "Iteration: 8984, training loss: 0.37217302281017994\n",
      "Iteration: 8985, training loss: 0.37217087903244667\n",
      "Iteration: 8986, training loss: 0.3721687355655656\n",
      "Iteration: 8987, training loss: 0.37216659240945577\n",
      "Iteration: 8988, training loss: 0.372164449564036\n",
      "Iteration: 8989, training loss: 0.3721623070292253\n",
      "Iteration: 8990, training loss: 0.37216016480494263\n",
      "Iteration: 8991, training loss: 0.372158022891107\n",
      "Iteration: 8992, training loss: 0.37215588128763766\n",
      "Iteration: 8993, training loss: 0.37215373999445345\n",
      "Iteration: 8994, training loss: 0.37215159901147354\n",
      "Iteration: 8995, training loss: 0.3721494583386171\n",
      "Iteration: 8996, training loss: 0.37214731797580347\n",
      "Iteration: 8997, training loss: 0.3721451779229517\n",
      "Iteration: 8998, training loss: 0.372143038179981\n",
      "Iteration: 8999, training loss: 0.3721408987468108\n",
      "Iteration: 9000, training loss: 0.3721387596233603\n",
      "Iteration: 9001, training loss: 0.3721366208095488\n",
      "Iteration: 9002, training loss: 0.3721344823052958\n",
      "Iteration: 9003, training loss: 0.3721323441105205\n",
      "Iteration: 9004, training loss: 0.3721302062251426\n",
      "Iteration: 9005, training loss: 0.3721280686490813\n",
      "Iteration: 9006, training loss: 0.37212593138225636\n",
      "Iteration: 9007, training loss: 0.372123794424587\n",
      "Iteration: 9008, training loss: 0.372121657775993\n",
      "Iteration: 9009, training loss: 0.3721195214363937\n",
      "Iteration: 9010, training loss: 0.3721173854057091\n",
      "Iteration: 9011, training loss: 0.3721152496838585\n",
      "Iteration: 9012, training loss: 0.37211311427076177\n",
      "Iteration: 9013, training loss: 0.3721109791663386\n",
      "Iteration: 9014, training loss: 0.37210884437050856\n",
      "Iteration: 9015, training loss: 0.3721067098831916\n",
      "Iteration: 9016, training loss: 0.3721045757043075\n",
      "Iteration: 9017, training loss: 0.37210244183377605\n",
      "Iteration: 9018, training loss: 0.37210030827151724\n",
      "Iteration: 9019, training loss: 0.3720981750174507\n",
      "Iteration: 9020, training loss: 0.3720960420714965\n",
      "Iteration: 9021, training loss: 0.3720939094335747\n",
      "Iteration: 9022, training loss: 0.37209177710360514\n",
      "Iteration: 9023, training loss: 0.3720896450815079\n",
      "Iteration: 9024, training loss: 0.37208751336720297\n",
      "Iteration: 9025, training loss: 0.3720853819606105\n",
      "Iteration: 9026, training loss: 0.37208325086165056\n",
      "Iteration: 9027, training loss: 0.3720811200702432\n",
      "Iteration: 9028, training loss: 0.3720789895863088\n",
      "Iteration: 9029, training loss: 0.3720768594097674\n",
      "Iteration: 9030, training loss: 0.37207472954053933\n",
      "Iteration: 9031, training loss: 0.37207259997854475\n",
      "Iteration: 9032, training loss: 0.3720704707237041\n",
      "Iteration: 9033, training loss: 0.37206834177593745\n",
      "Iteration: 9034, training loss: 0.37206621313516536\n",
      "Iteration: 9035, training loss: 0.3720640848013082\n",
      "Iteration: 9036, training loss: 0.37206195677428633\n",
      "Iteration: 9037, training loss: 0.3720598290540203\n",
      "Iteration: 9038, training loss: 0.37205770164043034\n",
      "Iteration: 9039, training loss: 0.37205557453343724\n",
      "Iteration: 9040, training loss: 0.37205344773296134\n",
      "Iteration: 9041, training loss: 0.3720513212389233\n",
      "Iteration: 9042, training loss: 0.37204919505124373\n",
      "Iteration: 9043, training loss: 0.37204706916984315\n",
      "Iteration: 9044, training loss: 0.37204494359464235\n",
      "Iteration: 9045, training loss: 0.37204281832556196\n",
      "Iteration: 9046, training loss: 0.3720406933625227\n",
      "Iteration: 9047, training loss: 0.3720385687054453\n",
      "Iteration: 9048, training loss: 0.3720364443542506\n",
      "Iteration: 9049, training loss: 0.37203432030885936\n",
      "Iteration: 9050, training loss: 0.3720321965691924\n",
      "Iteration: 9051, training loss: 0.37203007313517056\n",
      "Iteration: 9052, training loss: 0.37202795000671496\n",
      "Iteration: 9053, training loss: 0.37202582718374627\n",
      "Iteration: 9054, training loss: 0.37202370466618556\n",
      "Iteration: 9055, training loss: 0.3720215824539539\n",
      "Iteration: 9056, training loss: 0.3720194605469721\n",
      "Iteration: 9057, training loss: 0.37201733894516137\n",
      "Iteration: 9058, training loss: 0.3720152176484428\n",
      "Iteration: 9059, training loss: 0.37201309665673754\n",
      "Iteration: 9060, training loss: 0.3720109759699665\n",
      "Iteration: 9061, training loss: 0.3720088555880511\n",
      "Iteration: 9062, training loss: 0.37200673551091246\n",
      "Iteration: 9063, training loss: 0.3720046157384717\n",
      "Iteration: 9064, training loss: 0.3720024962706503\n",
      "Iteration: 9065, training loss: 0.37200037710736944\n",
      "Iteration: 9066, training loss: 0.3719982582485504\n",
      "Iteration: 9067, training loss: 0.3719961396941146\n",
      "Iteration: 9068, training loss: 0.37199402144398336\n",
      "Iteration: 9069, training loss: 0.3719919034980782\n",
      "Iteration: 9070, training loss: 0.3719897858563204\n",
      "Iteration: 9071, training loss: 0.3719876685186316\n",
      "Iteration: 9072, training loss: 0.37198555148493323\n",
      "Iteration: 9073, training loss: 0.3719834347551469\n",
      "Iteration: 9074, training loss: 0.37198131832919396\n",
      "Iteration: 9075, training loss: 0.3719792022069962\n",
      "Iteration: 9076, training loss: 0.3719770863884752\n",
      "Iteration: 9077, training loss: 0.3719749708735526\n",
      "Iteration: 9078, training loss: 0.37197285566215005\n",
      "Iteration: 9079, training loss: 0.37197074075418923\n",
      "Iteration: 9080, training loss: 0.37196862614959214\n",
      "Iteration: 9081, training loss: 0.3719665118482801\n",
      "Iteration: 9082, training loss: 0.3719643978501754\n",
      "Iteration: 9083, training loss: 0.3719622841551995\n",
      "Iteration: 9084, training loss: 0.3719601707632745\n",
      "Iteration: 9085, training loss: 0.3719580576743221\n",
      "Iteration: 9086, training loss: 0.3719559448882643\n",
      "Iteration: 9087, training loss: 0.37195383240502317\n",
      "Iteration: 9088, training loss: 0.3719517202245205\n",
      "Iteration: 9089, training loss: 0.37194960834667834\n",
      "Iteration: 9090, training loss: 0.37194749677141886\n",
      "Iteration: 9091, training loss: 0.371945385498664\n",
      "Iteration: 9092, training loss: 0.371943274528336\n",
      "Iteration: 9093, training loss: 0.37194116386035686\n",
      "Iteration: 9094, training loss: 0.3719390534946487\n",
      "Iteration: 9095, training loss: 0.3719369434311338\n",
      "Iteration: 9096, training loss: 0.37193483366973434\n",
      "Iteration: 9097, training loss: 0.37193272421037266\n",
      "Iteration: 9098, training loss: 0.37193061505297087\n",
      "Iteration: 9099, training loss: 0.37192850619745144\n",
      "Iteration: 9100, training loss: 0.37192639764373653\n",
      "Iteration: 9101, training loss: 0.3719242893917487\n",
      "Iteration: 9102, training loss: 0.37192218144141026\n",
      "Iteration: 9103, training loss: 0.3719200737926436\n",
      "Iteration: 9104, training loss: 0.3719179664453711\n",
      "Iteration: 9105, training loss: 0.3719158593995155\n",
      "Iteration: 9106, training loss: 0.3719137526549991\n",
      "Iteration: 9107, training loss: 0.37191164621174455\n",
      "Iteration: 9108, training loss: 0.3719095400696742\n",
      "Iteration: 9109, training loss: 0.37190743422871086\n",
      "Iteration: 9110, training loss: 0.3719053286887773\n",
      "Iteration: 9111, training loss: 0.3719032234497959\n",
      "Iteration: 9112, training loss: 0.37190111851168944\n",
      "Iteration: 9113, training loss: 0.3718990138743807\n",
      "Iteration: 9114, training loss: 0.37189690953779225\n",
      "Iteration: 9115, training loss: 0.3718948055018471\n",
      "Iteration: 9116, training loss: 0.37189270176646794\n",
      "Iteration: 9117, training loss: 0.37189059833157756\n",
      "Iteration: 9118, training loss: 0.37188849519709893\n",
      "Iteration: 9119, training loss: 0.3718863923629549\n",
      "Iteration: 9120, training loss: 0.37188428982906835\n",
      "Iteration: 9121, training loss: 0.3718821875953623\n",
      "Iteration: 9122, training loss: 0.3718800856617597\n",
      "Iteration: 9123, training loss: 0.37187798402818356\n",
      "Iteration: 9124, training loss: 0.3718758826945569\n",
      "Iteration: 9125, training loss: 0.37187378166080287\n",
      "Iteration: 9126, training loss: 0.3718716809268445\n",
      "Iteration: 9127, training loss: 0.37186958049260493\n",
      "Iteration: 9128, training loss: 0.37186748035800726\n",
      "Iteration: 9129, training loss: 0.3718653805229748\n",
      "Iteration: 9130, training loss: 0.37186328098743066\n",
      "Iteration: 9131, training loss: 0.3718611817512982\n",
      "Iteration: 9132, training loss: 0.3718590828145004\n",
      "Iteration: 9133, training loss: 0.37185698417696095\n",
      "Iteration: 9134, training loss: 0.371854885838603\n",
      "Iteration: 9135, training loss: 0.37185278779935\n",
      "Iteration: 9136, training loss: 0.3718506900591251\n",
      "Iteration: 9137, training loss: 0.371848592617852\n",
      "Iteration: 9138, training loss: 0.37184649547545395\n",
      "Iteration: 9139, training loss: 0.37184439863185453\n",
      "Iteration: 9140, training loss: 0.3718423020869771\n",
      "Iteration: 9141, training loss: 0.3718402058407455\n",
      "Iteration: 9142, training loss: 0.3718381098930829\n",
      "Iteration: 9143, training loss: 0.37183601424391327\n",
      "Iteration: 9144, training loss: 0.3718339188931601\n",
      "Iteration: 9145, training loss: 0.3718318238407468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9146, training loss: 0.37182972908659734\n",
      "Iteration: 9147, training loss: 0.3718276346306354\n",
      "Iteration: 9148, training loss: 0.37182554047278454\n",
      "Iteration: 9149, training loss: 0.3718234466129686\n",
      "Iteration: 9150, training loss: 0.37182135305111147\n",
      "Iteration: 9151, training loss: 0.3718192597871369\n",
      "Iteration: 9152, training loss: 0.3718171668209688\n",
      "Iteration: 9153, training loss: 0.371815074152531\n",
      "Iteration: 9154, training loss: 0.3718129817817474\n",
      "Iteration: 9155, training loss: 0.3718108897085418\n",
      "Iteration: 9156, training loss: 0.37180879793283844\n",
      "Iteration: 9157, training loss: 0.37180670645456126\n",
      "Iteration: 9158, training loss: 0.37180461527363406\n",
      "Iteration: 9159, training loss: 0.3718025243899811\n",
      "Iteration: 9160, training loss: 0.3718004338035265\n",
      "Iteration: 9161, training loss: 0.37179834351419416\n",
      "Iteration: 9162, training loss: 0.37179625352190837\n",
      "Iteration: 9163, training loss: 0.3717941638265933\n",
      "Iteration: 9164, training loss: 0.371792074428173\n",
      "Iteration: 9165, training loss: 0.3717899853265719\n",
      "Iteration: 9166, training loss: 0.3717878965217141\n",
      "Iteration: 9167, training loss: 0.371785808013524\n",
      "Iteration: 9168, training loss: 0.37178371980192576\n",
      "Iteration: 9169, training loss: 0.3717816318868438\n",
      "Iteration: 9170, training loss: 0.3717795442682026\n",
      "Iteration: 9171, training loss: 0.3717774569459263\n",
      "Iteration: 9172, training loss: 0.3717753699199396\n",
      "Iteration: 9173, training loss: 0.3717732831901668\n",
      "Iteration: 9174, training loss: 0.37177119675653225\n",
      "Iteration: 9175, training loss: 0.3717691106189607\n",
      "Iteration: 9176, training loss: 0.3717670247773766\n",
      "Iteration: 9177, training loss: 0.37176493923170456\n",
      "Iteration: 9178, training loss: 0.3717628539818691\n",
      "Iteration: 9179, training loss: 0.37176076902779476\n",
      "Iteration: 9180, training loss: 0.37175868436940646\n",
      "Iteration: 9181, training loss: 0.3717566000066287\n",
      "Iteration: 9182, training loss: 0.37175451593938613\n",
      "Iteration: 9183, training loss: 0.3717524321676036\n",
      "Iteration: 9184, training loss: 0.37175034869120577\n",
      "Iteration: 9185, training loss: 0.37174826551011764\n",
      "Iteration: 9186, training loss: 0.3717461826242638\n",
      "Iteration: 9187, training loss: 0.3717441000335692\n",
      "Iteration: 9188, training loss: 0.37174201773795873\n",
      "Iteration: 9189, training loss: 0.3717399357373572\n",
      "Iteration: 9190, training loss: 0.37173785403168974\n",
      "Iteration: 9191, training loss: 0.37173577262088114\n",
      "Iteration: 9192, training loss: 0.3717336915048564\n",
      "Iteration: 9193, training loss: 0.3717316106835406\n",
      "Iteration: 9194, training loss: 0.3717295301568589\n",
      "Iteration: 9195, training loss: 0.37172744992473605\n",
      "Iteration: 9196, training loss: 0.37172536998709743\n",
      "Iteration: 9197, training loss: 0.3717232903438681\n",
      "Iteration: 9198, training loss: 0.37172121099497313\n",
      "Iteration: 9199, training loss: 0.37171913194033784\n",
      "Iteration: 9200, training loss: 0.37171705317988735\n",
      "Iteration: 9201, training loss: 0.37171497471354703\n",
      "Iteration: 9202, training loss: 0.37171289654124196\n",
      "Iteration: 9203, training loss: 0.37171081866289757\n",
      "Iteration: 9204, training loss: 0.37170874107843915\n",
      "Iteration: 9205, training loss: 0.37170666378779216\n",
      "Iteration: 9206, training loss: 0.3717045867908818\n",
      "Iteration: 9207, training loss: 0.37170251008763355\n",
      "Iteration: 9208, training loss: 0.37170043367797295\n",
      "Iteration: 9209, training loss: 0.37169835756182523\n",
      "Iteration: 9210, training loss: 0.37169628173911623\n",
      "Iteration: 9211, training loss: 0.3716942062097712\n",
      "Iteration: 9212, training loss: 0.37169213097371584\n",
      "Iteration: 9213, training loss: 0.3716900560308755\n",
      "Iteration: 9214, training loss: 0.37168798138117615\n",
      "Iteration: 9215, training loss: 0.37168590702454307\n",
      "Iteration: 9216, training loss: 0.3716838329609021\n",
      "Iteration: 9217, training loss: 0.37168175919017904\n",
      "Iteration: 9218, training loss: 0.3716796857122994\n",
      "Iteration: 9219, training loss: 0.37167761252718906\n",
      "Iteration: 9220, training loss: 0.37167553963477373\n",
      "Iteration: 9221, training loss: 0.37167346703497917\n",
      "Iteration: 9222, training loss: 0.37167139472773125\n",
      "Iteration: 9223, training loss: 0.3716693227129559\n",
      "Iteration: 9224, training loss: 0.3716672509905789\n",
      "Iteration: 9225, training loss: 0.37166517956052625\n",
      "Iteration: 9226, training loss: 0.3716631084227238\n",
      "Iteration: 9227, training loss: 0.3716610375770976\n",
      "Iteration: 9228, training loss: 0.3716589670235736\n",
      "Iteration: 9229, training loss: 0.3716568967620779\n",
      "Iteration: 9230, training loss: 0.3716548267925365\n",
      "Iteration: 9231, training loss: 0.3716527571148753\n",
      "Iteration: 9232, training loss: 0.37165068772902066\n",
      "Iteration: 9233, training loss: 0.3716486186348986\n",
      "Iteration: 9234, training loss: 0.37164654983243545\n",
      "Iteration: 9235, training loss: 0.3716444813215572\n",
      "Iteration: 9236, training loss: 0.37164241310219\n",
      "Iteration: 9237, training loss: 0.3716403451742603\n",
      "Iteration: 9238, training loss: 0.37163827753769435\n",
      "Iteration: 9239, training loss: 0.3716362101924184\n",
      "Iteration: 9240, training loss: 0.3716341431383587\n",
      "Iteration: 9241, training loss: 0.37163207637544166\n",
      "Iteration: 9242, training loss: 0.3716300099035937\n",
      "Iteration: 9243, training loss: 0.3716279437227413\n",
      "Iteration: 9244, training loss: 0.3716258778328107\n",
      "Iteration: 9245, training loss: 0.3716238122337285\n",
      "Iteration: 9246, training loss: 0.3716217469254211\n",
      "Iteration: 9247, training loss: 0.3716196819078152\n",
      "Iteration: 9248, training loss: 0.3716176171808372\n",
      "Iteration: 9249, training loss: 0.37161555274441366\n",
      "Iteration: 9250, training loss: 0.3716134885984712\n",
      "Iteration: 9251, training loss: 0.3716114247429365\n",
      "Iteration: 9252, training loss: 0.37160936117773624\n",
      "Iteration: 9253, training loss: 0.37160729790279695\n",
      "Iteration: 9254, training loss: 0.3716052349180456\n",
      "Iteration: 9255, training loss: 0.37160317222340866\n",
      "Iteration: 9256, training loss: 0.371601109818813\n",
      "Iteration: 9257, training loss: 0.37159904770418545\n",
      "Iteration: 9258, training loss: 0.37159698587945283\n",
      "Iteration: 9259, training loss: 0.3715949243445419\n",
      "Iteration: 9260, training loss: 0.37159286309937967\n",
      "Iteration: 9261, training loss: 0.37159080214389295\n",
      "Iteration: 9262, training loss: 0.37158874147800863\n",
      "Iteration: 9263, training loss: 0.37158668110165377\n",
      "Iteration: 9264, training loss: 0.3715846210147553\n",
      "Iteration: 9265, training loss: 0.37158256121724015\n",
      "Iteration: 9266, training loss: 0.37158050170903556\n",
      "Iteration: 9267, training loss: 0.37157844249006844\n",
      "Iteration: 9268, training loss: 0.37157638356026595\n",
      "Iteration: 9269, training loss: 0.37157432491955517\n",
      "Iteration: 9270, training loss: 0.3715722665678632\n",
      "Iteration: 9271, training loss: 0.3715702085051174\n",
      "Iteration: 9272, training loss: 0.3715681507312448\n",
      "Iteration: 9273, training loss: 0.37156609324617257\n",
      "Iteration: 9274, training loss: 0.3715640360498282\n",
      "Iteration: 9275, training loss: 0.37156197914213873\n",
      "Iteration: 9276, training loss: 0.37155992252303166\n",
      "Iteration: 9277, training loss: 0.37155786619243425\n",
      "Iteration: 9278, training loss: 0.3715558101502738\n",
      "Iteration: 9279, training loss: 0.37155375439647775\n",
      "Iteration: 9280, training loss: 0.37155169893097356\n",
      "Iteration: 9281, training loss: 0.37154964375368865\n",
      "Iteration: 9282, training loss: 0.37154758886455047\n",
      "Iteration: 9283, training loss: 0.37154553426348647\n",
      "Iteration: 9284, training loss: 0.37154347995042436\n",
      "Iteration: 9285, training loss: 0.37154142592529144\n",
      "Iteration: 9286, training loss: 0.3715393721880154\n",
      "Iteration: 9287, training loss: 0.371537318738524\n",
      "Iteration: 9288, training loss: 0.37153526557674466\n",
      "Iteration: 9289, training loss: 0.37153321270260514\n",
      "Iteration: 9290, training loss: 0.37153116011603304\n",
      "Iteration: 9291, training loss: 0.37152910781695614\n",
      "Iteration: 9292, training loss: 0.37152705580530215\n",
      "Iteration: 9293, training loss: 0.37152500408099887\n",
      "Iteration: 9294, training loss: 0.37152295264397406\n",
      "Iteration: 9295, training loss: 0.3715209014941556\n",
      "Iteration: 9296, training loss: 0.3715188506314713\n",
      "Iteration: 9297, training loss: 0.37151680005584903\n",
      "Iteration: 9298, training loss: 0.37151474976721666\n",
      "Iteration: 9299, training loss: 0.3715126997655022\n",
      "Iteration: 9300, training loss: 0.3715106500506335\n",
      "Iteration: 9301, training loss: 0.37150860062253865\n",
      "Iteration: 9302, training loss: 0.37150655148114553\n",
      "Iteration: 9303, training loss: 0.3715045026263823\n",
      "Iteration: 9304, training loss: 0.3715024540581769\n",
      "Iteration: 9305, training loss: 0.3715004057764575\n",
      "Iteration: 9306, training loss: 0.37149835778115226\n",
      "Iteration: 9307, training loss: 0.3714963100721892\n",
      "Iteration: 9308, training loss: 0.3714942626494965\n",
      "Iteration: 9309, training loss: 0.37149221551300243\n",
      "Iteration: 9310, training loss: 0.37149016866263523\n",
      "Iteration: 9311, training loss: 0.37148812209832305\n",
      "Iteration: 9312, training loss: 0.3714860758199942\n",
      "Iteration: 9313, training loss: 0.37148402982757706\n",
      "Iteration: 9314, training loss: 0.37148198412099975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9315, training loss: 0.3714799387001909\n",
      "Iteration: 9316, training loss: 0.3714778935650786\n",
      "Iteration: 9317, training loss: 0.3714758487155915\n",
      "Iteration: 9318, training loss: 0.3714738041516579\n",
      "Iteration: 9319, training loss: 0.37147175987320635\n",
      "Iteration: 9320, training loss: 0.37146971588016525\n",
      "Iteration: 9321, training loss: 0.371467672172463\n",
      "Iteration: 9322, training loss: 0.3714656287500285\n",
      "Iteration: 9323, training loss: 0.37146358561278997\n",
      "Iteration: 9324, training loss: 0.3714615427606761\n",
      "Iteration: 9325, training loss: 0.3714595001936157\n",
      "Iteration: 9326, training loss: 0.37145745791153706\n",
      "Iteration: 9327, training loss: 0.37145541591436915\n",
      "Iteration: 9328, training loss: 0.3714533742020405\n",
      "Iteration: 9329, training loss: 0.37145133277447995\n",
      "Iteration: 9330, training loss: 0.37144929163161605\n",
      "Iteration: 9331, training loss: 0.37144725077337776\n",
      "Iteration: 9332, training loss: 0.37144521019969384\n",
      "Iteration: 9333, training loss: 0.37144316991049314\n",
      "Iteration: 9334, training loss: 0.3714411299057045\n",
      "Iteration: 9335, training loss: 0.3714390901852568\n",
      "Iteration: 9336, training loss: 0.3714370507490789\n",
      "Iteration: 9337, training loss: 0.37143501159709974\n",
      "Iteration: 9338, training loss: 0.3714329727292483\n",
      "Iteration: 9339, training loss: 0.37143093414545364\n",
      "Iteration: 9340, training loss: 0.37142889584564465\n",
      "Iteration: 9341, training loss: 0.37142685782975043\n",
      "Iteration: 9342, training loss: 0.37142482009769995\n",
      "Iteration: 9343, training loss: 0.3714227826494225\n",
      "Iteration: 9344, training loss: 0.37142074548484705\n",
      "Iteration: 9345, training loss: 0.3714187086039028\n",
      "Iteration: 9346, training loss: 0.37141667200651884\n",
      "Iteration: 9347, training loss: 0.3714146356926244\n",
      "Iteration: 9348, training loss: 0.37141259966214873\n",
      "Iteration: 9349, training loss: 0.37141056391502114\n",
      "Iteration: 9350, training loss: 0.3714085284511707\n",
      "Iteration: 9351, training loss: 0.3714064932705269\n",
      "Iteration: 9352, training loss: 0.37140445837301905\n",
      "Iteration: 9353, training loss: 0.37140242375857635\n",
      "Iteration: 9354, training loss: 0.37140038942712833\n",
      "Iteration: 9355, training loss: 0.37139835537860433\n",
      "Iteration: 9356, training loss: 0.37139632161293373\n",
      "Iteration: 9357, training loss: 0.37139428813004616\n",
      "Iteration: 9358, training loss: 0.371392254929871\n",
      "Iteration: 9359, training loss: 0.37139022201233757\n",
      "Iteration: 9360, training loss: 0.3713881893773758\n",
      "Iteration: 9361, training loss: 0.3713861570249149\n",
      "Iteration: 9362, training loss: 0.37138412495488454\n",
      "Iteration: 9363, training loss: 0.3713820931672144\n",
      "Iteration: 9364, training loss: 0.37138006166183407\n",
      "Iteration: 9365, training loss: 0.3713780304386733\n",
      "Iteration: 9366, training loss: 0.3713759994976616\n",
      "Iteration: 9367, training loss: 0.37137396883872886\n",
      "Iteration: 9368, training loss: 0.3713719384618047\n",
      "Iteration: 9369, training loss: 0.37136990836681905\n",
      "Iteration: 9370, training loss: 0.3713678785537014\n",
      "Iteration: 9371, training loss: 0.3713658490223819\n",
      "Iteration: 9372, training loss: 0.37136381977279026\n",
      "Iteration: 9373, training loss: 0.37136179080485626\n",
      "Iteration: 9374, training loss: 0.37135976211850985\n",
      "Iteration: 9375, training loss: 0.371357733713681\n",
      "Iteration: 9376, training loss: 0.3713557055902997\n",
      "Iteration: 9377, training loss: 0.3713536777482958\n",
      "Iteration: 9378, training loss: 0.3713516501875993\n",
      "Iteration: 9379, training loss: 0.3713496229081403\n",
      "Iteration: 9380, training loss: 0.3713475959098488\n",
      "Iteration: 9381, training loss: 0.3713455691926549\n",
      "Iteration: 9382, training loss: 0.3713435427564887\n",
      "Iteration: 9383, training loss: 0.3713415166012803\n",
      "Iteration: 9384, training loss: 0.3713394907269599\n",
      "Iteration: 9385, training loss: 0.3713374651334576\n",
      "Iteration: 9386, training loss: 0.3713354398207036\n",
      "Iteration: 9387, training loss: 0.3713334147886283\n",
      "Iteration: 9388, training loss: 0.3713313900371618\n",
      "Iteration: 9389, training loss: 0.37132936556623436\n",
      "Iteration: 9390, training loss: 0.37132734137577633\n",
      "Iteration: 9391, training loss: 0.3713253174657181\n",
      "Iteration: 9392, training loss: 0.37132329383598994\n",
      "Iteration: 9393, training loss: 0.3713212704865223\n",
      "Iteration: 9394, training loss: 0.3713192474172455\n",
      "Iteration: 9395, training loss: 0.37131722462809\n",
      "Iteration: 9396, training loss: 0.3713152021189862\n",
      "Iteration: 9397, training loss: 0.3713131798898649\n",
      "Iteration: 9398, training loss: 0.3713111579406562\n",
      "Iteration: 9399, training loss: 0.37130913627129086\n",
      "Iteration: 9400, training loss: 0.3713071148816993\n",
      "Iteration: 9401, training loss: 0.3713050937718123\n",
      "Iteration: 9402, training loss: 0.37130307294156034\n",
      "Iteration: 9403, training loss: 0.371301052390874\n",
      "Iteration: 9404, training loss: 0.37129903211968407\n",
      "Iteration: 9405, training loss: 0.3712970121279212\n",
      "Iteration: 9406, training loss: 0.37129499241551606\n",
      "Iteration: 9407, training loss: 0.37129297298239944\n",
      "Iteration: 9408, training loss: 0.371290953828502\n",
      "Iteration: 9409, training loss: 0.3712889349537547\n",
      "Iteration: 9410, training loss: 0.3712869163580882\n",
      "Iteration: 9411, training loss: 0.3712848980414334\n",
      "Iteration: 9412, training loss: 0.37128288000372117\n",
      "Iteration: 9413, training loss: 0.3712808622448824\n",
      "Iteration: 9414, training loss: 0.371278844764848\n",
      "Iteration: 9415, training loss: 0.3712768275635488\n",
      "Iteration: 9416, training loss: 0.37127481064091594\n",
      "Iteration: 9417, training loss: 0.3712727939968804\n",
      "Iteration: 9418, training loss: 0.37127077763137306\n",
      "Iteration: 9419, training loss: 0.3712687615443251\n",
      "Iteration: 9420, training loss: 0.3712667457356673\n",
      "Iteration: 9421, training loss: 0.3712647302053313\n",
      "Iteration: 9422, training loss: 0.3712627149532477\n",
      "Iteration: 9423, training loss: 0.37126069997934785\n",
      "Iteration: 9424, training loss: 0.3712586852835629\n",
      "Iteration: 9425, training loss: 0.371256670865824\n",
      "Iteration: 9426, training loss: 0.37125465672606245\n",
      "Iteration: 9427, training loss: 0.3712526428642094\n",
      "Iteration: 9428, training loss: 0.37125062928019614\n",
      "Iteration: 9429, training loss: 0.371248615973954\n",
      "Iteration: 9430, training loss: 0.37124660294541434\n",
      "Iteration: 9431, training loss: 0.3712445901945084\n",
      "Iteration: 9432, training loss: 0.3712425777211676\n",
      "Iteration: 9433, training loss: 0.37124056552532314\n",
      "Iteration: 9434, training loss: 0.3712385536069069\n",
      "Iteration: 9435, training loss: 0.3712365419658499\n",
      "Iteration: 9436, training loss: 0.37123453060208367\n",
      "Iteration: 9437, training loss: 0.37123251951553987\n",
      "Iteration: 9438, training loss: 0.3712305087061498\n",
      "Iteration: 9439, training loss: 0.37122849817384523\n",
      "Iteration: 9440, training loss: 0.37122648791855767\n",
      "Iteration: 9441, training loss: 0.3712244779402186\n",
      "Iteration: 9442, training loss: 0.37122246823875965\n",
      "Iteration: 9443, training loss: 0.37122045881411253\n",
      "Iteration: 9444, training loss: 0.3712184496662089\n",
      "Iteration: 9445, training loss: 0.37121644079498045\n",
      "Iteration: 9446, training loss: 0.3712144322003589\n",
      "Iteration: 9447, training loss: 0.371212423882276\n",
      "Iteration: 9448, training loss: 0.3712104158406634\n",
      "Iteration: 9449, training loss: 0.37120840807545297\n",
      "Iteration: 9450, training loss: 0.37120640058657656\n",
      "Iteration: 9451, training loss: 0.37120439337396605\n",
      "Iteration: 9452, training loss: 0.37120238643755316\n",
      "Iteration: 9453, training loss: 0.3712003797772699\n",
      "Iteration: 9454, training loss: 0.3711983733930481\n",
      "Iteration: 9455, training loss: 0.37119636728481986\n",
      "Iteration: 9456, training loss: 0.37119436145251683\n",
      "Iteration: 9457, training loss: 0.3711923558960713\n",
      "Iteration: 9458, training loss: 0.37119035061541517\n",
      "Iteration: 9459, training loss: 0.37118834561048053\n",
      "Iteration: 9460, training loss: 0.37118634088119934\n",
      "Iteration: 9461, training loss: 0.37118433642750376\n",
      "Iteration: 9462, training loss: 0.3711823322493259\n",
      "Iteration: 9463, training loss: 0.37118032834659787\n",
      "Iteration: 9464, training loss: 0.3711783247192519\n",
      "Iteration: 9465, training loss: 0.37117632136722\n",
      "Iteration: 9466, training loss: 0.3711743182904346\n",
      "Iteration: 9467, training loss: 0.37117231548882784\n",
      "Iteration: 9468, training loss: 0.37117031296233194\n",
      "Iteration: 9469, training loss: 0.37116831071087925\n",
      "Iteration: 9470, training loss: 0.37116630873440204\n",
      "Iteration: 9471, training loss: 0.37116430703283265\n",
      "Iteration: 9472, training loss: 0.37116230560610336\n",
      "Iteration: 9473, training loss: 0.3711603044541468\n",
      "Iteration: 9474, training loss: 0.3711583035768951\n",
      "Iteration: 9475, training loss: 0.3711563029742808\n",
      "Iteration: 9476, training loss: 0.3711543026462364\n",
      "Iteration: 9477, training loss: 0.3711523025926944\n",
      "Iteration: 9478, training loss: 0.3711503028135872\n",
      "Iteration: 9479, training loss: 0.3711483033088473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9480, training loss: 0.3711463040784074\n",
      "Iteration: 9481, training loss: 0.37114430512220004\n",
      "Iteration: 9482, training loss: 0.3711423064401577\n",
      "Iteration: 9483, training loss: 0.37114030803221315\n",
      "Iteration: 9484, training loss: 0.371138309898299\n",
      "Iteration: 9485, training loss: 0.3711363120383479\n",
      "Iteration: 9486, training loss: 0.37113431445229256\n",
      "Iteration: 9487, training loss: 0.3711323171400658\n",
      "Iteration: 9488, training loss: 0.37113032010160013\n",
      "Iteration: 9489, training loss: 0.37112832333682855\n",
      "Iteration: 9490, training loss: 0.37112632684568375\n",
      "Iteration: 9491, training loss: 0.37112433062809863\n",
      "Iteration: 9492, training loss: 0.371122334684006\n",
      "Iteration: 9493, training loss: 0.37112033901333863\n",
      "Iteration: 9494, training loss: 0.37111834361602963\n",
      "Iteration: 9495, training loss: 0.3711163484920117\n",
      "Iteration: 9496, training loss: 0.3711143536412179\n",
      "Iteration: 9497, training loss: 0.37111235906358125\n",
      "Iteration: 9498, training loss: 0.37111036475903464\n",
      "Iteration: 9499, training loss: 0.3711083707275111\n",
      "Iteration: 9500, training loss: 0.37110637696894366\n",
      "Iteration: 9501, training loss: 0.37110438348326547\n",
      "Iteration: 9502, training loss: 0.37110239027040953\n",
      "Iteration: 9503, training loss: 0.371100397330309\n",
      "Iteration: 9504, training loss: 0.371098404662897\n",
      "Iteration: 9505, training loss: 0.37109641226810663\n",
      "Iteration: 9506, training loss: 0.37109442014587124\n",
      "Iteration: 9507, training loss: 0.3710924282961238\n",
      "Iteration: 9508, training loss: 0.3710904367187979\n",
      "Iteration: 9509, training loss: 0.37108844541382635\n",
      "Iteration: 9510, training loss: 0.37108645438114274\n",
      "Iteration: 9511, training loss: 0.37108446362068037\n",
      "Iteration: 9512, training loss: 0.37108247313237247\n",
      "Iteration: 9513, training loss: 0.3710804829161524\n",
      "Iteration: 9514, training loss: 0.3710784929719536\n",
      "Iteration: 9515, training loss: 0.3710765032997093\n",
      "Iteration: 9516, training loss: 0.37107451389935314\n",
      "Iteration: 9517, training loss: 0.3710725247708186\n",
      "Iteration: 9518, training loss: 0.371070535914039\n",
      "Iteration: 9519, training loss: 0.3710685473289478\n",
      "Iteration: 9520, training loss: 0.3710665590154787\n",
      "Iteration: 9521, training loss: 0.37106457097356504\n",
      "Iteration: 9522, training loss: 0.3710625832031406\n",
      "Iteration: 9523, training loss: 0.37106059570413885\n",
      "Iteration: 9524, training loss: 0.37105860847649347\n",
      "Iteration: 9525, training loss: 0.37105662152013796\n",
      "Iteration: 9526, training loss: 0.37105463483500617\n",
      "Iteration: 9527, training loss: 0.3710526484210317\n",
      "Iteration: 9528, training loss: 0.37105066227814837\n",
      "Iteration: 9529, training loss: 0.37104867640628975\n",
      "Iteration: 9530, training loss: 0.3710466908053896\n",
      "Iteration: 9531, training loss: 0.3710447054753819\n",
      "Iteration: 9532, training loss: 0.37104272041620034\n",
      "Iteration: 9533, training loss: 0.3710407356277787\n",
      "Iteration: 9534, training loss: 0.3710387511100509\n",
      "Iteration: 9535, training loss: 0.37103676686295084\n",
      "Iteration: 9536, training loss: 0.3710347828864124\n",
      "Iteration: 9537, training loss: 0.3710327991803694\n",
      "Iteration: 9538, training loss: 0.371030815744756\n",
      "Iteration: 9539, training loss: 0.3710288325795061\n",
      "Iteration: 9540, training loss: 0.37102684968455363\n",
      "Iteration: 9541, training loss: 0.3710248670598326\n",
      "Iteration: 9542, training loss: 0.3710228847052772\n",
      "Iteration: 9543, training loss: 0.37102090262082144\n",
      "Iteration: 9544, training loss: 0.3710189208063993\n",
      "Iteration: 9545, training loss: 0.37101693926194507\n",
      "Iteration: 9546, training loss: 0.3710149579873928\n",
      "Iteration: 9547, training loss: 0.37101297698267666\n",
      "Iteration: 9548, training loss: 0.3710109962477308\n",
      "Iteration: 9549, training loss: 0.37100901578248957\n",
      "Iteration: 9550, training loss: 0.371007035586887\n",
      "Iteration: 9551, training loss: 0.37100505566085756\n",
      "Iteration: 9552, training loss: 0.3710030760043354\n",
      "Iteration: 9553, training loss: 0.37100109661725494\n",
      "Iteration: 9554, training loss: 0.37099911749955033\n",
      "Iteration: 9555, training loss: 0.3709971386511561\n",
      "Iteration: 9556, training loss: 0.3709951600720065\n",
      "Iteration: 9557, training loss: 0.37099318176203594\n",
      "Iteration: 9558, training loss: 0.370991203721179\n",
      "Iteration: 9559, training loss: 0.37098922594937006\n",
      "Iteration: 9560, training loss: 0.37098724844654346\n",
      "Iteration: 9561, training loss: 0.37098527121263386\n",
      "Iteration: 9562, training loss: 0.37098329424757565\n",
      "Iteration: 9563, training loss: 0.37098131755130354\n",
      "Iteration: 9564, training loss: 0.3709793411237519\n",
      "Iteration: 9565, training loss: 0.37097736496485545\n",
      "Iteration: 9566, training loss: 0.3709753890745488\n",
      "Iteration: 9567, training loss: 0.3709734134527665\n",
      "Iteration: 9568, training loss: 0.3709714380994432\n",
      "Iteration: 9569, training loss: 0.3709694630145137\n",
      "Iteration: 9570, training loss: 0.37096748819791275\n",
      "Iteration: 9571, training loss: 0.37096551364957486\n",
      "Iteration: 9572, training loss: 0.370963539369435\n",
      "Iteration: 9573, training loss: 0.37096156535742786\n",
      "Iteration: 9574, training loss: 0.37095959161348807\n",
      "Iteration: 9575, training loss: 0.37095761813755085\n",
      "Iteration: 9576, training loss: 0.37095564492955063\n",
      "Iteration: 9577, training loss: 0.37095367198942253\n",
      "Iteration: 9578, training loss: 0.37095169931710137\n",
      "Iteration: 9579, training loss: 0.3709497269125221\n",
      "Iteration: 9580, training loss: 0.3709477547756197\n",
      "Iteration: 9581, training loss: 0.37094578290632896\n",
      "Iteration: 9582, training loss: 0.37094381130458504\n",
      "Iteration: 9583, training loss: 0.3709418399703228\n",
      "Iteration: 9584, training loss: 0.3709398689034775\n",
      "Iteration: 9585, training loss: 0.3709378981039839\n",
      "Iteration: 9586, training loss: 0.3709359275717773\n",
      "Iteration: 9587, training loss: 0.37093395730679285\n",
      "Iteration: 9588, training loss: 0.3709319873089655\n",
      "Iteration: 9589, training loss: 0.3709300175782304\n",
      "Iteration: 9590, training loss: 0.3709280481145229\n",
      "Iteration: 9591, training loss: 0.37092607891777796\n",
      "Iteration: 9592, training loss: 0.370924109987931\n",
      "Iteration: 9593, training loss: 0.3709221413249172\n",
      "Iteration: 9594, training loss: 0.3709201729286718\n",
      "Iteration: 9595, training loss: 0.37091820479913007\n",
      "Iteration: 9596, training loss: 0.37091623693622733\n",
      "Iteration: 9597, training loss: 0.370914269339899\n",
      "Iteration: 9598, training loss: 0.37091230201008035\n",
      "Iteration: 9599, training loss: 0.3709103349467068\n",
      "Iteration: 9600, training loss: 0.3709083681497138\n",
      "Iteration: 9601, training loss: 0.3709064016190366\n",
      "Iteration: 9602, training loss: 0.3709044353546109\n",
      "Iteration: 9603, training loss: 0.37090246935637194\n",
      "Iteration: 9604, training loss: 0.3709005036242553\n",
      "Iteration: 9605, training loss: 0.37089853815819657\n",
      "Iteration: 9606, training loss: 0.37089657295813133\n",
      "Iteration: 9607, training loss: 0.3708946080239949\n",
      "Iteration: 9608, training loss: 0.37089264335572303\n",
      "Iteration: 9609, training loss: 0.3708906789532514\n",
      "Iteration: 9610, training loss: 0.3708887148165155\n",
      "Iteration: 9611, training loss: 0.37088675094545115\n",
      "Iteration: 9612, training loss: 0.3708847873399938\n",
      "Iteration: 9613, training loss: 0.3708828240000794\n",
      "Iteration: 9614, training loss: 0.3708808609256435\n",
      "Iteration: 9615, training loss: 0.37087889811662195\n",
      "Iteration: 9616, training loss: 0.3708769355729505\n",
      "Iteration: 9617, training loss: 0.37087497329456487\n",
      "Iteration: 9618, training loss: 0.370873011281401\n",
      "Iteration: 9619, training loss: 0.3708710495333946\n",
      "Iteration: 9620, training loss: 0.37086908805048163\n",
      "Iteration: 9621, training loss: 0.370867126832598\n",
      "Iteration: 9622, training loss: 0.3708651658796795\n",
      "Iteration: 9623, training loss: 0.37086320519166216\n",
      "Iteration: 9624, training loss: 0.37086124476848176\n",
      "Iteration: 9625, training loss: 0.37085928461007467\n",
      "Iteration: 9626, training loss: 0.37085732471637645\n",
      "Iteration: 9627, training loss: 0.37085536508732336\n",
      "Iteration: 9628, training loss: 0.37085340572285147\n",
      "Iteration: 9629, training loss: 0.37085144662289665\n",
      "Iteration: 9630, training loss: 0.3708494877873952\n",
      "Iteration: 9631, training loss: 0.37084752921628317\n",
      "Iteration: 9632, training loss: 0.3708455709094966\n",
      "Iteration: 9633, training loss: 0.37084361286697176\n",
      "Iteration: 9634, training loss: 0.3708416550886448\n",
      "Iteration: 9635, training loss: 0.37083969757445195\n",
      "Iteration: 9636, training loss: 0.37083774032432937\n",
      "Iteration: 9637, training loss: 0.3708357833382133\n",
      "Iteration: 9638, training loss: 0.37083382661604014\n",
      "Iteration: 9639, training loss: 0.37083187015774616\n",
      "Iteration: 9640, training loss: 0.37082991396326753\n",
      "Iteration: 9641, training loss: 0.37082795803254076\n",
      "Iteration: 9642, training loss: 0.37082600236550206\n",
      "Iteration: 9643, training loss: 0.370824046962088\n",
      "Iteration: 9644, training loss: 0.3708220918222348\n",
      "Iteration: 9645, training loss: 0.370820136945879\n",
      "Iteration: 9646, training loss: 0.37081818233295705\n",
      "Iteration: 9647, training loss: 0.3708162279834054\n",
      "Iteration: 9648, training loss: 0.3708142738971605\n",
      "Iteration: 9649, training loss: 0.370812320074159\n",
      "Iteration: 9650, training loss: 0.3708103665143373\n",
      "Iteration: 9651, training loss: 0.3708084132176321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9652, training loss: 0.3708064601839799\n",
      "Iteration: 9653, training loss: 0.37080450741331733\n",
      "Iteration: 9654, training loss: 0.37080255490558106\n",
      "Iteration: 9655, training loss: 0.37080060266070763\n",
      "Iteration: 9656, training loss: 0.37079865067863393\n",
      "Iteration: 9657, training loss: 0.37079669895929646\n",
      "Iteration: 9658, training loss: 0.370794747502632\n",
      "Iteration: 9659, training loss: 0.37079279630857737\n",
      "Iteration: 9660, training loss: 0.37079084537706924\n",
      "Iteration: 9661, training loss: 0.3707888947080445\n",
      "Iteration: 9662, training loss: 0.37078694430143994\n",
      "Iteration: 9663, training loss: 0.3707849941571922\n",
      "Iteration: 9664, training loss: 0.3707830442752385\n",
      "Iteration: 9665, training loss: 0.3707810946555154\n",
      "Iteration: 9666, training loss: 0.3707791452979599\n",
      "Iteration: 9667, training loss: 0.37077719620250904\n",
      "Iteration: 9668, training loss: 0.37077524736909956\n",
      "Iteration: 9669, training loss: 0.37077329879766874\n",
      "Iteration: 9670, training loss: 0.3707713504881532\n",
      "Iteration: 9671, training loss: 0.3707694024404902\n",
      "Iteration: 9672, training loss: 0.37076745465461675\n",
      "Iteration: 9673, training loss: 0.3707655071304698\n",
      "Iteration: 9674, training loss: 0.3707635598679866\n",
      "Iteration: 9675, training loss: 0.37076161286710413\n",
      "Iteration: 9676, training loss: 0.3707596661277594\n",
      "Iteration: 9677, training loss: 0.3707577196498899\n",
      "Iteration: 9678, training loss: 0.37075577343343247\n",
      "Iteration: 9679, training loss: 0.37075382747832447\n",
      "Iteration: 9680, training loss: 0.370751881784503\n",
      "Iteration: 9681, training loss: 0.3707499363519055\n",
      "Iteration: 9682, training loss: 0.370747991180469\n",
      "Iteration: 9683, training loss: 0.370746046270131\n",
      "Iteration: 9684, training loss: 0.3707441016208286\n",
      "Iteration: 9685, training loss: 0.37074215723249926\n",
      "Iteration: 9686, training loss: 0.37074021310508026\n",
      "Iteration: 9687, training loss: 0.37073826923850906\n",
      "Iteration: 9688, training loss: 0.3707363256327228\n",
      "Iteration: 9689, training loss: 0.37073438228765915\n",
      "Iteration: 9690, training loss: 0.3707324392032556\n",
      "Iteration: 9691, training loss: 0.3707304963794494\n",
      "Iteration: 9692, training loss: 0.3707285538161781\n",
      "Iteration: 9693, training loss: 0.3707266115133792\n",
      "Iteration: 9694, training loss: 0.37072466947099025\n",
      "Iteration: 9695, training loss: 0.3707227276889488\n",
      "Iteration: 9696, training loss: 0.37072078616719245\n",
      "Iteration: 9697, training loss: 0.37071884490565865\n",
      "Iteration: 9698, training loss: 0.37071690390428524\n",
      "Iteration: 9699, training loss: 0.37071496316300956\n",
      "Iteration: 9700, training loss: 0.37071302268176953\n",
      "Iteration: 9701, training loss: 0.3707110824605027\n",
      "Iteration: 9702, training loss: 0.3707091424991468\n",
      "Iteration: 9703, training loss: 0.3707072027976396\n",
      "Iteration: 9704, training loss: 0.3707052633559187\n",
      "Iteration: 9705, training loss: 0.37070332417392204\n",
      "Iteration: 9706, training loss: 0.3707013852515873\n",
      "Iteration: 9707, training loss: 0.3706994465888523\n",
      "Iteration: 9708, training loss: 0.37069750818565494\n",
      "Iteration: 9709, training loss: 0.37069557004193304\n",
      "Iteration: 9710, training loss: 0.3706936321576244\n",
      "Iteration: 9711, training loss: 0.37069169453266704\n",
      "Iteration: 9712, training loss: 0.3706897571669988\n",
      "Iteration: 9713, training loss: 0.3706878200605576\n",
      "Iteration: 9714, training loss: 0.3706858832132815\n",
      "Iteration: 9715, training loss: 0.37068394662510845\n",
      "Iteration: 9716, training loss: 0.3706820102959764\n",
      "Iteration: 9717, training loss: 0.37068007422582344\n",
      "Iteration: 9718, training loss: 0.3706781384145876\n",
      "Iteration: 9719, training loss: 0.3706762028622069\n",
      "Iteration: 9720, training loss: 0.3706742675686196\n",
      "Iteration: 9721, training loss: 0.37067233253376364\n",
      "Iteration: 9722, training loss: 0.3706703977575772\n",
      "Iteration: 9723, training loss: 0.37066846323999847\n",
      "Iteration: 9724, training loss: 0.3706665289809657\n",
      "Iteration: 9725, training loss: 0.3706645949804169\n",
      "Iteration: 9726, training loss: 0.3706626612382905\n",
      "Iteration: 9727, training loss: 0.3706607277545246\n",
      "Iteration: 9728, training loss: 0.37065879452905753\n",
      "Iteration: 9729, training loss: 0.3706568615618277\n",
      "Iteration: 9730, training loss: 0.37065492885277307\n",
      "Iteration: 9731, training loss: 0.3706529964018324\n",
      "Iteration: 9732, training loss: 0.37065106420894384\n",
      "Iteration: 9733, training loss: 0.37064913227404567\n",
      "Iteration: 9734, training loss: 0.37064720059707645\n",
      "Iteration: 9735, training loss: 0.3706452691779746\n",
      "Iteration: 9736, training loss: 0.37064333801667837\n",
      "Iteration: 9737, training loss: 0.37064140711312643\n",
      "Iteration: 9738, training loss: 0.3706394764672572\n",
      "Iteration: 9739, training loss: 0.37063754607900923\n",
      "Iteration: 9740, training loss: 0.3706356159483209\n",
      "Iteration: 9741, training loss: 0.3706336860751309\n",
      "Iteration: 9742, training loss: 0.3706317564593778\n",
      "Iteration: 9743, training loss: 0.37062982710100006\n",
      "Iteration: 9744, training loss: 0.3706278979999365\n",
      "Iteration: 9745, training loss: 0.3706259691561256\n",
      "Iteration: 9746, training loss: 0.37062404056950604\n",
      "Iteration: 9747, training loss: 0.3706221122400166\n",
      "Iteration: 9748, training loss: 0.37062018416759585\n",
      "Iteration: 9749, training loss: 0.37061825635218254\n",
      "Iteration: 9750, training loss: 0.3706163287937154\n",
      "Iteration: 9751, training loss: 0.3706144014921333\n",
      "Iteration: 9752, training loss: 0.370612474447375\n",
      "Iteration: 9753, training loss: 0.3706105476593792\n",
      "Iteration: 9754, training loss: 0.37060862112808474\n",
      "Iteration: 9755, training loss: 0.37060669485343056\n",
      "Iteration: 9756, training loss: 0.37060476883535554\n",
      "Iteration: 9757, training loss: 0.3706028430737985\n",
      "Iteration: 9758, training loss: 0.37060091756869845\n",
      "Iteration: 9759, training loss: 0.3705989923199942\n",
      "Iteration: 9760, training loss: 0.3705970673276248\n",
      "Iteration: 9761, training loss: 0.37059514259152926\n",
      "Iteration: 9762, training loss: 0.3705932181116465\n",
      "Iteration: 9763, training loss: 0.37059129388791545\n",
      "Iteration: 9764, training loss: 0.37058936992027536\n",
      "Iteration: 9765, training loss: 0.37058744620866524\n",
      "Iteration: 9766, training loss: 0.37058552275302403\n",
      "Iteration: 9767, training loss: 0.370583599553291\n",
      "Iteration: 9768, training loss: 0.3705816766094052\n",
      "Iteration: 9769, training loss: 0.3705797539213058\n",
      "Iteration: 9770, training loss: 0.37057783148893203\n",
      "Iteration: 9771, training loss: 0.37057590931222306\n",
      "Iteration: 9772, training loss: 0.37057398739111796\n",
      "Iteration: 9773, training loss: 0.3705720657255561\n",
      "Iteration: 9774, training loss: 0.3705701443154768\n",
      "Iteration: 9775, training loss: 0.3705682231608191\n",
      "Iteration: 9776, training loss: 0.3705663022615226\n",
      "Iteration: 9777, training loss: 0.37056438161752636\n",
      "Iteration: 9778, training loss: 0.37056246122876996\n",
      "Iteration: 9779, training loss: 0.37056054109519243\n",
      "Iteration: 9780, training loss: 0.3705586212167335\n",
      "Iteration: 9781, training loss: 0.3705567015933324\n",
      "Iteration: 9782, training loss: 0.37055478222492855\n",
      "Iteration: 9783, training loss: 0.3705528631114616\n",
      "Iteration: 9784, training loss: 0.3705509442528706\n",
      "Iteration: 9785, training loss: 0.3705490256490955\n",
      "Iteration: 9786, training loss: 0.37054710730007545\n",
      "Iteration: 9787, training loss: 0.3705451892057502\n",
      "Iteration: 9788, training loss: 0.3705432713660592\n",
      "Iteration: 9789, training loss: 0.3705413537809421\n",
      "Iteration: 9790, training loss: 0.37053943645033843\n",
      "Iteration: 9791, training loss: 0.3705375193741879\n",
      "Iteration: 9792, training loss: 0.37053560255242995\n",
      "Iteration: 9793, training loss: 0.3705336859850044\n",
      "Iteration: 9794, training loss: 0.3705317696718509\n",
      "Iteration: 9795, training loss: 0.37052985361290913\n",
      "Iteration: 9796, training loss: 0.37052793780811893\n",
      "Iteration: 9797, training loss: 0.37052602225741976\n",
      "Iteration: 9798, training loss: 0.3705241069607517\n",
      "Iteration: 9799, training loss: 0.3705221919180543\n",
      "Iteration: 9800, training loss: 0.3705202771292675\n",
      "Iteration: 9801, training loss: 0.3705183625943312\n",
      "Iteration: 9802, training loss: 0.37051644831318487\n",
      "Iteration: 9803, training loss: 0.37051453428576886\n",
      "Iteration: 9804, training loss: 0.3705126205120227\n",
      "Iteration: 9805, training loss: 0.37051070699188654\n",
      "Iteration: 9806, training loss: 0.37050879372530016\n",
      "Iteration: 9807, training loss: 0.37050688071220367\n",
      "Iteration: 9808, training loss: 0.3705049679525368\n",
      "Iteration: 9809, training loss: 0.37050305544623974\n",
      "Iteration: 9810, training loss: 0.3705011431932524\n",
      "Iteration: 9811, training loss: 0.3704992311935148\n",
      "Iteration: 9812, training loss: 0.3704973194469671\n",
      "Iteration: 9813, training loss: 0.3704954079535494\n",
      "Iteration: 9814, training loss: 0.3704934967132017\n",
      "Iteration: 9815, training loss: 0.3704915857258641\n",
      "Iteration: 9816, training loss: 0.37048967499147684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9817, training loss: 0.37048776450998\n",
      "Iteration: 9818, training loss: 0.37048585428131375\n",
      "Iteration: 9819, training loss: 0.37048394430541837\n",
      "Iteration: 9820, training loss: 0.37048203458223405\n",
      "Iteration: 9821, training loss: 0.370480125111701\n",
      "Iteration: 9822, training loss: 0.3704782158937595\n",
      "Iteration: 9823, training loss: 0.3704763069283499\n",
      "Iteration: 9824, training loss: 0.37047439821541234\n",
      "Iteration: 9825, training loss: 0.3704724897548874\n",
      "Iteration: 9826, training loss: 0.37047058154671514\n",
      "Iteration: 9827, training loss: 0.3704686735908362\n",
      "Iteration: 9828, training loss: 0.3704667658871908\n",
      "Iteration: 9829, training loss: 0.3704648584357193\n",
      "Iteration: 9830, training loss: 0.3704629512363623\n",
      "Iteration: 9831, training loss: 0.37046104428906024\n",
      "Iteration: 9832, training loss: 0.37045913759375343\n",
      "Iteration: 9833, training loss: 0.3704572311503824\n",
      "Iteration: 9834, training loss: 0.37045532495888794\n",
      "Iteration: 9835, training loss: 0.37045341901921014\n",
      "Iteration: 9836, training loss: 0.3704515133312899\n",
      "Iteration: 9837, training loss: 0.3704496078950676\n",
      "Iteration: 9838, training loss: 0.3704477027104839\n",
      "Iteration: 9839, training loss: 0.3704457977774795\n",
      "Iteration: 9840, training loss: 0.3704438930959949\n",
      "Iteration: 9841, training loss: 0.37044198866597083\n",
      "Iteration: 9842, training loss: 0.37044008448734794\n",
      "Iteration: 9843, training loss: 0.370438180560067\n",
      "Iteration: 9844, training loss: 0.3704362768840686\n",
      "Iteration: 9845, training loss: 0.3704343734592936\n",
      "Iteration: 9846, training loss: 0.37043247028568266\n",
      "Iteration: 9847, training loss: 0.37043056736317664\n",
      "Iteration: 9848, training loss: 0.37042866469171626\n",
      "Iteration: 9849, training loss: 0.37042676227124244\n",
      "Iteration: 9850, training loss: 0.37042486010169595\n",
      "Iteration: 9851, training loss: 0.37042295818301774\n",
      "Iteration: 9852, training loss: 0.37042105651514856\n",
      "Iteration: 9853, training loss: 0.3704191550980294\n",
      "Iteration: 9854, training loss: 0.3704172539316012\n",
      "Iteration: 9855, training loss: 0.37041535301580486\n",
      "Iteration: 9856, training loss: 0.3704134523505813\n",
      "Iteration: 9857, training loss: 0.37041155193587166\n",
      "Iteration: 9858, training loss: 0.37040965177161683\n",
      "Iteration: 9859, training loss: 0.37040775185775776\n",
      "Iteration: 9860, training loss: 0.37040585219423566\n",
      "Iteration: 9861, training loss: 0.37040395278099153\n",
      "Iteration: 9862, training loss: 0.3704020536179664\n",
      "Iteration: 9863, training loss: 0.37040015470510146\n",
      "Iteration: 9864, training loss: 0.37039825604233784\n",
      "Iteration: 9865, training loss: 0.3703963576296166\n",
      "Iteration: 9866, training loss: 0.37039445946687904\n",
      "Iteration: 9867, training loss: 0.3703925615540663\n",
      "Iteration: 9868, training loss: 0.3703906638911194\n",
      "Iteration: 9869, training loss: 0.3703887664779799\n",
      "Iteration: 9870, training loss: 0.37038686931458886\n",
      "Iteration: 9871, training loss: 0.3703849724008876\n",
      "Iteration: 9872, training loss: 0.3703830757368173\n",
      "Iteration: 9873, training loss: 0.3703811793223194\n",
      "Iteration: 9874, training loss: 0.37037928315733515\n",
      "Iteration: 9875, training loss: 0.370377387241806\n",
      "Iteration: 9876, training loss: 0.37037549157567323\n",
      "Iteration: 9877, training loss: 0.37037359615887827\n",
      "Iteration: 9878, training loss: 0.37037170099136246\n",
      "Iteration: 9879, training loss: 0.3703698060730674\n",
      "Iteration: 9880, training loss: 0.3703679114039344\n",
      "Iteration: 9881, training loss: 0.370366016983905\n",
      "Iteration: 9882, training loss: 0.37036412281292064\n",
      "Iteration: 9883, training loss: 0.37036222889092285\n",
      "Iteration: 9884, training loss: 0.37036033521785316\n",
      "Iteration: 9885, training loss: 0.3703584417936532\n",
      "Iteration: 9886, training loss: 0.3703565486182645\n",
      "Iteration: 9887, training loss: 0.37035465569162856\n",
      "Iteration: 9888, training loss: 0.370352763013687\n",
      "Iteration: 9889, training loss: 0.3703508705843816\n",
      "Iteration: 9890, training loss: 0.370348978403654\n",
      "Iteration: 9891, training loss: 0.37034708647144576\n",
      "Iteration: 9892, training loss: 0.3703451947876986\n",
      "Iteration: 9893, training loss: 0.3703433033523544\n",
      "Iteration: 9894, training loss: 0.3703414121653546\n",
      "Iteration: 9895, training loss: 0.3703395212266411\n",
      "Iteration: 9896, training loss: 0.3703376305361558\n",
      "Iteration: 9897, training loss: 0.3703357400938403\n",
      "Iteration: 9898, training loss: 0.37033384989963647\n",
      "Iteration: 9899, training loss: 0.3703319599534862\n",
      "Iteration: 9900, training loss: 0.3703300702553314\n",
      "Iteration: 9901, training loss: 0.3703281808051138\n",
      "Iteration: 9902, training loss: 0.3703262916027753\n",
      "Iteration: 9903, training loss: 0.3703244026482579\n",
      "Iteration: 9904, training loss: 0.3703225139415036\n",
      "Iteration: 9905, training loss: 0.3703206254824542\n",
      "Iteration: 9906, training loss: 0.3703187372710517\n",
      "Iteration: 9907, training loss: 0.3703168493072381\n",
      "Iteration: 9908, training loss: 0.37031496159095556\n",
      "Iteration: 9909, training loss: 0.3703130741221459\n",
      "Iteration: 9910, training loss: 0.3703111869007514\n",
      "Iteration: 9911, training loss: 0.3703092999267139\n",
      "Iteration: 9912, training loss: 0.3703074131999757\n",
      "Iteration: 9913, training loss: 0.3703055267204787\n",
      "Iteration: 9914, training loss: 0.3703036404881652\n",
      "Iteration: 9915, training loss: 0.3703017545029773\n",
      "Iteration: 9916, training loss: 0.37029986876485727\n",
      "Iteration: 9917, training loss: 0.3702979832737471\n",
      "Iteration: 9918, training loss: 0.3702960980295891\n",
      "Iteration: 9919, training loss: 0.3702942130323256\n",
      "Iteration: 9920, training loss: 0.3702923282818988\n",
      "Iteration: 9921, training loss: 0.37029044377825093\n",
      "Iteration: 9922, training loss: 0.37028855952132433\n",
      "Iteration: 9923, training loss: 0.3702866755110612\n",
      "Iteration: 9924, training loss: 0.3702847917474041\n",
      "Iteration: 9925, training loss: 0.3702829082302951\n",
      "Iteration: 9926, training loss: 0.3702810249596768\n",
      "Iteration: 9927, training loss: 0.3702791419354915\n",
      "Iteration: 9928, training loss: 0.3702772591576816\n",
      "Iteration: 9929, training loss: 0.37027537662618953\n",
      "Iteration: 9930, training loss: 0.3702734943409578\n",
      "Iteration: 9931, training loss: 0.37027161230192884\n",
      "Iteration: 9932, training loss: 0.37026973050904516\n",
      "Iteration: 9933, training loss: 0.3702678489622493\n",
      "Iteration: 9934, training loss: 0.37026596766148373\n",
      "Iteration: 9935, training loss: 0.3702640866066909\n",
      "Iteration: 9936, training loss: 0.37026220579781366\n",
      "Iteration: 9937, training loss: 0.3702603252347944\n",
      "Iteration: 9938, training loss: 0.3702584449175757\n",
      "Iteration: 9939, training loss: 0.3702565648461003\n",
      "Iteration: 9940, training loss: 0.37025468502031084\n",
      "Iteration: 9941, training loss: 0.37025280544014993\n",
      "Iteration: 9942, training loss: 0.37025092610556026\n",
      "Iteration: 9943, training loss: 0.3702490470164846\n",
      "Iteration: 9944, training loss: 0.3702471681728656\n",
      "Iteration: 9945, training loss: 0.3702452895746461\n",
      "Iteration: 9946, training loss: 0.37024341122176874\n",
      "Iteration: 9947, training loss: 0.3702415331141764\n",
      "Iteration: 9948, training loss: 0.37023965525181185\n",
      "Iteration: 9949, training loss: 0.37023777763461796\n",
      "Iteration: 9950, training loss: 0.37023590026253755\n",
      "Iteration: 9951, training loss: 0.37023402313551346\n",
      "Iteration: 9952, training loss: 0.3702321462534886\n",
      "Iteration: 9953, training loss: 0.3702302696164059\n",
      "Iteration: 9954, training loss: 0.3702283932242082\n",
      "Iteration: 9955, training loss: 0.37022651707683857\n",
      "Iteration: 9956, training loss: 0.3702246411742397\n",
      "Iteration: 9957, training loss: 0.370222765516355\n",
      "Iteration: 9958, training loss: 0.37022089010312714\n",
      "Iteration: 9959, training loss: 0.3702190149344992\n",
      "Iteration: 9960, training loss: 0.37021714001041434\n",
      "Iteration: 9961, training loss: 0.37021526533081545\n",
      "Iteration: 9962, training loss: 0.37021339089564576\n",
      "Iteration: 9963, training loss: 0.3702115167048482\n",
      "Iteration: 9964, training loss: 0.3702096427583661\n",
      "Iteration: 9965, training loss: 0.37020776905614244\n",
      "Iteration: 9966, training loss: 0.37020589559812045\n",
      "Iteration: 9967, training loss: 0.3702040223842432\n",
      "Iteration: 9968, training loss: 0.3702021494144541\n",
      "Iteration: 9969, training loss: 0.37020027668869615\n",
      "Iteration: 9970, training loss: 0.37019840420691275\n",
      "Iteration: 9971, training loss: 0.370196531969047\n",
      "Iteration: 9972, training loss: 0.37019465997504225\n",
      "Iteration: 9973, training loss: 0.37019278822484175\n",
      "Iteration: 9974, training loss: 0.37019091671838894\n",
      "Iteration: 9975, training loss: 0.3701890454556269\n",
      "Iteration: 9976, training loss: 0.3701871744364994\n",
      "Iteration: 9977, training loss: 0.37018530366094937\n",
      "Iteration: 9978, training loss: 0.37018343312892044\n",
      "Iteration: 9979, training loss: 0.37018156284035597\n",
      "Iteration: 9980, training loss: 0.37017969279519936\n",
      "Iteration: 9981, training loss: 0.370177822993394\n",
      "Iteration: 9982, training loss: 0.3701759534348836\n",
      "Iteration: 9983, training loss: 0.3701740841196113\n",
      "Iteration: 9984, training loss: 0.37017221504752085\n",
      "Iteration: 9985, training loss: 0.37017034621855566\n",
      "Iteration: 9986, training loss: 0.37016847763265937\n",
      "Iteration: 9987, training loss: 0.37016660928977546\n",
      "Iteration: 9988, training loss: 0.3701647411898476\n",
      "Iteration: 9989, training loss: 0.37016287333281933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9990, training loss: 0.37016100571863425\n",
      "Iteration: 9991, training loss: 0.370159138347236\n",
      "Iteration: 9992, training loss: 0.37015727121856834\n",
      "Iteration: 9993, training loss: 0.3701554043325749\n",
      "Iteration: 9994, training loss: 0.37015353768919923\n",
      "Iteration: 9995, training loss: 0.3701516712883852\n",
      "Iteration: 9996, training loss: 0.37014980513007656\n",
      "Iteration: 9997, training loss: 0.37014793921421696\n",
      "Iteration: 9998, training loss: 0.37014607354075024\n",
      "Iteration: 9999, training loss: 0.37014420810962023\n"
     ]
    }
   ],
   "source": [
    "y_train_RFC = rfc.predict(train_X.to_numpy())\n",
    "y_test_RFC = rfc.predict(test_X.to_numpy())\n",
    "\n",
    "from optimizer import Logistic_Regression\n",
    "\n",
    "LR = Logistic_Regression()\n",
    "\n",
    "w0 = np.zeros((train_X.shape[1]+1, 1))\n",
    "num_iter = 10000\n",
    "lr = 1e-2\n",
    "solver = \"GD\"\n",
    "\n",
    "w, hist = LR.fit(solver,\n",
    "               train_X.to_numpy(), \n",
    "               y_train_RFC.reshape((-1, 1)), \n",
    "               w0, \n",
    "               num_iter = num_iter, \n",
    "               lr = lr, \n",
    "               test_X = test_X.to_numpy(), \n",
    "               test_y = test_Y.to_numpy().reshape((-1, 1)),\n",
    "               debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marital-status_ Never-married        -1.019990\n",
       "relationship_ Own-child              -0.800609\n",
       "sex_ Female                          -0.664707\n",
       "occupation_ Other-service            -0.641157\n",
       "education_ HS-grad                   -0.557572\n",
       "                                        ...   \n",
       "education_ Masters                    0.526404\n",
       "relationship_ Wife                    0.549355\n",
       "occupation_ Prof-specialty            0.603843\n",
       "occupation_ Exec-managerial           0.751968\n",
       "marital-status_ Married-civ-spouse    0.772295\n",
       "Length: 108, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_features = pd.Series(w[1:].flatten(), index = train_X.columns)\n",
    "w_features.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fnlwgt                               -0.084626\n",
       "age                                   0.262488\n",
       "capital-gain                          0.452387\n",
       "hours-per-week                        0.186786\n",
       "marital-status_ Married-civ-spouse    0.772295\n",
       "education-num                         0.483809\n",
       "relationship_ Husband                 0.291109\n",
       "capital-loss                          0.337107\n",
       "occupation_ Exec-managerial           0.751968\n",
       "marital-status_ Never-married        -1.019990\n",
       "occupation_ Prof-specialty            0.603843\n",
       "education_ Bachelors                  0.411954\n",
       "relationship_ Not-in-family          -0.263928\n",
       "workclass_ Private                   -0.262263\n",
       "relationship_ Wife                    0.549355\n",
       "sex_ Female                          -0.664707\n",
       "education_ Masters                    0.526404\n",
       "relationship_ Own-child              -0.800609\n",
       "workclass_ Self-emp-not-inc          -0.353122\n",
       "education_ HS-grad                   -0.557572\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_features[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
