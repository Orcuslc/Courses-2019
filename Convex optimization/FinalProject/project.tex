\documentclass{article}
\usepackage[left=2cm,right=2cm,top=2cm]{geometry}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{amsthm,amsmath,amssymb}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\bx}{{\mathbf x}}
\newcommand{\ba}{{\mathbf a}}
\newcommand{\bv}{{\mathbf v}}
\newcommand{\bu}{{\mathbf u}}

\newcommand{\ml}{\mathcal{L}}

\numberwithin{equation}{section}
\numberwithin{figure}{section}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}[theorem]{Example}


\begin{document}

\title{Management Sciences Topics: Convex Optimization\\ Final Project}
\date{}
\maketitle

\section{Problem setup}

We need to solve the optimization problem of a one-hidden-layer neural network
\begin{equation}
\min_{x_k\in\mathbb{R}^d, y_k\in\mathbb{R}, z\in\mathbb{R}^K, w\in\mathbb{R}} \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}\left(b_iw+b_i\sum_{k=1}^K \sigma(a_i^\top x_k + y_k)z_k\right),
\end{equation}
where $K$ is the number of neurons, $a_i\in\mathbb{R}^d $ is a data point, $b_i\in\{-1, 1\} $ is the class label of $a_i $, $\sigma(z) = \max(z, 0) $ or $\frac{\exp(z)}{1+\exp(z)}$, $\mathcal{L}(z) = \max(1-z, 0)$ or $\log(1+\exp(-z))$.

\section{Stochastic subgradient method}

% First, we consider the subgradient with respective to each variables:

% \begin{equation}
% \begin{aligned}
% \partial_w f &= \frac{1}{n}\sum_{i=1}^n\mathcal{L}'\left(b_iw+b_i\sum_{k=1}^K \sigma(a_i^\top x_k + y_k)z_k\right) b_i, \\
% \partial_{z_k} f &= \frac{1}{n}\sum_{i=1}^n\mathcal{L}'\left(b_iw+b_i\sum_{i=1}^{K}\sigma(a_i^\top x_k + y_k)z_k\right) b_i\sigma(a_i^\top x_k+y_k), \\
% \partial_{y_k} f &= \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}'\left(b_iw+b_i\sum_{i=1}^{K}\sigma(a_i^\top x_k + y_k)z_k\right)b_i\sigma'(a_i^\top x_k + y_k)z_k, \\
% \partial_{x_k} f &= \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}'\left(b_iw+b_i\sum_{i=1}^{K}\sigma(a_i^\top x_k + y_k)z_k\right)b_i\sigma'(a_i^\top x_k+y_k)z_ka_i,
% \end{aligned}
% \end{equation}
% where $\sigma'$ and $\mathcal{L}'$ are both subgradients when the functions are not differentiable.

% We may vectorize the computation. First, define the parameter vectors:
% \begin{equation}
% \begin{aligned}
% X = [x_1, x_2, \hdots, x_K] \in\mathbb{R}^{d\times K}, \quad Y = [y_1, y_2, \hdots, y_k]^\top \in\mathbb{R}^{K\times 1}, \quad Z = [z_1, z_2, \hdots, z_k]^\top \in\mathbb{R}^{K\times 1},
% \end{aligned}
% \end{equation}
% and
% \begin{equation}
% \begin{aligned}
% W_i(x, y, z, w) &= b_iw+b_i\sum_{k=1}^{K}\sigma(a_i^\top x_k+y_k)z_k = b_i w + b_i \sigma(a_i^\top X \oplus Y^\top) Z \in \mathbb{R}.
% \end{aligned}
% \end{equation}
% Here $\oplus, \odot$ denote pointwise operations. Let
% \begin{equation}
% \begin{aligned}
% R_i &= a_i^\top X\oplus Y^\top \in \mathbb{R}^{1\times K}, \\
% R &= [R_1^\top, \hdots, R_n^\top]^\top \in \mathbb{R}^{n\times K}, \\
% W &= [W_1, \hdots, W_n]^\top \in \mathbb{R}^{n\times 1},
% \end{aligned}
% \end{equation}
% then
% \begin{equation}
% \begin{aligned}
% R &= AX\oplus Y^\top \\
% W &= b\odot (w+\sigma(R)Z),
% \end{aligned}
% \end{equation}
% and the target function and subgradients can be written as 
% \begin{equation}
% \begin{aligned}
% f &= \frac{1}{n} 1^\top \mathcal{L}(W), \\
% \partial_w f &= \frac{1}{n} b^\top \mathcal{L}'(W) , \\
% \partial_z f &= \frac{1}{n} \sigma(AX\oplus Y^\top)^\top (\mathcal{L}'(W)\odot b) = \frac{1}{n} \sigma(R)^\top (\mathcal{L}'(W)\odot b), \\
% \partial_y f &= \frac{1}{n} (\sigma'(R)^\top (\mathcal{L}'(W)\odot b)) \odot Z, \\
% \partial_x f &= \frac{1}{n} (A^\top (\mathcal{L}'(W)\odot b \odot \sigma'(R)))\odot Z^\top.
% \end{aligned}
% \end{equation}
% We use an iterative scheme to update the four variables.

We first consider the subgradient with respect to each variables. 

First, define the variables
\begin{equation}
X = [x_1, x_2, \hdots, x_K] \in\mathbb{R}^{d\times K}, \quad Y = [y_1, y_2, \hdots, y_k]^\top \in\mathbb{R}^{K\times 1}, \quad Z = [z_1, z_2, \hdots, z_k]^\top \in\mathbb{R}^{K\times 1}.
\end{equation}

Then a forward pass through the network can be written as

\begin{equation}
\begin{aligned}
&A_1 = AX\oplus Y^\top, \\
&A_2 = \sigma(A_1)Z \oplus w \\
&f = \frac{1}{n}1^\top L(b\odot A_2).
\end{aligned}
\end{equation}

By chain rule, the subgradients of $f$ with respect to each variable are
\begin{equation}
\begin{aligned}
&\frac{\partial f}{\partial A_2} = \frac{1}{n}L'(b\odot A_2)\odot b \\
&\frac{\partial f}{\partial w} = \frac{\partial f}{\partial A_2}\frac{\partial A_2}{\partial w} = \text{rowsum}(\frac{\partial f}{\partial A_2}\odot 1) = \left(\frac{\partial f}{\partial A_2}\right)^\top 1, \\
&\frac{\partial f}{\partial Z} = \frac{\partial f}{\partial A_2}\frac{\partial A_2}{\partial Z} = \sigma(A_1)^\top \frac{\partial f}{\partial A_2}, \\
&\frac{\partial f}{\partial A_1} = \frac{\partial f}{\partial A_2}\frac{\partial A_2}{\partial A_1} = \frac{\partial f}{\partial A_2}Z^\top \odot \sigma'(A_1), \\
&\frac{\partial f}{\partial Y} = \frac{\partial f}{\partial A_1}\frac{\partial A_1}{\partial Y} = \left(\frac{\partial f}{\partial A_1}\right)^\top 1, \\
&\frac{\partial f}{\partial X} = \frac{\partial f}{\partial A_1}\frac{\partial A_1}{\partial X} = A^\top \frac{\partial f}{\partial A_1}.
\end{aligned}
\end{equation}

The stochastic subgradients can be chosen to be the subgradient when input is a minibatch of the whole dataset, i.e.
\begin{equation}
G(x, \xi_i) = \partial_x f(x; A_{\xi_i}, b_{\xi_i})
\end{equation}
for each variable $x$, where $\xi_i $ is a uniformly sample index set for each $i$. We can control the size of each $\xi_i $ to vary from online learning to full-batch learning.

\section{Proximal Gradient method with line search}
In order to use APG for this problem, we need to choose 
\begin{equation}
\sigma(z) = \frac{\exp(z)}{1+\exp(z)}, \quad \mathcal{L}(z) = \log(1+\exp(-z))
\end{equation}
to guarantee the objective function is smooth.
\begin{remark}
For the stop criterion of line search: We try two different methods. 1. After updating $x_1 $ by $x_0 $, we still use $x_0 $ to compute the derivative with respect to $y, z, w$. Similarly, we still use $x_0, y_0 $ instead of the updated $x_1, y_1 $ to compute the derivative of $z, w$, and etc. 2. Use the u[]
\end{remark}


\section{Accelerated proximal gradient method}

In order to use APG for this problem, we need to choose 
\begin{equation}
\sigma(z) = \frac{\exp(z)}{1+\exp(z)}, \quad \mathcal{L}(z) = \log(1+\exp(-z))
\end{equation}
to guarantee the objective function is smooth. Notice
\begin{equation}
\mathcal{L}'(z) = -\frac{1}{1+\exp(z)}, 
\end{equation}
and
\begin{equation}
|\ml''(z)| = \left|\frac{e^z}{(1+e^z)^2}\right| \le \frac{1}{4},
\end{equation}
by Lagrange mean value theorem, we know the Lipschitz constant for $\ml'$ is $L=\frac{1}{4}$.

Now let's consider the Lipschitz constant for each derivatives. For $\partial_{A_2} $,
\begin{equation}
\begin{aligned}
\lVert \partial_{A_2}
\end{aligned}
\end{equation}


For $\frac{\partial f}{\partial w}$,
\begin{equation}
\begin{aligned}
|\partial_w (w_1) - \partial_w (w_2)| &= \frac{1}{n}\sum_{i=1}^{n}b_i(\ml'(b_i(\sigma(A_1)Z)_i + b_i w_1) - \ml' (b_i(\sigma(A_1)Z)_i + b_i w_2) \\
&\le \frac{1}{n}\sum_{i=1}^{n}b_i(\frac{1}{4}b_i|w_1-w_2|) = \frac{1}{4n}\sum_{i=1}^{n}|w_1-w_2| \\
&= \frac{1}{4}|w_1-w_2|.
\end{aligned}
\end{equation}
Then the Lipschitz constant for $\partial_w $ is $L_w = \frac{1}{4} $.


\end{document}
