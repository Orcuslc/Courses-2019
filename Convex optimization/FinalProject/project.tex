\documentclass{article}
\usepackage[left=2cm,right=2cm,top=2cm]{geometry}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{amsthm,amsmath,amssymb}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\bx}{{\mathbf x}}
\newcommand{\ba}{{\mathbf a}}
\newcommand{\bv}{{\mathbf v}}
\newcommand{\bu}{{\mathbf u}}

\newcommand{\ml}{\mathcal{L}}

\numberwithin{equation}{section}
\numberwithin{figure}{section}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}[theorem]{Example}


\begin{document}

\title{Management Sciences Topics: Convex Optimization\\ Final Project}
\date{}
\maketitle

\section{Problem setup}

We need to solve the optimization problem of a one-hidden-layer neural network
\begin{equation}
\min_{x_k\in\mathbb{R}^d, y_k\in\mathbb{R}, z\in\mathbb{R}^K, w\in\mathbb{R}} \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}\left(b_iw+b_i\sum_{k=1}^K \sigma(a_i^\top x_k + y_k)z_k\right),
\end{equation}
where $K$ is the number of neurons, $a_i\in\mathbb{R}^d $ is a data point, $b_i\in\{-1, 1\} $ is the class label of $a_i $, $\sigma(z) = \max(z, 0) $ or $\frac{\exp(z)}{1+\exp(z)}$, $\mathcal{L}(z) = \max(1-z, 0)$ or $\log(1+\exp(-z))$.

\section{Stochastic subgradient method}

% First, we consider the subgradient with respective to each variables:

% \begin{equation}
% \begin{aligned}
% \partial_w f &= \frac{1}{n}\sum_{i=1}^n\mathcal{L}'\left(b_iw+b_i\sum_{k=1}^K \sigma(a_i^\top x_k + y_k)z_k\right) b_i, \\
% \partial_{z_k} f &= \frac{1}{n}\sum_{i=1}^n\mathcal{L}'\left(b_iw+b_i\sum_{i=1}^{K}\sigma(a_i^\top x_k + y_k)z_k\right) b_i\sigma(a_i^\top x_k+y_k), \\
% \partial_{y_k} f &= \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}'\left(b_iw+b_i\sum_{i=1}^{K}\sigma(a_i^\top x_k + y_k)z_k\right)b_i\sigma'(a_i^\top x_k + y_k)z_k, \\
% \partial_{x_k} f &= \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}'\left(b_iw+b_i\sum_{i=1}^{K}\sigma(a_i^\top x_k + y_k)z_k\right)b_i\sigma'(a_i^\top x_k+y_k)z_ka_i,
% \end{aligned}
% \end{equation}
% where $\sigma'$ and $\mathcal{L}'$ are both subgradients when the functions are not differentiable.

% We may vectorize the computation. First, define the parameter vectors:
% \begin{equation}
% \begin{aligned}
% X = [x_1, x_2, \hdots, x_K] \in\mathbb{R}^{d\times K}, \quad Y = [y_1, y_2, \hdots, y_k]^\top \in\mathbb{R}^{K\times 1}, \quad Z = [z_1, z_2, \hdots, z_k]^\top \in\mathbb{R}^{K\times 1},
% \end{aligned}
% \end{equation}
% and
% \begin{equation}
% \begin{aligned}
% W_i(x, y, z, w) &= b_iw+b_i\sum_{k=1}^{K}\sigma(a_i^\top x_k+y_k)z_k = b_i w + b_i \sigma(a_i^\top X \oplus Y^\top) Z \in \mathbb{R}.
% \end{aligned}
% \end{equation}
% Here $\oplus, \odot$ denote pointwise operations. Let
% \begin{equation}
% \begin{aligned}
% R_i &= a_i^\top X\oplus Y^\top \in \mathbb{R}^{1\times K}, \\
% R &= [R_1^\top, \hdots, R_n^\top]^\top \in \mathbb{R}^{n\times K}, \\
% W &= [W_1, \hdots, W_n]^\top \in \mathbb{R}^{n\times 1},
% \end{aligned}
% \end{equation}
% then
% \begin{equation}
% \begin{aligned}
% R &= AX\oplus Y^\top \\
% W &= b\odot (w+\sigma(R)Z),
% \end{aligned}
% \end{equation}
% and the target function and subgradients can be written as 
% \begin{equation}
% \begin{aligned}
% f &= \frac{1}{n} 1^\top \mathcal{L}(W), \\
% \partial_w f &= \frac{1}{n} b^\top \mathcal{L}'(W) , \\
% \partial_z f &= \frac{1}{n} \sigma(AX\oplus Y^\top)^\top (\mathcal{L}'(W)\odot b) = \frac{1}{n} \sigma(R)^\top (\mathcal{L}'(W)\odot b), \\
% \partial_y f &= \frac{1}{n} (\sigma'(R)^\top (\mathcal{L}'(W)\odot b)) \odot Z, \\
% \partial_x f &= \frac{1}{n} (A^\top (\mathcal{L}'(W)\odot b \odot \sigma'(R)))\odot Z^\top.
% \end{aligned}
% \end{equation}
% We use an iterative scheme to update the four variables.

We first consider the subgradient with respect to each variables. 

First, define the variables
\begin{equation}
X = [x_1, x_2, \hdots, x_K] \in\mathbb{R}^{d\times K}, \quad Y = [y_1, y_2, \hdots, y_k]^\top \in\mathbb{R}^{K\times 1}, \quad Z = [z_1, z_2, \hdots, z_k]^\top \in\mathbb{R}^{K\times 1}.
\end{equation}

Then a forward pass through the network can be written as

\begin{equation}
\begin{aligned}
&A_1 = AX\oplus Y^\top, \\
&A_2 = \sigma(A_1)Z \oplus w \\
&f = \frac{1}{n}1^\top L(b\odot A_2).
\end{aligned}
\end{equation}

By chain rule, the subgradients of $f$ with respect to each variable are
\begin{equation}
\begin{aligned}
&\frac{\partial f}{\partial A_2} = \frac{1}{n}L'(b\odot A_2)\odot b \\
&\frac{\partial f}{\partial w} = \frac{\partial f}{\partial A_2}\frac{\partial A_2}{\partial w} = \text{rowsum}(\frac{\partial f}{\partial A_2}\odot 1) = \left(\frac{\partial f}{\partial A_2}\right)^\top 1, \\
&\frac{\partial f}{\partial Z} = \frac{\partial f}{\partial A_2}\frac{\partial A_2}{\partial Z} = \sigma(A_1)^\top \frac{\partial f}{\partial A_2}, \\
&\frac{\partial f}{\partial A_1} = \frac{\partial f}{\partial A_2}\frac{\partial A_2}{\partial A_1} = \frac{\partial f}{\partial A_2}Z^\top \odot \sigma'(A_1), \\
&\frac{\partial f}{\partial Y} = \frac{\partial f}{\partial A_1}\frac{\partial A_1}{\partial Y} = \left(\frac{\partial f}{\partial A_1}\right)^\top 1, \\
&\frac{\partial f}{\partial X} = \frac{\partial f}{\partial A_1}\frac{\partial A_1}{\partial X} = A^\top \frac{\partial f}{\partial A_1}.
\end{aligned}
\end{equation}

The stochastic subgradients can be chosen to be the subgradient when input is a minibatch of the whole dataset, i.e.
\begin{equation}
G(x, \xi_i) = \partial_x f(x; A_{\xi_i}, b_{\xi_i})
\end{equation}
for each variable $x$, where $\xi_i $ is a uniformly sample index set for each $i$. We can control the size of each $\xi_i $ to vary from online learning to full-batch learning.


\section{Accelerated proximal gradient method}

In order to use APG for this problem, we need to choose 
\begin{equation}
\sigma(z) = \frac{\exp(z)}{1+\exp(z)}, \quad \mathcal{L}(z) = \log(1+\exp(-z))
\end{equation}
to guarantee the objective function is smooth. Notice
\begin{equation}
\mathcal{L}'(z) = -\frac{1}{1+\exp(z)}, 
\end{equation}
and
\begin{equation}
|\ml''(z)| = \left|\frac{e^z}{(1+e^z)^2}\right| \le \frac{1}{4},
\end{equation}
by Lagrange mean value theorem, we know the Lipschitz constant for $\ml'$ is $L=\frac{1}{4}$.

Now let's consider the Lipschitz constant for each derivatives. 

For $\partial_{A_2} $,
% \begin{equation}
% \begin{aligned}
% \lVert \partial^2_{A_2}\rVert_2^2 = \left\lVert \frac{1}{n}\mathcal{L}''(b\odot A_2)\odot b\odot b\right\rVert_2^2 \le \left\lVert \left(\frac{1}{4n}, \hdots, \frac{1}{4n}\right)^\top\right\rVert_2^2 \le \frac{1}{16n}.
%  \end{aligned}
% \end{equation}
\begin{equation}
\begin{aligned}
\left|\frac{\partial f}{\partial A_2^1} - \frac{\partial f}{\partial A_2^2}\right| &= \frac{1}{n}|\mathcal{L}'(b\odot A_2^1)\odot b - \mathcal{L}'(b\odot A_2^2)\odot b| = \frac{1}{n}|(\mathcal{L}'(b\odot A_2^1) - \mathcal{L}'(b\odot A_2^2))\odot b| \\
&= \frac{1}{n} |\ml''(\xi)(b\odot(A_2^1 - A_2^2))\odot b| \\
&\le \frac{1}{4n}|A_2^1 - A_2^2|.
\end{aligned}
\end{equation}
Then the Lipschitz constant for $\frac{\partial f}{\partial A_2} $ is $L_{A_2} = \frac{1}{4n} $.


% For $\frac{\partial f}{\partial w}$,
% \begin{equation}
% \begin{aligned}
% |\partial_w (w_1) - \partial_w (w_2)| &= \frac{1}{n}\sum_{i=1}^{n}b_i(\ml'(b_i(\sigma(A_1)Z)_i + b_i w_1) - \ml' (b_i(\sigma(A_1)Z)_i + b_i w_2) \\
% &\le \frac{1}{n}\sum_{i=1}^{n}b_i(\frac{1}{4}b_i|w_1-w_2|) = \frac{1}{4n}\sum_{i=1}^{n}|w_1-w_2| \\
% &= \frac{1}{4}|w_1-w_2|.
% \end{aligned}
% \end{equation}
For $\frac{\partial f}{\partial w}$,
\begin{equation}
\begin{aligned}
\left|\frac{\partial f}{\partial w^1} - \frac{\partial f}{\partial w^2}\right| &= \left(\frac{\partial f}{\partial A_2^1} - \frac{\partial f}{\partial A_2^2}\right)^\top 1 \\
&\le \frac{1}{4n}|w_1 - w_2|n = \frac{1}{4}|w_1 - w_2|.
\end{aligned}
\end{equation}
Then the Lipschitz constant for $\partial_w $ is $L_w = \frac{1}{4} $.

For $\frac{\partial f}{\partial Z}$, we first notice $\sigma(x) \in (0, 1)$. Then 
\begin{equation}
\begin{aligned}
\left|\frac{\partial f}{\partial Z^1} - \frac{\partial f}{\partial Z^2}\right| &= \left| \sigma(A_1)^\top \left(\frac{\partial f}{\partial A_2^1}-\frac{\partial f}{\partial A_2^2}\right)\right|\le \frac{1}{4}\left| \sigma(A_1)^\top (A_2^1 - A_2^2)\right| \\
&= \frac{1}{4}|\sigma(A_1)^\top \sigma(A_1)(Z_1-Z_2)| \\
&\le \frac{1}{4}\lVert \sigma(A_1)^\top\sigma(A_1)\rVert_2 |Z_1-Z_2| \\
&\le \frac{1}{4}\lVert \sigma(A_1)^\top\sigma(A_1)\rVert_F |Z_1-Z_2| \\
&\le \frac{1}{4}\sqrt{k^2 n^4}|Z_1-Z_2| = \frac{1}{4}kn^2|Z_1-Z_2|.
\end{aligned}
\end{equation}
Then $L_z = \frac{1}{4}kn^2 $ can be an upper bound for the Lipschitz constant for $\partial_z $.

% For $\frac{\partial f}{\partial A_1}$, 
% \begin{equation}
% \begin{aligned}
% \left|\frac{\partial f}{\partial A_1^1} - \frac{\partial f}{\partial A_1^2}\right| = \frac{\partial f}{\partial A_2}
% \end{aligned}
% \end{equation}

The lipschitz constant for $\frac{\partial f}{\partial y}$ and $\frac{\partial f}{\partial x}$ are too complex to solve. 

\section{Proximal Gradient method with line search}
In order to use PG for this problem, we need to choose 
\begin{equation}
\sigma(z) = \frac{\exp(z)}{1+\exp(z)}, \quad \mathcal{L}(z) = \log(1+\exp(-z))
\end{equation}
to guarantee the objective function is smooth.

Since we use an iterative way to update the variables, the value of $x$ will be updated from $x_0 $ to $x_1 $ before we start to update $y, z, w$. So there are two schemes in updating:
\begin{enumerate}
\item Use $x_0 $ to update $y$, and use $x_0, y_0 $ to update $z$, and use $x_0, y_0, z_0 $ to update $w$ (refered as PG1).
\item Use $x_1 $ to update $y$. In this case, we need to recompute the objective function and the derivatives using $ x_1 $. For $y, z, w$, we also use this scheme (refered as PG2).
\end{enumerate}
We test both schemes in the experiments.

\section{Experiments}
For the experiments, we test the one-hidden-layer neural network on the datasets \texttt{rcv1.binary} and \texttt{covtype} from libsvm library. We use the same random seed across the experiments to get the same random initialization for a fair comparison. For all following experiments, we compute the objective value, subgradient and out-of-sample accuracy on the averaged variable after each iteration.

We compute the out-of-sample prediction accuracy by
\begin{equation}
Acc = \frac{1}{n}\sum_{i=1}^{n} 1_{\text{sign}(A_{2i}) = \text{sign}(b_i)} \times 100\%
\end{equation}
on an independent test set.

We compute 2-norm of $\partial x$ by first flatten it to a vector, and then compute the vector 2-norm instead of matrix 2-norm. 

\subsection{rcv1.binary}
For \texttt{rcv1.binary}, we use only the \texttt{rcv1\_train.binary} from libsvm library. We randomly sample $20\%$ of the dataset, i.e., 4048 samples to form the test set, and use the rest part as the training set. Both test set and training set remain the same across the four methods.

For memory issues (i.e., since we need to keep track of each variables and their derivatives), for PG and APG, we run the algorithm with $K = 200$ iterations and only keep track of the results from $2^{nd}, 4^{th}, \hdots, 200^{th} $ iterations. For SSG, we run $K = 1200$ iterations with minibatch size $N = 2699$ in each iteration, and only keep track of the results from $12^{th}, 24^{th}, \hdots, 1200^{th} $ iterations. Then the total numbers of training samples passed for all three algorithms are the same.

The objective and accuracy of the four methods are shown in Figure \ref{rc1.binary-obj-acc}. PG2 acchieves the lowest objective and highest accuracy among the four methods, with PG1 follows. However, for PG with line search, the computational complexity is much higher than APG and SSG because of the inner loop, and the computational cost of PG2 is much higher than PG1. Figure \ref{rc1.binary-subgradient} shows the 2-norm of each subgradient with respect to the number of iterations. The result agrees with the result of objective and accuracy.

\begin{figure}[h]
\centering
\vbox{
	\includegraphics[scale = 0.5]{plots/rcv1.binary/objective.jpg}
	\includegraphics[scale = 0.5]{plots/rcv1.binary/accuracy.jpg}
}
\caption{Objective and Accuracy of the four methods with respect to the number of iterations.}
\label{rc1.binary-obj-acc}
\end{figure}

\begin{figure}[h]
\centering
\vbox{
	\includegraphics[scale = 0.5]{plots/rcv1.binary/dx.jpg}
	\includegraphics[scale = 0.5]{plots/rcv1.binary/dy.jpg}
} 
\vbox{
	\includegraphics[scale = 0.5]{plots/rcv1.binary/dz.jpg}
	\includegraphics[scale = 0.5]{plots/rcv1.binary/dw.jpg}
}
\caption{Norm of subgradients of the four methods with respect to the number of iterations.}
\label{rc1.binary-subgradient}
\end{figure}

\subsection{covtype.binary}

We also test the four methods on the \texttt{covtype.binary}. We first rescale the $b$ vector to $\{-1, 1\}$ as mentioned in a homework before. We randomly sampled $20\%$ of the whole dataset, i.e., 116202 samples to form the test set, and use the rest part as the training set.

For APG and PG, we run 200 iterations and record the results from the even iterations. For SSG, we run 2000 iterations with minibatch size $N = 46481$, and only record the results from the $20^{th}, 40^{th}, \hdots, 2000^{th} $ iterations.

\end{document}
